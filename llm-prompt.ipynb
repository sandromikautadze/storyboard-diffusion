{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "from typing import Dict, List, Tuple, Any\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from PIL import Image\n",
    "from together import Together\n",
    "from src.prompt_scheme import SceneList\n",
    "from diffusers import StableDiffusionPipeline, UniPCMultistepScheduler\n",
    "from src.models import MultiPromptPipelineApproach1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up module-level logging.\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(logging.Formatter(\"%(asctime)s %(levelname)s: %(message)s\"))\n",
    "logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoryboardGenerator:\n",
    "    ORIENTATIONS: List[str] = [\n",
    "        \"Front View\", \"Profile View\", \"Back View\", \"From Behind\", \"From Above\",\n",
    "        \"From Below\", \"Three-Quarters View\", \"Long Shot\", \"Three-Quarters Rear View\"\n",
    "    ]\n",
    "\n",
    "    CAMERA_SHOTS: List[str] = [\n",
    "        \"Aerial View\", \"Birdâ€™s-Eye View\", \"Close-Up\", \"Cowboy Shot\", \"Dolly Zoom\",\n",
    "        \"Dutch Angle\", \"Establishing Shot\", \"Extreme Close-Up\", \"Extreme Long Shot\",\n",
    "        \"Full Shot\", \"Long Shot\", \"Medium Close-Up\", \"Medium Long Shot\", \"Medium Shot\",\n",
    "        \"Over-the-Shoulder Shot\", \"Point-of-View Shot\", \"Two-Shot\", \"Fisheye Shot\",\n",
    "        \"Worm's Eye\", \"Low-Angle Shot\", \"Macro Shot\", \"Tilt-Shift Shot\", \"Telephoto Shot\"\n",
    "    ]\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        script: str, \n",
    "        characters: Dict[str, Dict[str, str]], \n",
    "        style: str = \"storyboard\", \n",
    "        prompt_weights: List[float] = [2, 1.0, 1.2, 1.5, 0.9],  # used only for 'prompt_weights' generation and 'modified-cfg'\n",
    "        temperature: float = 0.7,\n",
    "        device: str = \"cpu\", \n",
    "        seed: int = 42\n",
    "    ) -> None:\n",
    "        load_dotenv()\n",
    "        self.together = Together()\n",
    "        self.script: str = script\n",
    "        self.characters: Dict[str, Dict[str, str]] = characters\n",
    "        self.style: str = style\n",
    "        self.prompt_weights: List[float] = prompt_weights\n",
    "        self.temperature: float = temperature\n",
    "        self.device: str = device\n",
    "        self.seed: int = seed\n",
    "        self.scenes: Any = None\n",
    "        self.formatted_prompts: Any = None\n",
    "        \n",
    "        # valid_generation_types = {\"unique\", \"prompt_weights\", \"modified-cfg\"}\n",
    "        self.current_generation_type: str = \"\"\n",
    "        # if self.generation_type not in valid_generation_types:\n",
    "            # raise ValueError(f\"Invalid generation_type: {self.generation_type}. Must be one of {valid_generation_types}\")\n",
    "        \n",
    "        self.pipe: Any = None\n",
    "        \n",
    "    def _setup_pipeline(self, generation_type: str) -> Any:\n",
    "        if generation_type in {\"unique\", \"prompt_weights\"}:\n",
    "            pipe = StableDiffusionPipeline.from_pretrained(\n",
    "                \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16\n",
    "            )\n",
    "        elif generation_type == \"modified-cfg\":\n",
    "            pipe = MultiPromptPipelineApproach1.from_pretrained(\n",
    "                \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported generation type: {generation_type}\")\n",
    "        \n",
    "        pipe = pipe.to(self.device)\n",
    "        pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "        pipe.enable_model_cpu_offload()\n",
    "        pipe.enable_attention_slicing()\n",
    "        return pipe\n",
    "    \n",
    "    def _get_pipeline(self, generation_type: str) -> Any:\n",
    "        # Reload only if the requested type is different from the current one.\n",
    "        if self.current_generation_type != generation_type or self.pipe is None:\n",
    "            self.pipe = self._setup_pipeline(generation_type)\n",
    "            self.current_generation_type = generation_type\n",
    "        return self.pipe\n",
    "\n",
    "\n",
    "    def _build_character_description(self, char_info: Dict[str, str]) -> str:\n",
    "        \"\"\"\n",
    "        Generates a textual description for a character given its attribute dictionary.\n",
    "        \"\"\"\n",
    "        features = [\n",
    "            char_info.get(\"ethnicity\", \"\"),\n",
    "            char_info.get(\"age\", \"\"),\n",
    "            char_info.get(\"gender\", \"\"),\n",
    "            char_info.get(\"hair\", \"\"),\n",
    "            char_info.get(\"facial_hair\", \"\"),\n",
    "            char_info.get(\"body_type\", \"\"),\n",
    "            f\"wearing {char_info.get('clothing', '')}\",\n",
    "            f\"with {char_info.get('accessories', '')}\" if char_info.get(\"accessories\") else \"\"\n",
    "        ]\n",
    "        return \", \".join(filter(None, features))\n",
    "    \n",
    "    def input_to_json(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Converts the script and character descriptions into a JSON structure for storyboard generation.\n",
    "        \"\"\"\n",
    "        character_descriptions = {\n",
    "            name: self._build_character_description(desc) \n",
    "            for name, desc in self.characters.items()\n",
    "        }\n",
    "        script_section = f\"Here is the film script: \\n{self.script}\"\n",
    "        characters_section = f\"The characters in the script have the following descriptions: \\n{json.dumps(character_descriptions, indent=2)}\"\n",
    "        instructions = f\"\"\"\n",
    "    ### Storyboard Generation Instructions\n",
    "    1. **Number of Scenes**: Divide the entire script into a reasonable number of scenes (typically between 4 to 7 scenes), not too many or too few.\n",
    "    2. **Single Distinct Moment**: Each scene captures a single moment.\n",
    "    3. **Camera Angles & Orientation**: Choose from these shot types: {', '.join(self.CAMERA_SHOTS)}.  \n",
    "    Choose from these orientations: {', '.join(self.ORIENTATIONS)}.\n",
    "    4. **Location & Time**: Clearly derive environment from the script (e.g. INT DAY, DON'S OFFICE, etc.). Describe it in its details (size, lighting, mood, organization of the objects, etc.). Notice that if it's the same across the different scenes, it must be written in the same way\n",
    "    5. **Characters**:\n",
    "    - List only characters relevant to the single moment in each scene.\n",
    "    - Each character must have the name and a short description (consistent from provided descriptions).\n",
    "    6. Clearly describe the scene including actions, character positions (foreground, background, left, right), emotions, and expressions.\n",
    "    7. **Scene Format**: Return JSON with a key 'scenes' as an array of structured objects:\n",
    "    - \"scene_number\": integer\n",
    "    - \"shot_type\": camera shot type (from provided list) \n",
    "    - \"orientation\": orientation (from provided list)\n",
    "    - \"characters\": list of objects with:\n",
    "            - \"name\": character's name, not as they appear on the script but as they were given to you in the description.\n",
    "    - \"environment\": short description of the location\n",
    "    - \"description\": short, vivid description focusing on actions, expressions, emotions of each single character. Also their relative position is clearly described. The description must be succint, without extra articles or words, it should be visual and useful for an image generation prompt. Ensure it makes sense with the shot type (e.g., if it's medium shot, don't say that the face is covering the full image, otherwise it should be a close up). Don't write the words they say, since they occupy tokens, unless it's a fundamental part of the script. Avoid useless adjectives or adverbs, be concise and clear.\n",
    "\n",
    "    Follow the above instructions very carefully. Notice that the scenes have no knowledge of each other's contents. So in case something is necessary, describe it again. \n",
    "    \"\"\"\n",
    "\n",
    "        example_input = \"\"\"\n",
    "    ### Example\n",
    "    Input: \n",
    "    - Script is \n",
    "    INT DAY: DON'S OFFICE (SUMMER 1945)\n",
    "\n",
    "            DON CORLEONE\n",
    "    ACT LIKE A MAN!  By Christ in\n",
    "    Heaven, is it possible you turned\n",
    "    out no better than a Hollywood\n",
    "    finocchio.\n",
    "\n",
    "    Both HAGEN and JOHNNY cannot refrain from laughing.  The DON\n",
    "    smiles.  SONNY enters as noiselessly as possible, still\n",
    "    adjusting his clothes.\n",
    "\n",
    "            DON CORLEONE\n",
    "    All right, Hollywood...Now tell me\n",
    "    about this Hollywood Pezzonovanta\n",
    "    who won't let you work.\n",
    "\n",
    "            JOHNNY\n",
    "    He owns the studio.  Just a month\n",
    "    ago he bought the movie rights to\n",
    "    this book, a best seller.  And the\n",
    "    main character is a guy just like\n",
    "    me.  I wouldn't even have to act,\n",
    "    just be myself.\n",
    "\n",
    "    The DON is silent, stern.\n",
    "\n",
    "            DON CORLEONE\n",
    "    You take care of your family?\n",
    "\n",
    "            JOHNNY\n",
    "    Sure.\n",
    "\n",
    "    He glances at SONNY, who makes himself as inconspicuous as\n",
    "    he can.\n",
    "\n",
    "            DON CORLEONE\n",
    "    You look terrible.  I want you to\n",
    "    eat well, to rest.  And spend time\n",
    "    with your family.  And then, at the\n",
    "    end of the month, this big shot\n",
    "    will give you the part you want.\n",
    "\n",
    "            JOHNNY\n",
    "    It's too late.  All the contracts\n",
    "    have been signed, they're almost\n",
    "    ready to shoot.\n",
    "\n",
    "            DON CORLEONE\n",
    "    I'll make him an offer he can't\n",
    "    refuse.\n",
    "\n",
    "    He takes JOHNNY to the door, pinching his cheek hard enough\n",
    "    to hurt.\n",
    "\n",
    "            DON CORLEONE\n",
    "    Now go back to the party and leave\n",
    "    it to me.\n",
    "\n",
    "    He closes the door, smiling to himself.  Turns to HAGEN.\n",
    "\n",
    "            DON CORLEONE\n",
    "    When does my daughter leave with\n",
    "    her bridegroom?\n",
    "\n",
    "            HAGEN\n",
    "    They'll cut the cake in a few\n",
    "    minutes...leave right after that.\n",
    "    Your new son-in-law, do we give him\n",
    "    something important?\n",
    "\n",
    "            DON CORLEONE\n",
    "    No, give him a living.  But never\n",
    "    let him know the family's business.\n",
    "    What else, Tom?\n",
    "\n",
    "            HAGEN\n",
    "    I've called the hospital; they've\n",
    "    notified Consigliere Genco's family\n",
    "    to come and wait.  He won't last\n",
    "    out the night.\n",
    "\n",
    "    This saddens the DON.  He sighs.\n",
    "\n",
    "            DON CORLEONE\n",
    "    Genco will wait for me.  Santino,\n",
    "    tell your brothers they will come\n",
    "    with me to the hospital to see\n",
    "    Genco.  Tell Fredo to drive the big\n",
    "    car, and ask Johnny to come with us.\n",
    "\n",
    "            SONNY\n",
    "    And Michael?\n",
    "\n",
    "            DON CORLEONE\n",
    "    All my sons.\n",
    "            (to HAGEN)\n",
    "    Tom, I want you to go to California\n",
    "    tonight.  Make the arrangements.\n",
    "    But don't leave until I come back\n",
    "    from the hospital and speak to you.\n",
    "    Understood?\n",
    "\n",
    "            HAGEN\n",
    "    Understood.\n",
    "\n",
    "    - Characters description from the dictionary gives\n",
    "            - Don Vito Corleone: 'Italian-American, early 60s, male, slicked-back gray-black hair, stocky, slightly hunched posture, wearing dark three-piece suit, with gold ring on right hand, pocket watch'\n",
    "            - Johnny Fontane: 'late 30s, male, short, slicked-back black hair, clean shaven, slim and fit, wearing dark, stylish suit with an open collar, with gold ring, cigarette'\n",
    "            - Tom Hagen: 'German-Irish, early 40s, male, short, neatly combed brown hair, clean-shaven, medium build, upright posture, wearing gray suit, dark tie'\n",
    "            - Sonny: 'Italian-American, early 30s, male, curly, dark brown hair, clean-shaven, athletic build, wearing formal suit, slightly disheveled'\n",
    "    \"\"\"\n",
    "\n",
    "        example_output = \"\"\"\n",
    "    Example Output:\n",
    "    {\n",
    "    \"scenes\": [\n",
    "    {\n",
    "    \"scene_number\": 1,\n",
    "    \"shot_type\": \"Medium Shot\",\n",
    "    \"orientation\": \"Front View\",\n",
    "    \"characters\": [\n",
    "            {\n",
    "            \"name\": \"Don Vito Corleone\"\n",
    "            },\n",
    "            {\n",
    "            \"name\": \"Johnny Fontane\"\n",
    "            },\n",
    "            {\n",
    "            \"name\": \"Tom Hagen\"\n",
    "            }\n",
    "    ],\n",
    "    \"environment\": \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
    "    \"description\": \"Don Corleone stands imposingly behind desk, face stern with righteous anger, pointing finger at Johnny. Johnny appears embarrassed, head slightly bowed. Hagen stands to the right, barely containing laughter. Tension and amusement mix in intimate office atmosphere.\"\n",
    "    },\n",
    "    {\n",
    "    \"scene_number\": 2,\n",
    "    \"shot_type\": \"Two-Shot\",\n",
    "    \"orientation\": \"Profile View\",\n",
    "    \"characters\": [\n",
    "            {\n",
    "            \"name\": \"Don Vito Corleone\"\n",
    "            },\n",
    "            {\n",
    "            \"name\": \"Johnny Fontane\"\n",
    "            },\n",
    "            {\n",
    "            \"name\": \"Tom Hagen\"\n",
    "            },\n",
    "            {\n",
    "            \"name\": \"Sonny\"\n",
    "            }\n",
    "    ],\n",
    "    \"environment\": \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
    "    \"description\": \"Sonny quietly enters room from right, adjusting disheveled clothes. Don leans forward at desk, expression softening to business-like focus. Johnny stands center, straightening posture. Hagen observes from left corner. Atmosphere shifts from personal rebuke to business discussion.\"\n",
    "    },\n",
    "    {\n",
    "    \"scene_number\": 3,\n",
    "    \"shot_type\": \"Close-Up\",\n",
    "    \"orientation\": \"Front View\",\n",
    "    \"characters\": [\n",
    "            {\n",
    "            \"name\": \"Don Vito Corleone\"\n",
    "            }\n",
    "    ],\n",
    "    \"environment\": \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
    "    \"description\": \"Don Corleone's face fills frame, stern and contemplative. Eyes narrowed, jaw set firmly. Saying 'I'll make him an offer he can't refuse' with quiet, confident menace. Power and authority emanate from his expression.\"\n",
    "    },\n",
    "    {\n",
    "    \"scene_number\": 4,\n",
    "    \"shot_type\": \"Medium Close-Up\",\n",
    "    \"orientation\": \"Three-Quarters View\",\n",
    "    \"characters\": [\n",
    "            {\n",
    "            \"name\": \"Don Vito Corleone\"\n",
    "            },\n",
    "            {\n",
    "            \"name\": \"Johnny Fontane\"\n",
    "            }\n",
    "    ],\n",
    "    \"environment\": \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
    "    \"description\": \"Don Corleone escorts Johnny to door, pinching his cheek firmly. Don's expression shows affection mixed with dominance. Johnny winces slightly at pain while showing relief and gratitude. Door frame visible on right edge of shot.\"\n",
    "    },\n",
    "    {\n",
    "    \"scene_number\": 5,\n",
    "    \"shot_type\": \"Medium Shot\",\n",
    "    \"orientation\": \"Front View\",\n",
    "    \"characters\": [\n",
    "            {\n",
    "            \"name\": \"Don Vito Corleone\"\n",
    "            },\n",
    "            {\n",
    "            \"name\": \"Tom Hagen\"\n",
    "            }\n",
    "    ],\n",
    "    \"environment\": \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
    "    \"description\": \"Don Corleone turns from closed door, small smile fading to serious business expression. Hagen stands attentively near desk, notepad ready. Don moves toward chair, shoulders slightly hunched, gold ring catching light as he gestures.\"\n",
    "    },\n",
    "    {\n",
    "    \"scene_number\": 6,\n",
    "    \"shot_type\": \"Over-the-Shoulder Shot\",\n",
    "    \"orientation\": \"Profile View\",\n",
    "    \"characters\": [\n",
    "            {\n",
    "            \"name\": \"Don Vito Corleone\"\n",
    "            },\n",
    "            {\n",
    "            \"name\": \"Tom Hagen\"\n",
    "            },\n",
    "            {\n",
    "            \"name\": \"Sonny\"\n",
    "            }\n",
    "    ],\n",
    "    \"environment\": \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
    "    \"description\": \"Camera over Don's shoulder, facing Hagen and Sonny. Don's gray-black hair and dark suit visible in foreground. Hagen's face shows respectful attention. Sonny stands beside him, now composed. Don's voice carries weight as he issues final instructions about hospital visit.\"\n",
    "    }\n",
    "    ]\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "        user_content = f\"{script_section}\\n\\n{characters_section}\\n\\n{instructions}\\n{example_input}\\n{example_output}\"\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": (\n",
    "                \"You are an AI specialized in creating structured storyboard scenes from a film script \"\n",
    "                \"for image generation (e.g., stable diffusion). Each scene must capture a single distinct moment, \"\n",
    "                \"should list relevant characters with consistent appearances, specify the environment, camera shot, \"\n",
    "                \"and orientation, and provide direct clues for a diffusion model to generate images.\"\n",
    "                )},\n",
    "            {\"role\": \"user\", \"content\": user_content}\n",
    "        ]\n",
    "        \n",
    "        response = self.together.chat.completions.create(\n",
    "            model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "            messages=messages,\n",
    "            max_tokens=10000,\n",
    "            temperature=self.temperature,\n",
    "            response_format={\"type\": \"json_object\", \"schema\": SceneList.model_json_schema()}\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            output_json = response.choices[0].message.content\n",
    "            self.scenes = json.loads(output_json)[\"scenes\"]\n",
    "            return self.scenes\n",
    "        except (json.JSONDecodeError, KeyError) as e:\n",
    "            logger.error(\"Error parsing JSON output: %s\", e)\n",
    "            return []\n",
    "        \n",
    "    def scenes_to_formatted_prompts(self) -> List[Tuple[List[str], List[float]]]:\n",
    "        \"\"\"\n",
    "        Converts a list of scenes into structured diffusion model prompts with weights.\n",
    "        \n",
    "        Returns:\n",
    "            List[Tuple[List[str], List[float]]]: Each tuple contains subprompt texts and their corresponding weights.\n",
    "        \"\"\"\n",
    "        if self.formatted_prompts is not None:\n",
    "            return self.formatted_prompts\n",
    "        logger.info(\"Generating formatted prompts...\")\n",
    "        weight_map = {\n",
    "            \"style\": self.prompt_weights[0],\n",
    "            \"environment\": self.prompt_weights[1],\n",
    "            \"shot\": self.prompt_weights[2],\n",
    "            \"description\": self.prompt_weights[3]\n",
    "        }\n",
    "        character_weight = self.prompt_weights[4]\n",
    "\n",
    "        style_value = (\"rough b&w pencil sketch, simple sketch lines, minimal shading, rough hatching, draft-style, \"\n",
    "                       \"J.C. Leyendecker style\") if self.style == \"storyboard\" else self.style\n",
    "\n",
    "        formatted_results: List[Tuple[List[str], List[float]]] = []\n",
    "        if self.scenes is None:\n",
    "            self.input_to_json()\n",
    "\n",
    "        for scene in self.scenes:\n",
    "            subprompts: Dict[str, str] = {}\n",
    "            for i, char in enumerate(scene[\"characters\"]):\n",
    "                char_name = char[\"name\"]\n",
    "                char_info = self.characters.get(char_name)\n",
    "                if not char_info:\n",
    "                    matching_keys = [key for key in self.characters if char_name in key]\n",
    "                    if matching_keys:\n",
    "                        char_info = self.characters.get(matching_keys[0],\n",
    "                                                        {\"age\": \"unknown\", \"gender\": \"unknown\", \"hair\": \"unknown\",\n",
    "                                                         \"clothing\": \"unknown\", \"body_type\": \"unknown\"})\n",
    "                    else:\n",
    "                        char_info = {\"age\": \"unknown\", \"gender\": \"unknown\", \"hair\": \"unknown\", \"clothing\": \"unknown\", \"body_type\": \"unknown\"}\n",
    "                char_desc = self._build_character_description(char_info)\n",
    "                subprompts[f\"character{i+1}\"] = f\"{char_name}: {char_desc}\"\n",
    "            \n",
    "            subprompts[\"style\"] = style_value\n",
    "            subprompts[\"environment\"] = scene[\"environment\"]\n",
    "            subprompts[\"shot\"] = f\"{scene['shot_type']}, {scene['orientation']}\"\n",
    "            subprompts[\"description\"] = scene[\"description\"]\n",
    "\n",
    "            subprompt_texts: List[str] = []\n",
    "            subprompt_weights: List[float] = []\n",
    "            for key, text in subprompts.items():\n",
    "                subprompt_texts.append(text)\n",
    "                if key.startswith(\"character\"):\n",
    "                    subprompt_weights.append(character_weight)\n",
    "                else:\n",
    "                    subprompt_weights.append(weight_map.get(key, 1.0))\n",
    "            formatted_results.append((subprompt_texts, subprompt_weights))\n",
    "        self.formatted_prompts = formatted_results\n",
    "        logger.info(\"Formatted prompts generated successfully.\")\n",
    "        return formatted_results\n",
    "    \n",
    "    def _save_image(self, image: Image.Image, image_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Saves the generated image to the specified path.\n",
    "        \"\"\"\n",
    "        image.save(image_path)\n",
    "        logger.info(\"Image saved to %s\", image_path)\n",
    "    \n",
    "    # unique prompt\n",
    "    def build_unique_prompts(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Builds unique prompt strings for each scene by concatenating the style, shot, and description.\n",
    "        \n",
    "        Returns:\n",
    "            List[str]: List of unique prompt strings.\n",
    "        \"\"\"\n",
    "        if self.formatted_prompts is None:\n",
    "            self.scenes_to_formatted_prompts()\n",
    "        style_override = \"rough b&w simple pencil sketch, J.C. Leyendecker style,\" if self.style == \"storyboard\" else self.style\n",
    "        unique_prompts: List[str] = []\n",
    "        for subprompt_texts, _ in self.formatted_prompts:\n",
    "            if len(subprompt_texts) < 2:\n",
    "                logger.error(\"Insufficient subprompt texts to build unique prompt.\")\n",
    "                continue\n",
    "            # Assumes the penultimate text is the shot prompt and the last is the description.\n",
    "            shot_prompt = subprompt_texts[-2]\n",
    "            description = subprompt_texts[-1]\n",
    "            unique_prompt = f\"{style_override} {shot_prompt}: {description}\"\n",
    "            unique_prompts.append(unique_prompt)\n",
    "        return unique_prompts\n",
    "        \n",
    "    def generate_and_save_images_unique_prompts(\n",
    "        self,\n",
    "        save_dir: str, \n",
    "        generation_type: str = \"unique\", \n",
    "        negative_prompt: str = \"low quality, photorealistic, 3d render, overly detailed, digital art, painting, vibrant colors, fine art, NSFW\", \n",
    "        num_inference_steps: int = 50, \n",
    "        guidance_scale: float = 7.5,\n",
    "        )-> List[Image.Image]:\n",
    "        \"\"\"\n",
    "        Generates images using unique prompts and saves them.\n",
    "        \n",
    "        Returns:\n",
    "            List[Image.Image]: List of generated images.\n",
    "        \"\"\"\n",
    "        pipe = self._get_pipeline(generation_type)  # generation_type should be \"unique\" here\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        unique_prompts = self.build_unique_prompts()\n",
    "        generated_images: List[Image.Image] = []\n",
    "        for i, unique_prompt in enumerate(unique_prompts):\n",
    "            with torch.no_grad():\n",
    "                output = pipe(\n",
    "                    prompt=unique_prompt,\n",
    "                    negative_prompt=negative_prompt,\n",
    "                    guidance_scale=guidance_scale,\n",
    "                    num_inference_steps=num_inference_steps\n",
    "                )\n",
    "            generated_image = output.images[0]\n",
    "            generated_images.append(generated_image)\n",
    "            image_path = os.path.join(save_dir, f\"image_{i+1}.png\")\n",
    "            self._save_image(generated_image, image_path)\n",
    "        return generated_images\n",
    "    \n",
    "    # prompt weights\n",
    "    def weighted_sum_prompt_embeddings(\n",
    "        self, \n",
    "        subprompt_texts: List[str], \n",
    "        subprompt_weights: List[float], \n",
    "        num_images_per_prompt: int = 1\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes a weighted sum of text embeddings for a list of subprompts.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Combined prompt embeddings.\n",
    "        \"\"\"\n",
    "        encoded_prompts = []\n",
    "        for text in subprompt_texts:\n",
    "            text_inputs = self.pipe.tokenizer(\n",
    "                text,\n",
    "                padding=\"max_length\",\n",
    "                max_length=self.pipe.tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            input_ids = text_inputs.input_ids.to(self.device)\n",
    "            attention_mask = text_inputs.attention_mask.to(self.device) if \"attention_mask\" in text_inputs else None\n",
    "            text_embeds = self.pipe.text_encoder(input_ids, attention_mask=attention_mask)[0]\n",
    "            encoded_prompts.append(text_embeds)\n",
    "        weighted_embedding = sum(weight * embeds for weight, embeds in zip(subprompt_weights, encoded_prompts))\n",
    "        weight_total = sum(subprompt_weights)\n",
    "        combined_embedding = weighted_embedding / weight_total\n",
    "        batch_size, seq_len, embed_dim = combined_embedding.shape\n",
    "        combined_embedding = combined_embedding.repeat(1, num_images_per_prompt, 1)\n",
    "        combined_embedding = combined_embedding.view(batch_size * num_images_per_prompt, seq_len, embed_dim)\n",
    "        return combined_embedding\n",
    "    \n",
    "    def generate_and_save_images_prompt_weights(\n",
    "        self, \n",
    "        save_dir: str, \n",
    "        generation_type: str = \"prompt_weights\", \n",
    "        negative_prompt: str = \"low quality, photorealistic, 3d render, overly detailed, digital art, painting, vibrant colors, fine art, NSFW\", \n",
    "        num_inference_steps: int = 50, \n",
    "        guidance_scale: float = 7.5,\n",
    "    ) -> List[Image.Image]:\n",
    "        \"\"\"\n",
    "        Generates images using weighted prompt embeddings and saves them.\n",
    "        \"\"\"\n",
    "        pipe = self._get_pipeline(generation_type)  # generation_type should be \"prompt_weights\" here\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        generated_images: List[Image.Image] = []\n",
    "        formatted_prompts = self.scenes_to_formatted_prompts()\n",
    "        for i, (subprompt_texts, subprompt_weights) in enumerate(formatted_prompts):\n",
    "            combined_embeddings = self.weighted_sum_prompt_embeddings(subprompt_texts, subprompt_weights)\n",
    "            with torch.no_grad():\n",
    "                output = pipe(\n",
    "                    prompt_embeds=combined_embeddings,\n",
    "                    negative_prompt=negative_prompt,\n",
    "                    guidance_scale=guidance_scale,\n",
    "                    num_inference_steps=num_inference_steps\n",
    "                )\n",
    "            generated_image = output.images[0]\n",
    "            generated_images.append(generated_image)\n",
    "            image_path = os.path.join(save_dir, f\"image_{i+1}.png\")\n",
    "            self._save_image(generated_image, image_path)\n",
    "        return generated_images\n",
    "    \n",
    "    # modified-cfg\n",
    "    def encode_subprompt(self, text: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Tokenizes and encodes a single subprompt into an embedding.\n",
    "        \"\"\"\n",
    "        text_inputs = self.pipe.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.pipe.tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        text_embeds = self.pipe.text_encoder(\n",
    "            text_inputs.input_ids.to(self.device),\n",
    "            attention_mask=text_inputs.attention_mask.to(self.device)\n",
    "        )[0]\n",
    "        return text_embeds\n",
    "        \n",
    "    def generate_and_save_images_multi_prompt(\n",
    "        self, \n",
    "        save_dir: str, \n",
    "        generation_type: str = \"modified-cfg\", \n",
    "        negative_prompt: str = \"low quality, photorealistic, 3d render, overly detailed, digital art, painting, vibrant colors, fine art, NSFW\", \n",
    "        num_inference_steps: int = 50, \n",
    "        guidance_scale: float = 7.5,\n",
    "    ) -> List[Image.Image]:\n",
    "        \"\"\"\n",
    "        Generates images using the Multi-Prompt pipeline and saves them.\n",
    "        \"\"\"\n",
    "        pipe = self._get_pipeline(generation_type)  # generation_type should be \"modified-cfg\" here\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        generated_images: List[Image.Image] = []\n",
    "        uncond_embeds = self.encode_subprompt(negative_prompt)\n",
    "        formatted_prompts = self.scenes_to_formatted_prompts()\n",
    "        for i, (subprompt_texts, subprompt_weights) in enumerate(formatted_prompts):\n",
    "            subprompt_embeds = [self.encode_subprompt(sp) for sp in subprompt_texts]\n",
    "            logger.info(\"Generating image for scene %d...\", i+1)\n",
    "            with torch.no_grad():\n",
    "                output = pipe(\n",
    "                    subprompt_embeds=subprompt_embeds,\n",
    "                    subprompt_weights=subprompt_weights,\n",
    "                    uncond_embeds=uncond_embeds,\n",
    "                    guidance_scale=guidance_scale,\n",
    "                    num_inference_steps=num_inference_steps\n",
    "                )\n",
    "            generated_image = output.images[0]\n",
    "            generated_images.append(generated_image)\n",
    "            image_path = os.path.join(save_dir, f\"image_{i+1}.png\")\n",
    "            self._save_image(generated_image, image_path)\n",
    "        return generated_images\n",
    "    \n",
    "    def generate_and_save_images(\n",
    "        self, \n",
    "        save_dir: str, \n",
    "        generation_type: str, \n",
    "        negative_prompt: str = \"low quality, photorealistic, 3d render, overly detailed, digital art, painting, vibrant colors, fine art, NSFW\", \n",
    "        num_inference_steps: int = 50, \n",
    "        guidance_scale: float = 7.5,\n",
    "    ) -> List[Image.Image]:\n",
    "        \"\"\"\n",
    "        Unified method that generates and saves images based on the provided generation type.\n",
    "        \"\"\"\n",
    "        if generation_type == \"unique\":\n",
    "            return self.generate_and_save_images_unique_prompts(save_dir, generation_type, negative_prompt, num_inference_steps, guidance_scale)\n",
    "        elif generation_type == \"prompt_weights\":\n",
    "            return self.generate_and_save_images_prompt_weights(save_dir, generation_type, negative_prompt, num_inference_steps, guidance_scale)\n",
    "        elif generation_type == \"modified-cfg\":\n",
    "            return self.generate_and_save_images_multi_prompt(save_dir, generation_type, negative_prompt, num_inference_steps, guidance_scale)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported generation type: {generation_type}\")\n",
    "        \n",
    "    def generate_and_save_prompts_txt(self, save_dir: str, generation_type: str) -> None:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        file_path = os.path.join(save_dir, \"prompts.txt\")\n",
    "        lines = []\n",
    "        if generation_type == \"unique\":\n",
    "            unique_prompts = self.build_unique_prompts()\n",
    "            for i, prompt in enumerate(unique_prompts):\n",
    "                lines.append(f\"Scene {i+1} Unique Prompt:\\n{prompt}\\n\")\n",
    "        else:\n",
    "            # For prompt_weights and modified-cfg, save subprompts breakdown.\n",
    "            if self.formatted_prompts is None:\n",
    "                self.scenes_to_formatted_prompts()\n",
    "            for i, (subprompt_texts, subprompt_weights) in enumerate(self.formatted_prompts):\n",
    "                lines.append(f\"Scene {i+1} Subprompts:\")\n",
    "                for j, text in enumerate(subprompt_texts):\n",
    "                    weight = subprompt_weights[j]\n",
    "                    lines.append(f\"  Subprompt {j+1} (weight {weight}): {text}\")\n",
    "                lines.append(\"\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\\n\".join(lines))\n",
    "        logger.info(\"Prompts saved to %s\", file_path)\n",
    "        \n",
    "    def generate_and_save(\n",
    "        self, \n",
    "        save_dir: str, \n",
    "        generation_type: str, \n",
    "        negative_prompt: str = \"low quality, photorealistic, 3d render, overly detailed, digital art, painting, vibrant colors, fine art, NSFW\", \n",
    "        num_inference_steps: int = 50, \n",
    "        guidance_scale: float = 7.5,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Unified method that generates and saves images and prompt text.\n",
    "        \"\"\"\n",
    "        self.generate_and_save_images(save_dir, generation_type, negative_prompt, num_inference_steps, guidance_scale)\n",
    "        self.generate_and_save_prompts_txt(save_dir, generation_type)\n",
    "        logger.info(\"Storyboard generation completed successfully.\")\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            f\"StoryboardGenerator(script={self.script[:50]}..., \"\n",
    "            f\"characters={list(self.characters.keys())}, \"\n",
    "            f\"style={self.style}, \"\n",
    "            f\"prompt_weights={self.prompt_weights}, \"\n",
    "            f\"temperature={self.temperature}, \"\n",
    "            f\"device={self.device}, \"\n",
    "            f\"seed={self.seed})\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters_dict = {\n",
    "    \"Don Vito Corleone\": {\n",
    "        \"age\": \"early 60s\", \"gender\": \"male\", \"hair\": \"slicked-back gray-black hair\",\n",
    "        \"clothing\": \"dark three-piece suit\",\n",
    "        \"body_type\": \"stocky, slightly hunched posture\",\n",
    "        \"accessories\": \"gold ring on right hand, pocket watch\",\n",
    "        \"ethnicity\": \"Italian-American\"\n",
    "    },\n",
    "    \"Tom Hagen\": {\n",
    "        \"age\": \"early 40s\", \"gender\": \"male\", \"hair\": \"short, neatly combed brown hair\",\n",
    "        \"facial_hair\": \"clean-shaven\", \"clothing\": \"gray suit, dark tie\",\n",
    "        \"body_type\": \"medium build, upright posture\", \"ethnicity\": \"German-Irish\"\n",
    "    },\n",
    "    \"Johnny Fontane\": {\n",
    "        \"age\": \"late 30s\", \"gender\": \"male\", \"hair\": \"short, slicked-back black hair\",\n",
    "        \"facial_hair\": \"clean shaven\", \"clothing\": \"dark, stylish suit with an open collar\",\n",
    "        \"body_type\": \"slim and fit\", \"accessories\": \"gold ring, cigarette\"\n",
    "    },\n",
    "    \"Sonny\": {\n",
    "        \"age\": \"early 30s\", \"gender\": \"male\", \"hair\": \"curly, dark brown hair\",\n",
    "        \"facial_hair\": \"clean-shaven\", \"clothing\": \"formal suit, slightly disheveled\",\n",
    "        \"body_type\": \"athletic build\", \"ethnicity\": \"Italian-American\",\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "script = \"\"\"\n",
    "INT DAY: DON'S OFFICE (SUMMER 1945)\n",
    "\n",
    "\t\t\t\tDON CORLEONE\n",
    "\t\tACT LIKE A MAN!  By Christ in\n",
    "\t\tHeaven, is it possible you turned\n",
    "\t\tout no better than a Hollywood\n",
    "\t\tfinocchio.\n",
    "\n",
    "\tBoth HAGEN and JOHNNY cannot refrain from laughing.  The DON\n",
    "\tsmiles.  SONNY enters as noiselessly as possible, still\n",
    "\tadjusting his clothes.\n",
    "\n",
    "\t\t\t\tDON CORLEONE\n",
    "\t\tAll right, Hollywood...Now tell me\n",
    "\t\tabout this Hollywood Pezzonovanta\n",
    "\t\twho won't let you work.\n",
    "\n",
    "\t\t\t\tJOHNNY\n",
    "\t\tHe owns the studio.  Just a month\n",
    "\t\tago he bought the movie rights to\n",
    "\t\tthis book, a best seller.  And the\n",
    "\t\tmain character is a guy just like\n",
    "\t\tme.  I wouldn't even have to act,\n",
    "\t\tjust be myself.\n",
    "\n",
    "\tThe DON is silent, stern.\n",
    "\n",
    "\t\t\t\tDON CORLEONE\n",
    "\t\tYou take care of your family?\n",
    "\n",
    "\t\t\t\tJOHNNY\n",
    "\t\tSure.\n",
    "\n",
    "\tHe glances at SONNY, who makes himself as inconspicuous as\n",
    "\the can.\n",
    "\n",
    "\t\t\t\tDON CORLEONE\n",
    "\t\tYou look terrible.  I want you to\n",
    "\t\teat well, to rest.  And spend time\n",
    "\t\twith your family.  And then, at the\n",
    "\t\tend of the month, this big shot\n",
    "\t\twill give you the part you want.\n",
    "\n",
    "\t\t\t\tJOHNNY\n",
    "\t\tIt's too late.  All the contracts\n",
    "\t\thave been signed, they're almost\n",
    "\t\tready to shoot.\n",
    "\n",
    "\t\t\t\tDON CORLEONE\n",
    "\t\tI'll make him an offer he can't\n",
    "\t\trefuse.\n",
    "\n",
    "\tHe takes JOHNNY to the door, pinching his cheek hard enough\n",
    "\tto hurt.\n",
    "\n",
    "\t\t\t\tDON CORLEONE\n",
    "\t\tNow go back to the party and leave\n",
    "\t\tit to me.\n",
    "\n",
    "\tHe closes the door, smiling to himself.  Turns to HAGEN.\n",
    "\n",
    "\t\t\t\tDON CORLEONE\n",
    "\t\tWhen does my daughter leave with\n",
    "\t\ther bridegroom?\n",
    "\n",
    "\t\t\t\tHAGEN\n",
    "\t\tThey'll cut the cake in a few\n",
    "\t\tminutes...leave right after that.\n",
    "\t\tYour new son-in-law, do we give him\n",
    "\t\tsomething important?\n",
    "\n",
    "\t\t\t\tDON CORLEONE\n",
    "\t\tNo, give him a living.  But never\n",
    "\t\tlet him know the family's business.\n",
    "\t\tWhat else, Tom?\n",
    "\n",
    "\t\t\t\tHAGEN\n",
    "\t\tI've called the hospital; they've\n",
    "\t\tnotified Consiglere Genco's family\n",
    "\t\tto come and wait.  He won't last\n",
    "\t\tout the night.\n",
    "\n",
    "\tThis saddens the DON.  He sighs.\n",
    "\n",
    "\t\t\t\tDON CORLEONE\n",
    "\t\tGenco will wait for me.  Santino,\n",
    "\t\ttell your brothers they will come\n",
    "\t\twith me to the hospital to see\n",
    "\t\tGenco.  Tell Fredo to drive the big\n",
    "\t\tcar, and ask Johnny to come with us.\n",
    "\n",
    "\t\t\t\tSONNY\n",
    "\t\tAnd Michael?\n",
    "\n",
    "\t\t\t\tDON CORLEONE\n",
    "\t\tAll my sons.\n",
    "\t\t\t  (to HAGEN)\n",
    "\t\tTom, I want you to go to California\n",
    "\t\ttonight.  Make the arrangements.\n",
    "\t\tBut don't leave until I come back\n",
    "\t\tfrom the hospital and speak to you.\n",
    "\t\tUnderstood?\n",
    "\n",
    "\t\t\t\tHAGEN\n",
    "\t\tUnderstood.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = StoryboardGenerator(script, characters_dict, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d3b97ad298e446bb5e21d894cccadf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 01:54:16,828 INFO: Generating formatted prompts...\n",
      "2025-03-23 01:54:34,614 INFO: Formatted prompts generated successfully.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad92be404c964efa9586606f55c11d8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 01:54:55,612 INFO: Image saved to unique\\image_1.png\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (81 > 77). Running this sequence through the model will result in indexing errors\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['to business discussion .']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e4f86e691b24c40849e8f75a36dbbcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 01:55:15,186 INFO: Image saved to unique\\image_2.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed0f83cdd1354406b74906764e2e45d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Potential NSFW content was detected in one or more images. A black image will be returned instead. Try again with a different prompt and/or seed.\n",
      "2025-03-23 01:55:36,702 INFO: Image saved to unique\\image_3.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "235754547706464a8d7d02e6d512a44f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 01:55:58,793 INFO: Image saved to unique\\image_4.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b7c4b2227144249a277ece733211556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 01:56:21,863 INFO: Image saved to unique\\image_5.png\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['instructions about hospital visit .']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6458e002c884545a1f72c4215fbf2af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 01:56:45,684 INFO: Image saved to unique\\image_6.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "350dde810cca44b0abd8baeada7eea0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 01:57:10,200 INFO: Image saved to unique\\image_7.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c028269c4f1643b68d547d3f75f746b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 01:57:34,499 INFO: Image saved to unique\\image_8.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "151c0b723d2b4b4e82c0721a08f82d7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 01:57:59,050 INFO: Image saved to unique\\image_9.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "119a356a86d34273920a99ecd036ed3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 01:58:24,654 INFO: Image saved to unique\\image_10.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65d6e13926f6491482176595e801fd29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 01:58:50,718 INFO: Image saved to unique\\image_11.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4de66a5108644a84abe9ce380de93240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 01:59:17,273 INFO: Image saved to unique\\image_12.png\n",
      "2025-03-23 01:59:17,276 INFO: Prompts saved to unique\\prompts.txt\n",
      "2025-03-23 01:59:17,276 INFO: Storyboard generation completed successfully.\n"
     ]
    }
   ],
   "source": [
    "generator.generate_and_save(save_dir=\"unique\", generation_type=\"unique\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af4f433591c34834b0466ac139b29f22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8578feed747840289a74f3b8e991f27e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 02:00:40,351 INFO: Image saved to prompt_weights\\image_1.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c538fb75cea439bb6114e3cf9fe4b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 02:01:01,686 INFO: Image saved to prompt_weights\\image_2.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2be2b296878749498306465aebd965fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 02:01:24,876 INFO: Image saved to prompt_weights\\image_3.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c7f33c7b2184357a2b2d4314dbb9959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 02:01:49,511 INFO: Image saved to prompt_weights\\image_4.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9ff296c6aad4943b072a27524837cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 02:02:15,246 INFO: Image saved to prompt_weights\\image_5.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14b726e8bd34401385b5932c05f5adad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 02:02:41,154 INFO: Image saved to prompt_weights\\image_6.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5eb097cf40142c6af90ae662ab623fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 02:03:07,764 INFO: Image saved to prompt_weights\\image_7.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20987b566e7244bba1a744169edd7aa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 02:03:34,245 INFO: Image saved to prompt_weights\\image_8.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc5fee66b84e48b2abe7fcef91d0f5fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 02:04:01,476 INFO: Image saved to prompt_weights\\image_9.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0be680d28f24faa9f0e2be2efe34513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 02:04:28,728 INFO: Image saved to prompt_weights\\image_10.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4648b731f6184810a46009b343e99a10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 02:04:56,155 INFO: Image saved to prompt_weights\\image_11.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "454fd4612a8444189c5ff608ecbdeed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 02:05:23,733 INFO: Image saved to prompt_weights\\image_12.png\n",
      "2025-03-23 02:05:23,740 INFO: Prompts saved to prompt_weights\\prompts.txt\n",
      "2025-03-23 02:05:23,741 INFO: Storyboard generation completed successfully.\n"
     ]
    }
   ],
   "source": [
    "generator.generate_and_save(save_dir=\"prompt_weights\", generation_type=\"prompt_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5f3601f6565453d85298a359f45989c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 02:05:34,559 INFO: Generating image for scene 1...\n",
      "2025-03-23 02:07:25,643 INFO: Image saved to modified-cfg\\image_1.png\n",
      "2025-03-23 02:07:25,828 INFO: Generating image for scene 2...\n",
      "2025-03-23 02:09:21,488 INFO: Image saved to modified-cfg\\image_2.png\n",
      "2025-03-23 02:09:21,714 INFO: Generating image for scene 3...\n",
      "2025-03-23 02:10:52,393 INFO: Image saved to modified-cfg\\image_3.png\n",
      "2025-03-23 02:10:52,557 INFO: Generating image for scene 4...\n",
      "2025-03-23 02:12:30,974 INFO: Image saved to modified-cfg\\image_4.png\n",
      "2025-03-23 02:12:31,152 INFO: Generating image for scene 5...\n",
      "2025-03-23 02:14:14,676 INFO: Image saved to modified-cfg\\image_5.png\n",
      "2025-03-23 02:14:14,850 INFO: Generating image for scene 6...\n",
      "2025-03-23 02:16:10,601 INFO: Image saved to modified-cfg\\image_6.png\n",
      "2025-03-23 02:16:10,773 INFO: Generating image for scene 7...\n",
      "2025-03-23 02:17:55,524 INFO: Image saved to modified-cfg\\image_7.png\n",
      "2025-03-23 02:17:55,681 INFO: Generating image for scene 8...\n",
      "2025-03-23 02:19:23,041 INFO: Image saved to modified-cfg\\image_8.png\n",
      "2025-03-23 02:19:23,201 INFO: Generating image for scene 9...\n",
      "2025-03-23 02:21:04,454 INFO: Image saved to modified-cfg\\image_9.png\n",
      "2025-03-23 02:21:04,701 INFO: Generating image for scene 10...\n",
      "2025-03-23 02:23:00,440 INFO: Image saved to modified-cfg\\image_10.png\n",
      "2025-03-23 02:23:00,603 INFO: Generating image for scene 11...\n",
      "2025-03-23 02:24:42,949 INFO: Image saved to modified-cfg\\image_11.png\n",
      "2025-03-23 02:24:43,100 INFO: Generating image for scene 12...\n",
      "2025-03-23 02:26:09,635 INFO: Image saved to modified-cfg\\image_12.png\n",
      "2025-03-23 02:26:09,644 INFO: Prompts saved to modified-cfg\\prompts.txt\n",
      "2025-03-23 02:26:09,645 INFO: Storyboard generation completed successfully.\n"
     ]
    }
   ],
   "source": [
    "generator.generate_and_save(save_dir=\"modified-cfg\", generation_type=\"modified-cfg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dotenv import load_dotenv\n",
    "# import json\n",
    "# from together import Together\n",
    "# from pydantic import BaseModel\n",
    "# from typing import List, Dict\n",
    "# load_dotenv()\n",
    "# together = Together() # add .env file with TOGETHER_API_KEY variable\n",
    "# ORIENTATIONS = [\n",
    "#     \"Front View\", \"Profile View\", \"Back View\", \"From Behind\", \"From Above\",\n",
    "#     \"From Below\", \"Three-Quarters View\", \"Long Shot\", \"Three-Quarters Rear View\"\n",
    "# ]\n",
    "\n",
    "# CAMERA_SHOTS = [\n",
    "#     \"Aerial View\", \"Birdâ€™s-Eye View\", \"Close-Up\", \"Cowboy Shot\", \"Dolly Zoom\",\n",
    "#     \"Dutch Angle\", \"Establishing Shot\", \"Extreme Close-Up\", \"Extreme Long Shot\",\n",
    "#     \"Full Shot\", \"Long Shot\", \"Medium Close-Up\", \"Medium Long Shot\", \"Medium Shot\",\n",
    "#     \"Over-the-Shoulder Shot\", \"Point-of-View Shot\", \"Two-Shot\", \"Fisheye Shot\",\n",
    "#     \"Worm's Eye\", \"Low-Angle Shot\", \"Macro Shot\", \"Tilt-Shift Shot\", \"Telephoto Shot\"\n",
    "# ]\n",
    "\n",
    "# class Character(BaseModel):\n",
    "#     name: str\n",
    "    \n",
    "# class Scene(BaseModel):\n",
    "#     scene_number: int\n",
    "#     shot_type: str\n",
    "#     orientation: str\n",
    "#     characters: List[Character]\n",
    "#     environment: str\n",
    "#     description: str\n",
    "    \n",
    "# class SceneList(BaseModel):\n",
    "#     scenes: List[Scene]\n",
    "\n",
    "# def _build_character_description(characters_dict: Dict[str, str]):\n",
    "#     \"\"\"\n",
    "#     Generates text description of character from the dictionary\n",
    "#     \"\"\"\n",
    "#     features = [\n",
    "#         characters_dict.get(\"ethnicity\", \"\"),\n",
    "#         characters_dict.get(\"age\", \"\"),\n",
    "#         characters_dict.get(\"gender\", \"\"),\n",
    "#         characters_dict.get(\"hair\", \"\"),\n",
    "#         characters_dict.get(\"facial_hair\", \"\"),\n",
    "#         characters_dict.get(\"body_type\", \"\"),\n",
    "#         f\"wearing {characters_dict.get('clothing', '')}\",\n",
    "#         f\"with {characters_dict.get('accessories', '')}\" if characters_dict.get(\"accessories\") else \"\"\n",
    "#     ]\n",
    "    \n",
    "#     return \", \".join(filter(None, features))\n",
    "\n",
    "# def input_to_json(script: str, characters: Dict[str, Dict], temperature: int = 0.7):\n",
    "#     \"\"\"\n",
    "#     Converts a script and character descriptions into a JSON format for storyboard generation.\n",
    "#     \"\"\"\n",
    "#     character_descriptions = {name: _build_character_description(desc) for name, desc in characters.items()}\n",
    "    \n",
    "#     script_section = f\"Here is the film script: \\n{script}\"\n",
    "#     characters_section = f\"The characters in the script have the following descriptions: \\n{json.dumps(character_descriptions, indent=2)}\"\n",
    "    \n",
    "#     instructions = \"\"\"\n",
    "# ### Storyboard Generation Instructions\n",
    "# 1. **Number of Scenes**: Divide the entire script into a reasonable number of scenes (typically between 4 to 7 scenes), not too many or too few.\n",
    "# 2. **Single Distinct Moment**: Each scene captures a single moment.\n",
    "# 3. **Camera Angles & Orientation**: Choose from these shot types: {', '.join(self.CAMERA_SHOTS)}.  \n",
    "# Choose from these orientations: {', '.join(self.ORIENTATIONS)}.\n",
    "# 4. **Location & Time**: Clearly derive environment from the script (e.g. INT DAY, DON'S OFFICE, etc.). Describe it in its details (size, lighhting, mood, organization of the objects, etc.). Notice that if it's the same across the different scenes, it must be written in the same way\n",
    "# 5. **Characters**:\n",
    "# - List only characters relevant to the single moment in each scene.\n",
    "# - Each character must have the name and a short description (consistent from provided descriptions).\n",
    "# 6. Clearly describe the scene including actions, character positions (foreground, background, left, right), emotions, and expressions.\n",
    "# 7. **Scene Format**: Return JSON with a key 'scenes' as an array of structured objects:\n",
    "# - \"scene_number\": integer\n",
    "# - \"shot_type\": camera shot type (from provided list) \n",
    "# - \"orientation\": orientation (from provided list)\n",
    "# - \"characters\": list of objects with:\n",
    "#         - \"name\": character's name, not as they appear on the script but as they were given to you in the description.\n",
    "# - \"environment\": short description of the location\n",
    "# - \"description\": short, vivid description focusing on actions, expressions, emotions of each single character. Also their relative position is clearly described. The description must be succint, without extra articles or words, it should be visual and useful for an image generation prompt. Ensure it makes sense with the shot type (e.g., if it's medium shot, don't say that the face is covering the full image, otherwise it should be a close up). Don't write the words they say, since they occupy tokens, unless it's a fundamental part of the script. Avoid useless adjetives or adverbs, be concise and clear.\n",
    "\n",
    "# Follow the above instructions very carefully. Notice that the scenes have no knowledge of each other's contents. So in case something is necessary, describe it again. \n",
    "# \"\"\"\n",
    "\n",
    "#     example_input = \"\"\"\n",
    "# ### Example\n",
    "# Input: \n",
    "# - Script is \n",
    "# INT DAY: DON'S OFFICE (SUMMER 1945)\n",
    "\n",
    "#         DON CORLEONE\n",
    "# ACT LIKE A MAN!  By Christ in\n",
    "# Heaven, is it possible you turned\n",
    "# out no better than a Hollywood\n",
    "# finocchio.\n",
    "\n",
    "# Both HAGEN and JOHNNY cannot refrain from laughing.  The DON\n",
    "# smiles.  SONNY enters as noiselessly as possible, still\n",
    "# adjusting his clothes.\n",
    "\n",
    "#         DON CORLEONE\n",
    "# All right, Hollywood...Now tell me\n",
    "# about this Hollywood Pezzonovanta\n",
    "# who won't let you work.\n",
    "\n",
    "#         JOHNNY\n",
    "# He owns the studio.  Just a month\n",
    "# ago he bought the movie rights to\n",
    "# this book, a best seller.  And the\n",
    "# main character is a guy just like\n",
    "# me.  I wouldn't even have to act,\n",
    "# just be myself.\n",
    "\n",
    "# The DON is silent, stern.\n",
    "\n",
    "#         DON CORLEONE\n",
    "# You take care of your family?\n",
    "\n",
    "#         JOHNNY\n",
    "# Sure.\n",
    "\n",
    "# He glances at SONNY, who makes himself as inconspicuous as\n",
    "# he can.\n",
    "\n",
    "#         DON CORLEONE\n",
    "# You look terrible.  I want you to\n",
    "# eat well, to rest.  And spend time\n",
    "# with your family.  And then, at the\n",
    "# end of the month, this big shot\n",
    "# will give you the part you want.\n",
    "\n",
    "#         JOHNNY\n",
    "# It's too late.  All the contracts\n",
    "# have been signed, they're almost\n",
    "# ready to shoot.\n",
    "\n",
    "#         DON CORLEONE\n",
    "# I'll make him an offer he can't\n",
    "# refuse.\n",
    "\n",
    "# He takes JOHNNY to the door, pinching his cheek hard enough\n",
    "# to hurt.\n",
    "\n",
    "#         DON CORLEONE\n",
    "# Now go back to the party and leave\n",
    "# it to me.\n",
    "\n",
    "# He closes the door, smiling to himself.  Turns to HAGEN.\n",
    "\n",
    "#         DON CORLEONE\n",
    "# When does my daughter leave with\n",
    "# her bridegroom?\n",
    "\n",
    "#         HAGEN\n",
    "# They'll cut the cake in a few\n",
    "# minutes...leave right after that.\n",
    "# Your new son-in-law, do we give him\n",
    "# something important?\n",
    "\n",
    "#         DON CORLEONE\n",
    "# No, give him a living.  But never\n",
    "# let him know the family's business.\n",
    "# What else, Tom?\n",
    "\n",
    "#         HAGEN\n",
    "# I've called the hospital; they've\n",
    "# notified Consigliere Genco's family\n",
    "# to come and wait.  He won't last\n",
    "# out the night.\n",
    "\n",
    "# This saddens the DON.  He sighs.\n",
    "\n",
    "#         DON CORLEONE\n",
    "# Genco will wait for me.  Santino,\n",
    "# tell your brothers they will come\n",
    "# with me to the hospital to see\n",
    "# Genco.  Tell Fredo to drive the big\n",
    "# car, and ask Johnny to come with us.\n",
    "\n",
    "#         SONNY\n",
    "# And Michael?\n",
    "\n",
    "#         DON CORLEONE\n",
    "# All my sons.\n",
    "#         (to HAGEN)\n",
    "# Tom, I want you to go to California\n",
    "# tonight.  Make the arrangements.\n",
    "# But don't leave until I come back\n",
    "# from the hospital and speak to you.\n",
    "# Understood?\n",
    "\n",
    "#         HAGEN\n",
    "# Understood.\n",
    "\n",
    "# - Characters description from the dictionary gives\n",
    "#         - Don Vito Corleone: 'Italian-American, early 60s, male, slicked-back gray-black hair, stocky, slightly hunched posture, wearing dark three-piece suit, with gold ring on right hand, pocket watch'\n",
    "#         - Johnny Fontane: 'late 30s, male, short, slicked-back black hair, clean shaven, slim and fit, wearing dark, stylish suit with an open collar, with gold ring, cigarette'\n",
    "#         - Tom Hagen: 'German-Irish, early 40s, male, short, neatly combed brown hair, clean-shaven, medium build, upright posture, wearing gray suit, dark tie'\n",
    "#         - Sonny: 'Italian-American, early 30s, male, curly, dark brown hair, clean-shaven, athletic build, wearing formal suit, slightly disheveled'\n",
    "# \"\"\"\n",
    "\n",
    "#     example_output = \"\"\"\n",
    "# Example Output:\n",
    "# {\n",
    "#    \"scenes\": [\n",
    "#    {\n",
    "#    \"scene_number\": 1,\n",
    "#    \"shot_type\": \"Medium Shot\",\n",
    "#    \"orientation\": \"Front View\",\n",
    "#    \"characters\": [\n",
    "#            {\n",
    "#            \"name\": \"Don Vito Corleone\"\n",
    "#            },\n",
    "#            {\n",
    "#            \"name\": \"Johnny Fontane\"\n",
    "#            },\n",
    "#            {\n",
    "#            \"name\": \"Tom Hagen\"\n",
    "#            }\n",
    "#    ],\n",
    "#    \"environment\": \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
    "#    \"description\": \"Don Corleone stands imposingly behind desk, face stern with righteous anger, pointing finger at Johnny. Johnny appears embarrassed, head slightly bowed. Hagen stands to the right, barely containing laughter. Tension and amusement mix in intimate office atmosphere.\"\n",
    "#    },\n",
    "#    {\n",
    "#    \"scene_number\": 2,\n",
    "#    \"shot_type\": \"Two-Shot\",\n",
    "#    \"orientation\": \"Profile View\",\n",
    "#    \"characters\": [\n",
    "#            {\n",
    "#            \"name\": \"Don Vito Corleone\"\n",
    "#            },\n",
    "#            {\n",
    "#            \"name\": \"Johnny Fontane\"\n",
    "#            },\n",
    "#            {\n",
    "#            \"name\": \"Tom Hagen\"\n",
    "#            },\n",
    "#            {\n",
    "#            \"name\": \"Sonny\"\n",
    "#            }\n",
    "#    ],\n",
    "#    \"environment\": \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
    "#    \"description\": \"Sonny quietly enters room from right, adjusting disheveled clothes. Don leans forward at desk, expression softening to business-like focus. Johnny stands center, straightening posture. Hagen observes from left corner. Atmosphere shifts from personal rebuke to business discussion.\"\n",
    "#    },\n",
    "#    {\n",
    "#    \"scene_number\": 3,\n",
    "#    \"shot_type\": \"Close-Up\",\n",
    "#    \"orientation\": \"Front View\",\n",
    "#    \"characters\": [\n",
    "#            {\n",
    "#            \"name\": \"Don Vito Corleone\"\n",
    "#            }\n",
    "#    ],\n",
    "#    \"environment\": \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
    "#    \"description\": \"Don Corleone's face fills frame, stern and contemplative. Eyes narrowed, jaw set firmly. Saying 'I'll make him an offer he can't refuse' with quiet, confident menace. Power and authority emanate from his expression.\"\n",
    "#    },\n",
    "#    {\n",
    "#    \"scene_number\": 4,\n",
    "#    \"shot_type\": \"Medium Close-Up\",\n",
    "#    \"orientation\": \"Three-Quarters View\",\n",
    "#    \"characters\": [\n",
    "#            {\n",
    "#            \"name\": \"Don Vito Corleone\"\n",
    "#            },\n",
    "#            {\n",
    "#            \"name\": \"Johnny Fontane\"\n",
    "#            }\n",
    "#    ],\n",
    "#    \"environment\": \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
    "#    \"description\": \"Don Corleone escorts Johnny to door, pinching his cheek firmly. Don's expression shows affection mixed with dominance. Johnny winces slightly at pain while showing relief and gratitude. Door frame visible on right edge of shot.\"\n",
    "#    },\n",
    "#    {\n",
    "#    \"scene_number\": 5,\n",
    "#    \"shot_type\": \"Medium Shot\",\n",
    "#    \"orientation\": \"Front View\",\n",
    "#    \"characters\": [\n",
    "#            {\n",
    "#            \"name\": \"Don Vito Corleone\"\n",
    "#            },\n",
    "#            {\n",
    "#            \"name\": \"Tom Hagen\"\n",
    "#            }\n",
    "#    ],\n",
    "#    \"environment\": \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
    "#    \"description\": \"Don Corleone turns from closed door, small smile fading to serious business expression. Hagen stands attentively near desk, notepad ready. Don moves toward chair, shoulders slightly hunched, gold ring catching light as he gestures.\"\n",
    "#    },\n",
    "#    {\n",
    "#    \"scene_number\": 6,\n",
    "#    \"shot_type\": \"Over-the-Shoulder Shot\",\n",
    "#    \"orientation\": \"Profile View\",\n",
    "#    \"characters\": [\n",
    "#            {\n",
    "#            \"name\": \"Don Vito Corleone\"\n",
    "#            },\n",
    "#            {\n",
    "#            \"name\": \"Tom Hagen\"\n",
    "#            },\n",
    "#            {\n",
    "#            \"name\": \"Sonny\"\n",
    "#            }\n",
    "#    ],\n",
    "#    \"environment\": \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
    "#    \"description\": \"Camera over Don's shoulder, facing Hagen and Sonny. Don's gray-black hair and dark suit visible in foreground. Hagen's face shows respectful attention. Sonny stands beside him, now composed. Don's voice carries weight as he issues final instructions about hospital visit.\"\n",
    "#    }\n",
    "#    ]\n",
    "# }\n",
    "# \"\"\"\n",
    "\n",
    "#     # Combine all content together without nesting f-strings\n",
    "#     user_content = f\"{script_section}\\n\\n{characters_section}\\n\\n{instructions}\\n{example_input}\\n{example_output}\"\n",
    "    \n",
    "#     messages = [\n",
    "#         {\"role\": \"system\", \"content\": (\n",
    "#             \"You are an AI specialized in creating structured storyboard scenes from a film script \"\n",
    "#             \"for image generation (e.g., stable diffusion). Each scene must capture a single distinct moment, \"\n",
    "#             \"should list relevant characters with consistent appearances, specify the environment, camera shot, \"\n",
    "#             \"and orientation, and provide direct clues for a diffusion model to generate images.\"\n",
    "#             )},\n",
    "#         {\"role\": \"user\", \"content\": user_content}\n",
    "#     ]\n",
    "    \n",
    "#     response = together.chat.completions.create(\n",
    "#         model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "#         messages=messages,\n",
    "#         max_tokens=10000,\n",
    "#         temperature=temperature,\n",
    "#         response_format={\"type\": \"json_object\", \"schema\": SceneList.model_json_schema()}\n",
    "#     )\n",
    "\n",
    "#     try:\n",
    "#         output_json = response.choices[0].message.content\n",
    "#         return json.loads(output_json)[\"scenes\"]\n",
    "#     except (json.JSONDecodeError, KeyError) as e:\n",
    "#         print(\"Error parsing JSON output:\", e)\n",
    "#         return []\n",
    "\n",
    "# scenes = input_to_json(script, characters_dict)\n",
    "# print(json.dumps(scenes, indent=4))\n",
    "\n",
    "# def scenes_to_formatted_prompts(scenes, characters_dict, style=\"storyboard\", prompt_weights=[2, 1.0, 1.2, 1.5, 0.9]):\n",
    "#     \"\"\"\n",
    "#     Converts a list of scenes into structured diffusion model prompts with weights in one pass,\n",
    "#     with a fallback for character names not matching exactly.\n",
    "    \n",
    "#     Parameters:\n",
    "#     - scenes (list): List of scene dictionaries.\n",
    "#     - characters_dict (dict): Dictionary with character details.\n",
    "#     - style (str): Artistic style string (default \"storyboard\").\n",
    "#     - prompt_weights (list): Weights for \"style\", \"environment\", \"shot\", \"description\", and characters.\n",
    "    \n",
    "#     Returns:\n",
    "#     - List of tuples (subprompt_texts, subprompt_weights) for each scene.\n",
    "#     \"\"\"\n",
    "#     # Define weight mapping for non-character keys\n",
    "#     weight_map = {\n",
    "#         \"style\": prompt_weights[0],\n",
    "#         \"environment\": prompt_weights[1],\n",
    "#         \"shot\": prompt_weights[2],\n",
    "#         \"description\": prompt_weights[3]\n",
    "#     }\n",
    "#     character_weight = prompt_weights[4]\n",
    "\n",
    "#     # Determine style string based on input style\n",
    "#     if style == \"storyboard\":\n",
    "#         style_value = \"rough b&w pencil sketch, simple sketch lines, minimal shading, rough hatching, draft-style, J.C. Leyendecker style\"\n",
    "#     else:\n",
    "#         style_value = style\n",
    "\n",
    "#     formatted_results = []\n",
    "\n",
    "#     for scene in scenes:\n",
    "#         subprompts = {}\n",
    "\n",
    "#         # Add each character's prompt with fallback handling\n",
    "#         for i, char in enumerate(scene[\"characters\"]):\n",
    "#             char_name = char[\"name\"]\n",
    "#             char_info = characters_dict.get(char_name)\n",
    "            \n",
    "#             # If not found, try to find a key that contains the given name as a substring\n",
    "#             if not char_info:\n",
    "#                 matching_keys = [key for key in characters_dict if char_name in key]\n",
    "#                 if matching_keys:\n",
    "#                     char_info = characters_dict[matching_keys[0]]\n",
    "#                 else:\n",
    "#                     # Provide a default description if still not found\n",
    "#                     char_info = {\"age\": \"unknown\", \"gender\": \"unknown\", \"hair\": \"unknown\",\n",
    "#                                  \"clothing\": \"unknown\", \"body_type\": \"unknown\"}\n",
    "            \n",
    "#             char_desc = _build_character_description(char_info)\n",
    "#             subprompts[f\"character{i+1}\"] = f\"{char_name}: {char_desc}\"\n",
    "        \n",
    "#         # Add other scene details\n",
    "#         subprompts[\"style\"] = style_value\n",
    "#         subprompts[\"environment\"] = scene[\"environment\"]\n",
    "#         subprompts[\"shot\"] = f\"{scene['shot_type']}, {scene['orientation']}\"\n",
    "#         subprompts[\"description\"] = scene[\"description\"]\n",
    "\n",
    "#         # Prepare lists for texts and weights\n",
    "#         subprompt_texts = []\n",
    "#         subprompt_weights = []\n",
    "#         for key, text in subprompts.items():\n",
    "#             subprompt_texts.append(text)\n",
    "#             if key.startswith(\"character\"):\n",
    "#                 subprompt_weights.append(character_weight)\n",
    "#             else:\n",
    "#                 subprompt_weights.append(weight_map.get(key, 1.0))\n",
    "        \n",
    "#         formatted_results.append((subprompt_texts, subprompt_weights))\n",
    "    \n",
    "#     return formatted_results\n",
    "\n",
    "# formatted_prompts = scenes_to_formatted_prompts(scenes, characters_dict)\n",
    "# print(formatted_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unique Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# from diffusers import StableDiffusionPipeline, UniPCMultistepScheduler\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16)\n",
    "# pipe = pipe.to(device)\n",
    "# pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "# pipe.enable_model_cpu_offload()\n",
    "# pipe.enable_attention_slicing()\n",
    "\n",
    "# def build_unique_prompts(formatted_prompts, style_override=\"rough b&w simple pencil sketch, J.C. Leyendecker style,\"):\n",
    "#     \"\"\"\n",
    "#     Given formatted prompts (a list of tuples where each tuple is \n",
    "#     (subprompt_texts, subprompt_weights)), build a unique prompt for each scene\n",
    "#     by concatenating the style, shot prompt, and description.\n",
    "    \n",
    "#     Returns:\n",
    "#         List[str]: Unique prompt strings for each scene.\n",
    "#     \"\"\"\n",
    "#     unique_prompts = []\n",
    "#     for subprompt_texts, _ in formatted_prompts:\n",
    "#         shot_prompt = subprompt_texts[-2]  \n",
    "#         description = subprompt_texts[-1]\n",
    "#         unique_prompt = f\"{style_override} {shot_prompt}: {description}\"\n",
    "#         unique_prompts.append(unique_prompt)\n",
    "#     return unique_prompts\n",
    "\n",
    "# def generate_and_save_images_unique_prompts(formatted_prompts, pipe, save_dir, device,\n",
    "#                                             negative_prompt=\"low quality, photorealistic, 3d render, overly detailed, digital art, painting, vibrant colors, fine art, NSFW\",\n",
    "#                                             num_inference_steps=50):\n",
    "#     \"\"\"\n",
    "#     Generate images using unique prompts built from formatted_prompts.\n",
    "#     \"\"\"\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "#     unique_prompts = build_unique_prompts(formatted_prompts)\n",
    "#     generated_images = []\n",
    "    \n",
    "#     for i, unique_prompt in enumerate(unique_prompts):\n",
    "#         with torch.no_grad():\n",
    "#             output = pipe(prompt=unique_prompt,\n",
    "#                           negative_prompt=negative_prompt,\n",
    "#                           num_inference_steps=num_inference_steps)\n",
    "#         generated_image = output.images[0]\n",
    "#         generated_images.append(generated_image)\n",
    "#         image_path = os.path.join(save_dir, f\"image_{i+1}.png\")\n",
    "#         generated_image.save(image_path)\n",
    "#         print(f\"Image {i+1} saved to {image_path}\")\n",
    "    \n",
    "#     return generated_images\n",
    "\n",
    "# save_directory = \"stories/unique_prompts\"\n",
    "# generated_images = generate_and_save_images_unique_prompts(formatted_prompts, pipe, save_directory, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subprompt Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# from diffusers import StableDiffusionPipeline, UniPCMultistepScheduler\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16)\n",
    "# pipe = pipe.to(device)\n",
    "# pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "# pipe.enable_model_cpu_offload()\n",
    "# pipe.enable_attention_slicing()\n",
    "\n",
    "# def weighted_sum_prompt_embeddings(pipe, subprompt_texts, subprompt_weights, device, num_images_per_prompt=1):\n",
    "#     \"\"\"\n",
    "#     Computes a weighted sum of text embeddings for a list of subprompts.\n",
    "    \n",
    "#     Args:\n",
    "#         pipe: The Stable Diffusion pipeline instance.\n",
    "#         subprompt_texts (List[str]): List of subprompt strings.\n",
    "#         subprompt_weights (List[float]): Corresponding weights for each subprompt.\n",
    "#         device (str): The device to run on (\"cuda\" or \"cpu\").\n",
    "#         num_images_per_prompt (int): Number of images to generate per prompt.\n",
    "    \n",
    "#     Returns:\n",
    "#         torch.Tensor: Combined prompt embeddings of shape (batch_size * num_images_per_prompt, seq_len, embed_dim)\n",
    "#     \"\"\"\n",
    "#     encoded_prompts = []\n",
    "#     for text in subprompt_texts:\n",
    "#         # Tokenize the subprompt text\n",
    "#         text_inputs = pipe.tokenizer(\n",
    "#             text,\n",
    "#             padding=\"max_length\",\n",
    "#             max_length=pipe.tokenizer.model_max_length,\n",
    "#             truncation=True,\n",
    "#             return_tensors=\"pt\",\n",
    "#         )\n",
    "#         input_ids = text_inputs.input_ids.to(device)\n",
    "#         attention_mask = text_inputs.attention_mask.to(device) if \"attention_mask\" in text_inputs else None\n",
    "\n",
    "#         # Encode the subprompt into text embeddings\n",
    "#         text_embeds = pipe.text_encoder(input_ids, attention_mask=attention_mask)[0]\n",
    "#         encoded_prompts.append(text_embeds)\n",
    "    \n",
    "#     # Compute the weighted sum of the embeddings\n",
    "#     weighted_embedding = sum(weight * embeds for weight, embeds in zip(subprompt_weights, encoded_prompts))\n",
    "#     weight_total = sum(subprompt_weights)\n",
    "#     combined_embedding = weighted_embedding / weight_total  # Normalize if desired\n",
    "\n",
    "#     # Duplicate embeddings for each image per prompt if necessary\n",
    "#     batch_size, seq_len, embed_dim = combined_embedding.shape\n",
    "#     combined_embedding = combined_embedding.repeat(1, num_images_per_prompt, 1)\n",
    "#     combined_embedding = combined_embedding.view(batch_size * num_images_per_prompt, seq_len, embed_dim)\n",
    "    \n",
    "#     return combined_embedding\n",
    "\n",
    "# def generate_and_save_images_prompt_weights(scenes, characters_dict, pipe, save_dir, device,\n",
    "#                              negative_prompt=\"low quality, photorealistic, 3d render, overly detailed, digital art, painting, vibrant colors, fine art, NSFW\",\n",
    "#                              num_inference_steps=50):\n",
    "#     \"\"\"\n",
    "#     Generate images for each scene using prompt embeddings from the provided pipeline\n",
    "#     and save each image to the specified directory with a unique filename.\n",
    "\n",
    "#     Args:\n",
    "#         scenes (list): List of scene objects.\n",
    "#         characters_dict (dict): Dictionary of character descriptions.\n",
    "#         pipe: The Stable Diffusion pipeline instance.\n",
    "#         save_dir (str): The directory where images will be saved.\n",
    "#         device (str): The device to use (\"cuda\" or \"cpu\").\n",
    "#         negative_prompt (str, optional): Negative prompt to steer generation.\n",
    "#         num_inference_steps (int, optional): Number of inference steps for image generation.\n",
    "\n",
    "#     Returns:\n",
    "#         list: List of generated PIL.Image objects.\n",
    "#     \"\"\"\n",
    "#     print(\"Generating images...\")\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "#     generated_images = []\n",
    "#     for i, scene in enumerate(scenes):\n",
    "#         # scene_prompts = _scene_to_prompts(scene, characters_dict)\n",
    "#         # subprompt_texts, subprompt_weights = format_subprompts_for_diffusion(scene_prompts)\n",
    "#         subprompt_texts, subprompt_weights = scenes_to_formatted_prompts([scene], characters_dict)[0]\n",
    "#         combined_embeddings = weighted_sum_prompt_embeddings(pipe, subprompt_texts, subprompt_weights, device)\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             output = pipe(prompt_embeds=combined_embeddings,\n",
    "#                           negative_prompt=negative_prompt,\n",
    "#                           num_inference_steps=num_inference_steps)\n",
    "#         generated_image = output.images[0]\n",
    "#         generated_images.append(generated_image)\n",
    "#         image_path = os.path.join(save_dir, f\"image_{i+1}.png\")\n",
    "#         generated_image.save(image_path)\n",
    "#         print(f\"Image {i+1} saved to {image_path}\")\n",
    "        \n",
    "#     return generated_images\n",
    "\n",
    "# save_directory = \"stories/prompt_weight\"\n",
    "# generated_images = generate_and_save_images_prompt_weights(scenes, characters_dict, pipe, save_directory, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified Classifier-Free Guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# from diffusers import StableDiffusionPipeline, UniPCMultistepScheduler\n",
    "\n",
    "# def encode_subprompt(pipe: StableDiffusionPipeline, text: str, device: str = \"cuda\"):\n",
    "#     \"\"\"\n",
    "#     Tokenize and encode a single subprompt into a [batch_size=1, seq_len, hidden_dim] embedding.\n",
    "#     \"\"\"\n",
    "#     text_inputs = pipe.tokenizer(\n",
    "#         text,\n",
    "#         padding=\"max_length\",\n",
    "#         max_length=pipe.tokenizer.model_max_length,\n",
    "#         truncation=True,\n",
    "#         return_tensors=\"pt\",\n",
    "#     )\n",
    "#     text_embeds = pipe.text_encoder(\n",
    "#         text_inputs.input_ids.to(device),\n",
    "#         attention_mask=text_inputs.attention_mask.to(device)\n",
    "#     )[0]\n",
    "#     return text_embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Unconditional Pass + Multiple Conditional Passes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\hat{\\epsilon}_{\\text{cond\\_combined}}=\\frac{1}{\\sum_{i=1}^nw_i}\\sum_{i=1}^nw_i\\hat{\\epsilon}_{\\text{cond}_i}$\n",
    "where we have one pass per subprompt to get $\\hat{\\epsilon}_{\\text{cond}_i}$ and $n$ is the number of subprompts.\n",
    "Then the classifier free guidance with scale $g$ is $$\\hat{\\epsilon}=\\hat{\\epsilon}_{\\text{uncond}}+g(\\hat{\\epsilon}_{\\text{cond\\_combined}}-\\hat{\\epsilon}_{\\text{uncond}})$$\n",
    "where we have one unconditional pass at each step to get $\\hat{\\epsilon}_{\\text{uncond}}$\n",
    "\n",
    "- Total UNet calls per step: $1+n$\n",
    "- Each subprompt has a relative weight but they all share the same baseline unconditional pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiPromptPipelineApproach1(StableDiffusionPipeline):\n",
    "#     \"\"\"\n",
    "#     Multi-Prompt CFG with a SINGLE unconditional pass:\n",
    "#       - At each diffusion step:\n",
    "#         1. uncond_out = UNet(latent, uncond_embeds)\n",
    "#         2. cond_out_i = UNet(latent, cond_embeds_i) for each subprompt i\n",
    "#         3. cond_combined = weighted average of all cond_out_i\n",
    "#         4. final_out = uncond_out + guidance_scale*(cond_combined - uncond_out)\n",
    "#     \"\"\"\n",
    "\n",
    "#     @torch.no_grad()\n",
    "#     def __call__(\n",
    "#         self,\n",
    "#         subprompt_embeds: list[torch.Tensor],\n",
    "#         subprompt_weights: list[float],\n",
    "#         uncond_embeds: torch.Tensor,\n",
    "#         height: int = 512,\n",
    "#         width: int = 512,\n",
    "#         guidance_scale: float = 7.5,\n",
    "#         num_inference_steps: int = 50,\n",
    "#         generator: torch.Generator = None,\n",
    "#         latents: torch.Tensor = None,\n",
    "#         output_type: str = \"pil\",\n",
    "#         return_dict: bool = True,\n",
    "#         **kwargs\n",
    "#     ):\n",
    "#         device = self._execution_device\n",
    "#         batch_size = uncond_embeds.shape[0]\n",
    "#         num_subprompts = len(subprompt_embeds)\n",
    "\n",
    "#         if num_subprompts != len(subprompt_weights):\n",
    "#             raise ValueError(\"subprompt_embeds and subprompt_weights must have the same length.\")\n",
    "\n",
    "#         # 1. Validate or fallback to default height/width\n",
    "#         if not height or not width:\n",
    "#             height, width = self._default_height_width()\n",
    "\n",
    "#         # 2. Set timesteps on the scheduler\n",
    "#         self.scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "#         timesteps = self.scheduler.timesteps\n",
    "\n",
    "#         # 3. Prepare latents\n",
    "#         if latents is None:\n",
    "#             shape = (batch_size, self.unet.config.in_channels, height // 8, width // 8)\n",
    "#             latents = torch.randn(shape, generator=generator, device=device, dtype=uncond_embeds.dtype)\n",
    "#             latents = latents * self.scheduler.init_noise_sigma\n",
    "#         else:\n",
    "#             latents = latents.to(device)\n",
    "\n",
    "#         # 4. Diffusion loop\n",
    "#         for i, t in enumerate(timesteps):\n",
    "#             latent_model_input = self.scheduler.scale_model_input(latents, t)\n",
    "\n",
    "#             # (A) Unconditional pass\n",
    "#             uncond_out = self.unet(latent_model_input, t, encoder_hidden_states=uncond_embeds, **kwargs).sample\n",
    "\n",
    "#             # (B) Conditional passes (one per subprompt)\n",
    "#             cond_outs = []\n",
    "#             for cond_embed in subprompt_embeds:\n",
    "#                 out = self.unet(latent_model_input, t, encoder_hidden_states=cond_embed, **kwargs).sample\n",
    "#                 cond_outs.append(out)\n",
    "\n",
    "#             # (C) Weighted average of conditional outputs\n",
    "#             total_w = sum(subprompt_weights)\n",
    "#             cond_combined = sum(w * o for w, o in zip(subprompt_weights, cond_outs)) / total_w\n",
    "\n",
    "#             # (D) Classifier-Free Guidance\n",
    "#             guided_out = uncond_out + guidance_scale * (cond_combined - uncond_out)\n",
    "\n",
    "#             # (E) Step\n",
    "#             latents = self.scheduler.step(guided_out, t, latents, **kwargs).prev_sample\n",
    "\n",
    "#         # 5. Decode latents\n",
    "#         if output_type == \"latent\":\n",
    "#             if return_dict:\n",
    "#                 from diffusers.pipelines.stable_diffusion.pipeline_output import StableDiffusionPipelineOutput\n",
    "#                 return StableDiffusionPipelineOutput(images=latents, nsfw_content_detected=None)\n",
    "#             return latents\n",
    "\n",
    "#         image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=False)[0]\n",
    "#         image = self.image_processor.postprocess(image, output_type=output_type)\n",
    "\n",
    "#         if return_dict:\n",
    "#             from diffusers.pipelines.stable_diffusion.pipeline_output import StableDiffusionPipelineOutput\n",
    "#             return StableDiffusionPipelineOutput(images=image, nsfw_content_detected=None)\n",
    "#         return image\n",
    "\n",
    "# # --- Load the Multi-Prompt Approach 1 pipeline ---\n",
    "# print(\"Loading Approach 1 pipeline for scenes...\")\n",
    "# pipe1 = MultiPromptPipelineApproach1.from_pretrained(\n",
    "#     \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16\n",
    "# ).to(\"cuda\")\n",
    "# pipe1.scheduler = UniPCMultistepScheduler.from_config(pipe1.scheduler.config)\n",
    "# pipe1.enable_model_cpu_offload()\n",
    "# pipe1.enable_attention_slicing()\n",
    "\n",
    "####### EXAMPLE\n",
    "# # Subprompts\n",
    "# subprompts = [\n",
    "#     \"ancient forest, misty atmosphere\",\n",
    "#     \"mysterious ruins in the distance\"\n",
    "# ]\n",
    "# # Encode each subprompt\n",
    "# subprompt_embeds_1 = [encode_subprompt(pipe1, sp) for sp in subprompts]\n",
    "\n",
    "# # Encode unconditional\n",
    "# uncond_embeds_1 = encode_subprompt(pipe1, \"\")  # blank or negative prompt\n",
    "\n",
    "# # Generate with approach 1\n",
    "\n",
    "# # Weights for each subprompt\n",
    "# weights_1 = [2.0, 5.0]\n",
    "# print(\"Generating image with Approach 1 (single unconditional pass)...\")\n",
    "# output1 = pipe1(\n",
    "#     subprompt_embeds=subprompt_embeds_1,\n",
    "#     subprompt_weights=weights_1,\n",
    "#     uncond_embeds=uncond_embeds_1,\n",
    "#     guidance_scale=7.5,\n",
    "#     num_inference_steps=25\n",
    "# )\n",
    "# output1.images[0].save(\"approach1_result.png\")\n",
    "# print(\"Saved approach1_result.png\")\n",
    "\n",
    "# # --- Generate images for each scene ---\n",
    "# def generate_and_save_images_multi_prompt(scenes, characters_dict, pipe, save_dir, device,\n",
    "#                                           num_inference_steps=50, guidance_scale=7.5):\n",
    "#     \"\"\"\n",
    "#     Generate images for each scene using the Multi-Prompt pipeline and save each image to the specified directory.\n",
    "    \n",
    "#     Args:\n",
    "#         scenes (list): List of scene objects (each scene is a dict).\n",
    "#         characters_dict (dict): Dictionary of character descriptions.\n",
    "#         pipe: The MultiPromptPipelineApproach1 pipeline instance.\n",
    "#         save_dir (str): Directory where images will be saved.\n",
    "#         device (str): Device to use (e.g., \"cuda\" or \"cpu\").\n",
    "#         num_inference_steps (int, optional): Number of diffusion steps.\n",
    "#         guidance_scale (float, optional): Guidance scale for classifier-free guidance.\n",
    "        \n",
    "#     Returns:\n",
    "#         list: List of generated PIL.Image objects.\n",
    "#     \"\"\"\n",
    "#     import os\n",
    "#     import torch\n",
    "\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "#     generated_images = []\n",
    "#     uncond_embeds = encode_subprompt(pipe,\n",
    "#                                      \"low quality, photorealistic, 3d render, overly detailed, digital art, painting, vibrant colors, fine art, NSFW\",\n",
    "#                                      device=device)\n",
    "\n",
    "#     # Iterate over scenes\n",
    "#     for i, scene in enumerate(scenes):\n",
    "#         # Convert the scene to subprompts and their corresponding weights\n",
    "#         subprompt_texts, subprompt_weights = scenes_to_formatted_prompts([scene], characters_dict)[0]\n",
    "\n",
    "#         # Encode each subprompt into an embedding\n",
    "#         subprompt_embeds = [encode_subprompt(pipe, sp, device=device) for sp in subprompt_texts]\n",
    "#         # Encode unconditional (negative/blank) prompt for the baseline\n",
    "#         # uncond_embeds = encode_subprompt(pipe, \"\", device=device)\n",
    "\n",
    "#         print(f\"Generating image for scene {i+1}...\")\n",
    "#         with torch.no_grad():\n",
    "#             output = pipe(\n",
    "#                 subprompt_embeds=subprompt_embeds,\n",
    "#                 subprompt_weights=subprompt_weights,\n",
    "#                 uncond_embeds=uncond_embeds,\n",
    "#                 guidance_scale=guidance_scale,\n",
    "#                 num_inference_steps=num_inference_steps\n",
    "#             )\n",
    "#         generated_image = output.images[0]\n",
    "#         generated_images.append(generated_image)\n",
    "#         image_path = os.path.join(save_dir, f\"scene_{i+1}.png\")\n",
    "#         generated_image.save(image_path)\n",
    "#         print(f\"Image {i+1} saved to {image_path}\")\n",
    "\n",
    "#     return generated_images\n",
    "\n",
    "# save_directory = \"stories/multi_prompt_approach1\"\n",
    "# generated_images = generate_and_save_images_multi_prompt(scenes, characters_dict, pipe1, save_directory, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Unconditional Passes (One per Subprompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have $$\\hat{\\epsilon}=\\hat{\\epsilon}_{\\text{uncond}}+g\\sum_{i=1}^nw_i(\\hat{\\epsilon}_{\\text{cond}_i}-\\hat{\\epsilon}_{\\text{uncond}_i})$$\n",
    "- Total UNet calls per step: $1+2n$ (One global unconditional + two passes for each subprompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiPromptPipelineApproach2(StableDiffusionPipeline):\n",
    "#     \"\"\"\n",
    "#     Multi-Prompt CFG with MULTIPLE unconditional passes:\n",
    "#       - 1 global unconditional pass per step: e_uncond\n",
    "#       - For each subprompt i:\n",
    "#           e_uncond_i (subprompt-specific unconditional)\n",
    "#           e_cond_i    (subprompt conditional)\n",
    "#       - Combine: e = e_uncond + g * sum_i[ w_i * ( e_cond_i - e_uncond_i ) ]\n",
    "#     \"\"\"\n",
    "\n",
    "#     @torch.no_grad()\n",
    "#     def __call__(\n",
    "#         self,\n",
    "#         global_uncond_embeds: torch.Tensor,\n",
    "#         subprompt_pairs: list[tuple[torch.Tensor, torch.Tensor]],\n",
    "#         subprompt_weights: list[float],\n",
    "#         guidance_scale: float = 7.5,\n",
    "#         height: int = 512,\n",
    "#         width: int = 512,\n",
    "#         num_inference_steps: int = 50,\n",
    "#         generator: torch.Generator = None,\n",
    "#         latents: torch.Tensor = None,\n",
    "#         output_type: str = \"pil\",\n",
    "#         return_dict: bool = True,\n",
    "#         **kwargs\n",
    "#     ):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             global_uncond_embeds (Tensor): [batch, seq_len, hidden_dim] for the entire prompt's unconditional pass.\n",
    "#             subprompt_pairs (list of (uncond_i, cond_i)):\n",
    "#                 Each element is a tuple: (uncond_embeds_i, cond_embeds_i).\n",
    "#             subprompt_weights (list[float]): Weights w_i for each subprompt i.\n",
    "#         \"\"\"\n",
    "#         device = self._execution_device\n",
    "#         batch_size = global_uncond_embeds.shape[0]\n",
    "#         num_subprompts = len(subprompt_pairs)\n",
    "\n",
    "#         if num_subprompts != len(subprompt_weights):\n",
    "#             raise ValueError(\"subprompt_pairs and subprompt_weights must have the same length.\")\n",
    "\n",
    "#         # 1. Validate or fallback to default\n",
    "#         if not height or not width:\n",
    "#             height, width = self._default_height_width()\n",
    "\n",
    "#         # 2. Scheduler timesteps\n",
    "#         self.scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "#         timesteps = self.scheduler.timesteps\n",
    "\n",
    "#         # 3. Prepare latents\n",
    "#         if latents is None:\n",
    "#             shape = (batch_size, self.unet.config.in_channels, height // 8, width // 8)\n",
    "#             latents = torch.randn(shape, generator=generator, device=device, dtype=global_uncond_embeds.dtype)\n",
    "#             latents = latents * self.scheduler.init_noise_sigma\n",
    "#         else:\n",
    "#             latents = latents.to(device)\n",
    "\n",
    "#         # 4. Diffusion loop\n",
    "#         for i, t in enumerate(timesteps):\n",
    "#             latent_model_input = self.scheduler.scale_model_input(latents, t)\n",
    "\n",
    "#             # (A) Single global unconditional pass\n",
    "#             e_uncond_global = self.unet(\n",
    "#                 latent_model_input, t, encoder_hidden_states=global_uncond_embeds, **kwargs\n",
    "#             ).sample\n",
    "\n",
    "#             # (B) For each subprompt: unconditional + conditional\n",
    "#             sub_deltas = []\n",
    "#             for (uncond_i, cond_i), w in zip(subprompt_pairs, subprompt_weights):\n",
    "#                 e_uncond_i = self.unet(latent_model_input, t, encoder_hidden_states=uncond_i, **kwargs).sample\n",
    "#                 e_cond_i = self.unet(latent_model_input, t, encoder_hidden_states=cond_i, **kwargs).sample\n",
    "\n",
    "#                 # Delta for subprompt i\n",
    "#                 delta_i = w * (e_cond_i - e_uncond_i)\n",
    "#                 sub_deltas.append(delta_i)\n",
    "\n",
    "#             # (C) Combine sub-deltas\n",
    "#             sum_deltas = sum(sub_deltas)  # sum_i w_i ( e_cond_i - e_uncond_i )\n",
    "\n",
    "#             # (D) Final output\n",
    "#             guided_out = e_uncond_global + guidance_scale * sum_deltas\n",
    "\n",
    "#             # (E) Scheduler step\n",
    "#             latents = self.scheduler.step(guided_out, t, latents, **kwargs).prev_sample\n",
    "\n",
    "#         # 5. Decode\n",
    "#         if output_type == \"latent\":\n",
    "#             if return_dict:\n",
    "#                 from diffusers.pipelines.stable_diffusion.pipeline_output import StableDiffusionPipelineOutput\n",
    "#                 return StableDiffusionPipelineOutput(images=latents, nsfw_content_detected=None)\n",
    "#             return latents\n",
    "\n",
    "#         image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=False)[0]\n",
    "#         image = self.image_processor.postprocess(image, output_type=output_type)\n",
    "\n",
    "#         if return_dict:\n",
    "#             from diffusers.pipelines.stable_diffusion.pipeline_output import StableDiffusionPipelineOutput\n",
    "#             return StableDiffusionPipelineOutput(images=image, nsfw_content_detected=None)\n",
    "#         return image\n",
    "\n",
    "# print(\"Loading Approach 2 pipeline...\")\n",
    "# pipe2 = MultiPromptPipelineApproach2.from_pretrained(\n",
    "#     \"runwayml/stable-diffusion-v1-5\",\n",
    "#     torch_dtype=torch.float16\n",
    "# ).to(\"cuda\")\n",
    "# pipe2.scheduler = UniPCMultistepScheduler.from_config(pipe2.scheduler.config)\n",
    "# pipe2.enable_model_cpu_offload()\n",
    "# pipe2.enable_attention_slicing()\n",
    "\n",
    "#### EXAMPLE\n",
    "# # Suppose we want environment and style separately\n",
    "# global_uncond = encode_subprompt(pipe2, \"\")  # global unconditional\n",
    "# env_uncond = encode_subprompt(pipe2, \"\")     # unconditional for environment\n",
    "# env_cond   = encode_subprompt(pipe2, \"ancient forest, misty atmosphere\")\n",
    "# style_uncond = encode_subprompt(pipe2, \"\")   # unconditional for style\n",
    "# style_cond   = encode_subprompt(pipe2, \"cinematic style, high contrast\")\n",
    "\n",
    "# # subprompt_pairs = [ (uncond_env, cond_env), (uncond_style, cond_style) ]\n",
    "# subprompt_pairs_2 = [\n",
    "#     (env_uncond, env_cond),\n",
    "#     (style_uncond, style_cond)\n",
    "# ]\n",
    "\n",
    "# weights_2 = [1.5, 1.8]\n",
    "# print(\"Generating image with Approach 2 (multiple unconditional passes)...\")\n",
    "# output2 = pipe2(\n",
    "#     global_uncond_embeds=global_uncond,\n",
    "#     subprompt_pairs=subprompt_pairs_2,\n",
    "#     subprompt_weights=weights_2,\n",
    "#     guidance_scale=7.5,\n",
    "#     num_inference_steps=25\n",
    "# )\n",
    "# output2.images[0].save(\"approach2_result.png\")\n",
    "# print(\"Saved approach2_result.png\")\n",
    "\n",
    "# def generate_and_save_images_multi_prompt2(scenes, characters_dict, pipe, save_dir, device,\n",
    "#                                              num_inference_steps=50, guidance_scale=7.5):\n",
    "#     \"\"\"\n",
    "#     Generate images for each scene using Multi-Prompt Approach 2 (multiple unconditional passes)\n",
    "#     and save each image to the specified directory.\n",
    "    \n",
    "#     Args:\n",
    "#         scenes (list): List of scene objects (each scene is a dict).\n",
    "#         characters_dict (dict): Dictionary of character descriptions.\n",
    "#         pipe: The MultiPromptPipelineApproach2 pipeline instance.\n",
    "#         save_dir (str): Directory where images will be saved.\n",
    "#         device (str): Device to use (e.g., \"cuda\" or \"cpu\").\n",
    "#         num_inference_steps (int, optional): Number of diffusion steps.\n",
    "#         guidance_scale (float, optional): Guidance scale for classifier-free guidance.\n",
    "        \n",
    "#     Returns:\n",
    "#         list: List of generated PIL.Image objects.\n",
    "#     \"\"\"\n",
    "#     import os\n",
    "#     import torch\n",
    "\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "#     generated_images = []\n",
    "\n",
    "#     for i, scene in enumerate(scenes):\n",
    "#         # Get subprompt texts and corresponding weights for the scene.\n",
    "#         subprompt_texts, subprompt_weights = scenes_to_formatted_prompts([scene], characters_dict)[0]\n",
    "\n",
    "#         # Encode the global unconditional prompt once.\n",
    "#         global_uncond_embeds = encode_subprompt(pipe, \"\", device=device)\n",
    "\n",
    "#         # For each subprompt, encode a pair: (unconditional, conditional)\n",
    "#         subprompt_pairs = []\n",
    "#         for sp in subprompt_texts:\n",
    "#             uncond_i = encode_subprompt(pipe, \"\", device=device)\n",
    "#             cond_i = encode_subprompt(pipe, sp, device=device)\n",
    "#             subprompt_pairs.append((uncond_i, cond_i))\n",
    "\n",
    "#         print(f\"Generating image for scene {i+1} using Approach 2...\")\n",
    "#         with torch.no_grad():\n",
    "#             output = pipe(\n",
    "#                 global_uncond_embeds=global_uncond_embeds,\n",
    "#                 subprompt_pairs=subprompt_pairs,\n",
    "#                 subprompt_weights=subprompt_weights,\n",
    "#                 guidance_scale=guidance_scale,\n",
    "#                 num_inference_steps=num_inference_steps\n",
    "#             )\n",
    "#         generated_image = output.images[0]\n",
    "#         generated_images.append(generated_image)\n",
    "#         image_path = os.path.join(save_dir, f\"scene_{i+1}_approach2.png\")\n",
    "#         generated_image.save(image_path)\n",
    "#         print(f\"Image {i+1} saved to {image_path}\")\n",
    "\n",
    "#     return generated_images\n",
    "\n",
    "# # Example usage:\n",
    "# save_directory = \"stories/multi_prompt_approach2\"\n",
    "# generated_images = generate_and_save_images_multi_prompt2(scenes, characters_dict, pipe2, save_directory, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopped this because it's extremely slow (20 min for one image) and it's not good either."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldl-ecole",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
