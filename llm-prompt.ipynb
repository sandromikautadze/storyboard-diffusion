{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from together import Together\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "together = Together() # add .env file with TOGETHER_API_KEY variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIENTATIONS = [\n",
    "    \"Front View\", \"Profile View\", \"Back View\", \"From Behind\", \"From Above\",\n",
    "    \"From Below\", \"Three-Quarters View\", \"Long Shot\", \"Three-Quarters Rear View\"\n",
    "]\n",
    "\n",
    "EXPRESSIONS = [\n",
    "    \"afraid\", \"amused\", \"angry\", \"anxious\", \"ashamed\", \"bored\", \"confident\",\n",
    "    \"confused\", \"contempt\", \"curious\", \"depressed\", \"determined\", \"disgusted\",\n",
    "    \"ecstatic\", \"embarrassed\", \"enraged\", \"excited\", \"fear\", \"frightened\", \"frown\",\n",
    "    \"frustrated\", \"guilty\", \"happy\", \"hopeful\", \"hurt\", \"indifferent\", \"jealous\",\n",
    "    \"joyful\", \"miserable\", \"nervous\", \"neutral\", \"optimistic\", \"proud\", \"puzzled\",\n",
    "    \"relieved\", \"sad\", \"scared\", \"shocked\", \"shy\", \"skeptical\", \"sleepy\", \"smile\",\n",
    "    \"smug\", \"sorry\", \"stubborn\", \"surprised\", \"suspicious\", \"thoughtful\", \"tired\",\n",
    "    \"withdrawn\", \"worried\"\n",
    "]\n",
    "\n",
    "CAMERA_SHOTS = [\n",
    "    \"Aerial View\", \"Bird’s-Eye View\", \"Close-Up\", \"Cowboy Shot\", \"Dolly Zoom\",\n",
    "    \"Dutch Angle\", \"Establishing Shot\", \"Extreme Close-Up\", \"Extreme Long Shot\",\n",
    "    \"Full Shot\", \"Long Shot\", \"Medium Close-Up\", \"Medium Long Shot\", \"Medium Shot\",\n",
    "    \"Over-the-Shoulder Shot\", \"Point-of-View Shot\", \"Two-Shot\", \"Fisheye Shot\",\n",
    "    \"Worm's Eye\", \"Low-Angle Shot\", \"Macro Shot\", \"Tilt-Shift Shot\", \"Telephoto Shot\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scene(BaseModel):\n",
    "    scene_number: int = Field(description=\"The sequential number of the scene\")\n",
    "    description: str = Field(description=\"A single-moment scene optimized for diffusion model sketch prompting\")\n",
    "    orientation: str = Field(description=\"Character orientation chosen from predefined list\")\n",
    "    expression: str = Field(description=\"Character facial expressions with names, e.g., 'John is worried'\")\n",
    "    setting: str = Field(description=\"Where is it happening? Time of day? Environmental details?\")\n",
    "    shot_type: str = Field(description=\"Camera shot chosen from predefined list\")\n",
    "\n",
    "class SceneList(BaseModel):\n",
    "    scenes: List[Scene]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_scenes(story_text, characters, num_scenes=5, style=\"sketch storyboard\"):\n",
    "    \"\"\"\n",
    "    Extracts structured storyboard scenes from a story using Meta's Llama-3 via Together AI.\n",
    "\n",
    "    Parameters:\n",
    "    - story_text (str): The full story provided by the user.\n",
    "    - num_scenes (int): Desired number of storyboard scenes.\n",
    "    - characters (dict): Dictionary where each key is a character's name and each value is their description.\n",
    "    - style (str): The preferred visual style (default is \"sketch storyboard\").\n",
    "\n",
    "    Returns:\n",
    "    - List of structured storyboard scenes optimized for diffusion model prompting.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert character descriptions into a structured reference\n",
    "    character_details = \"\\n\".join([f\"- {name}: {desc}\" for name, desc in characters.items()])\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an AI specialized in creating optimized storyboard scene descriptions for diffusion models.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"\n",
    "        The user has provided the following story:\n",
    "        \"{story_text}\"\n",
    "\n",
    "        ### **Storyboard Generation Instructions**\n",
    "        - Expand the story logically to create a **continuous, structured visual narrative**.\n",
    "        - Ensure **smooth transitions** between scenes, making the story feel like a real movie storyboard.\n",
    "        - Divide the story into exactly {num_scenes} scenes, each capturing a **single distinct moment**.\n",
    "        - Camera angles must be **carefully chosen to evoke emotion and storytelling impact**.\n",
    "\n",
    "        ### **General Character Descriptions**\n",
    "        These characters appear throughout the storyboard.  \n",
    "        Their **appearance, clothing, and defining traits remain consistent**:  \n",
    "        {character_details}\n",
    "\n",
    "        ### **Scene Description Guidelines for Diffusion Model Prompting**\n",
    "        - Each scene must depict **one specific moment**, not multiple actions.\n",
    "        - The description should be **highly visual and structured for AI image generation**.\n",
    "        - **Do not** repeat character descriptions in every scene—focus on their actions and positioning.\n",
    "        - Integrate the following elements into the scene description:\n",
    "          - **Action**: Describe the key action taking place.\n",
    "          - **Characters**: Do **not** list them separately—integrate them into the expression field.\n",
    "          - **Expression**: Instead of listing emotions separately, attach them to character names.  \n",
    "            Example: `\"John looks nervous\"` instead of `\"expression: nervous\"`.\n",
    "          - **Setting**: Where is this happening? Time of day? Important visual cues.\n",
    "          - **Shot Type**: Specify the camera angle without including it in the text.\n",
    "          - **Style**: This is a **'{style}'**, meant for **rough sketch-based storyboarding**.\n",
    "\n",
    "        ### **Output Format (JSON)**\n",
    "        Return a **list** of {num_scenes} structured scenes in JSON format. Each scene should have:\n",
    "        - \"scene_number\": The scene index.\n",
    "        - \"description\": A single-moment description optimized for sketch-based image generation.\n",
    "        - \"orientation\": The character’s body position, chosen from predefined options.\n",
    "        - \"expression\": A sentence describing the emotions of the characters, including their names.\n",
    "        - \"setting\": Where is it happening? Time of day? Environmental details?\n",
    "        - \"shot_type\": The camera shot, chosen from predefined options.\n",
    "        \"\"\"}\n",
    "    ]\n",
    "\n",
    "    # call Together AI's API with structured JSON list output\n",
    "    response = together.chat.completions.create(\n",
    "        model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "        messages=messages,\n",
    "        max_tokens=1500,\n",
    "        temperature=0.7,\n",
    "        response_format={\"type\": \"json_object\", \"schema\": SceneList.model_json_schema()}\n",
    "    )\n",
    "\n",
    "    # extract response text and parse it\n",
    "    try:\n",
    "        scenes = json.loads(response.choices[0].message.content)[\"scenes\"]\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error: LLM output could not be parsed as JSON.\")\n",
    "        scenes = []\n",
    "\n",
    "    return scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_story = \"\"\"\n",
    "John enters the dark alley, his footsteps echoing between the damp brick walls.\n",
    "Suddenly, a shadowy figure appears at the far end under the flickering streetlamp.\n",
    "\"\"\"\n",
    "\n",
    "characters = {\n",
    "    \"John\": \"A tall man with short brown hair, blue eyes, and a leather jacket.\",\n",
    "    \"Shadowy Figure\": \"A mysterious person in a long cloak, face obscured by darkness.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"scene_number\": 1,\n",
      "        \"description\": \"John enters the dark alley, his footsteps echoing between the damp brick walls.\",\n",
      "        \"orientation\": \"medium shot\",\n",
      "        \"expression\": \"John looks cautious\",\n",
      "        \"setting\": \"a dimly lit alley at night, with a high ceiling and brick walls.\",\n",
      "        \"shot_type\": \"over-the-shoulder\"\n",
      "    },\n",
      "    {\n",
      "        \"scene_number\": 2,\n",
      "        \"description\": \"John pauses, his eyes fixed on the flickering streetlamp as the shadowy figure steps into view.\",\n",
      "        \"orientation\": \"medium shot\",\n",
      "        \"expression\": \"John's expression turns tense as he looks at the shadowy figure\",\n",
      "        \"setting\": \"the same alley, now lit by a single flickering streetlamp.\",\n",
      "        \"shot_type\": \"two-shot\"\n",
      "    },\n",
      "    {\n",
      "        \"scene_number\": 3,\n",
      "        \"description\": \"The shadowy figure raises its hood, and John takes a step back, his eyes fixed on the figure.\",\n",
      "        \"orientation\": \"medium shot\",\n",
      "        \"expression\": \"John looks fearful as the shadowy figure raises its hood\",\n",
      "        \"setting\": \"the same alley, with the streetlamp casting eerie shadows.\",\n",
      "        \"shot_type\": \"close-up\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "scenes = extract_scenes(\n",
    "    story_text=sample_story, \n",
    "    characters=characters, \n",
    "    num_scenes=3\n",
    ")\n",
    "\n",
    "print(json.dumps(scenes, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _scene_to_prompt(scene, style=\"storyboard sketch\"):\n",
    "    \"\"\"\n",
    "    Converts a structured scene representation into a short, optimized text prompt for Stable Diffusion.\n",
    "\n",
    "    Parameters:\n",
    "    - scene (dict): A single scene's structured representation.\n",
    "    - style (str): The desired artistic style (default is \"storyboard sketch\").\n",
    "\n",
    "    Returns:\n",
    "    - str: A structured text prompt optimized for Stable Diffusion.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract necessary details\n",
    "    description = scene[\"description\"]\n",
    "    orientation = scene[\"orientation\"]\n",
    "    expression = scene[\"expression\"]\n",
    "    setting = scene[\"setting\"]\n",
    "    shot_type = scene[\"shot_type\"]\n",
    "\n",
    "    # **Optimized, Concise Prompt**\n",
    "    prompt = (\n",
    "        f\"{style}. \"\n",
    "        f\"{description} \"\n",
    "        f\"{expression}. \"\n",
    "        f\"{orientation}, {shot_type}. \"\n",
    "        f\"Setting: {setting}.\"\n",
    "    )\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_prompts(scenes, style=\"rough storyboard sketch\"):\n",
    "    \"\"\"\n",
    "    Converts all scenes into structured diffusion model prompts.\n",
    "\n",
    "    Parameters:\n",
    "    - scenes (list): List of structured scenes.\n",
    "    - style (str): The desired artistic style.\n",
    "\n",
    "    Returns:\n",
    "    - list: List of formatted text prompts.\n",
    "    \"\"\"\n",
    "    prompts = [_scene_to_prompt(scene, style) for scene in scenes]\n",
    "    return prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 1:\n",
      "rough storyboard sketch. John enters the dark alley, his footsteps echoing between the damp brick walls. John looks cautious. medium shot, over-the-shoulder. Setting: a dimly lit alley at night, with a high ceiling and brick walls..\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Prompt 2:\n",
      "rough storyboard sketch. John pauses, his eyes fixed on the flickering streetlamp as the shadowy figure steps into view. John's expression turns tense as he looks at the shadowy figure. medium shot, two-shot. Setting: the same alley, now lit by a single flickering streetlamp..\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Prompt 3:\n",
      "rough storyboard sketch. The shadowy figure raises its hood, and John takes a step back, his eyes fixed on the figure. John looks fearful as the shadowy figure raises its hood. medium shot, close-up. Setting: the same alley, with the streetlamp casting eerie shadows..\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generated_prompts = generate_all_prompts(scenes)\n",
    "\n",
    "# Print results\n",
    "for i, prompt in enumerate(generated_prompts):\n",
    "    print(f\"Prompt {i+1}:\")\n",
    "    print(prompt)\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9399f34b0f75404dbfa9cbb83fb3f0ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StableDiffusionPipeline {\n",
       "  \"_class_name\": \"StableDiffusionPipeline\",\n",
       "  \"_diffusers_version\": \"0.32.1\",\n",
       "  \"_name_or_path\": \"CompVis/stable-diffusion-v1-4\",\n",
       "  \"feature_extractor\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPImageProcessor\"\n",
       "  ],\n",
       "  \"image_encoder\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"requires_safety_checker\": true,\n",
       "  \"safety_checker\": [\n",
       "    \"stable_diffusion\",\n",
       "    \"StableDiffusionSafetyChecker\"\n",
       "  ],\n",
       "  \"scheduler\": [\n",
       "    \"diffusers\",\n",
       "    \"PNDMScheduler\"\n",
       "  ],\n",
       "  \"text_encoder\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTextModel\"\n",
       "  ],\n",
       "  \"tokenizer\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTokenizer\"\n",
       "  ],\n",
       "  \"unet\": [\n",
       "    \"diffusers\",\n",
       "    \"UNet2DConditionModel\"\n",
       "  ],\n",
       "  \"vae\": [\n",
       "    \"diffusers\",\n",
       "    \"AutoencoderKL\"\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "# Load Stable Diffusion model\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float16)\n",
    "pipe.to(\"cuda\")  # Move to GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "181b0db2953045dea79802b17f474c24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate image from first prompt\n",
    "image = pipe(generated_prompts[0]).images[0]\n",
    "\n",
    "# Save image\n",
    "image.save(\"scene1_storyboard.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d5757021aaf4f7d84bb90853951d2ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate image from first prompt\n",
    "image = pipe(generated_prompts[1]).images[0]\n",
    "\n",
    "# Save image\n",
    "image.save(\"scene2_storyboard.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b887b0ec63ab414bb3f250c8db2b7e1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate image from first prompt\n",
    "image = pipe(generated_prompts[2]).images[0]\n",
    "\n",
    "# Save image\n",
    "image.save(\"scene3_storyboard.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldl-ecole",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
