{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dotenv import load_dotenv\n",
    "# import json\n",
    "# from together import Together\n",
    "# from pydantic import BaseModel\n",
    "# from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_dotenv()\n",
    "# together = Together() # add .env file with TOGETHER_API_KEY variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters_dict = {\n",
    "    \"Don Vito Corleone\": {\n",
    "        \"age\": \"early 60s\", \"gender\": \"male\", \"hair\": \"slicked-back gray-black hair\",\n",
    "        \"clothing\": \"dark three-piece suit\",\n",
    "        \"body_type\": \"stocky, slightly hunched posture\",\n",
    "        \"accessories\": \"gold ring on right hand, pocket watch\",\n",
    "        \"ethnicity\": \"Italian-American\"\n",
    "    },\n",
    "    \"Tom Hagen\": {\n",
    "        \"age\": \"early 40s\", \"gender\": \"male\", \"hair\": \"short, neatly combed brown hair\",\n",
    "        \"facial_hair\": \"clean-shaven\", \"clothing\": \"gray suit, dark tie\",\n",
    "        \"body_type\": \"medium build, upright posture\", \"ethnicity\": \"German-Irish\"\n",
    "    },\n",
    "    \"Johnny Fontane\": {\n",
    "        \"age\": \"late 30s\", \"gender\": \"male\", \"hair\": \"short, slicked-back black hair\",\n",
    "        \"facial_hair\": \"clean shaven\", \"clothing\": \"dark, stylish suit with an open collar\",\n",
    "        \"body_type\": \"slim and fit\", \"accessories\": \"gold ring, cigarette\"\n",
    "    },\n",
    "    \"Sonny\": {\n",
    "        \"age\": \"early 30s\", \"gender\": \"male\", \"hair\": \"curly, dark brown hair\",\n",
    "        \"facial_hair\": \"clean-shaven\", \"clothing\": \"formal suit, slightly disheveled\",\n",
    "        \"body_type\": \"athletic build\", \"ethnicity\": \"Italian-American\",\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "script = \"\"\"\n",
    "INT DAY: DON'S OFFICE (SUMMER 1945)\n",
    "\n",
    "\t\t\t\tDON CORLEONE\n",
    "\t\tACT LIKE A MAN!  By Christ in\n",
    "\t\tHeaven, is it possible you turned\n",
    "\t\tout no better than a Hollywood\n",
    "\t\tfinocchio.\n",
    "\n",
    "\tBoth HAGEN and JOHNNY cannot refrain from laughing.  The DON\n",
    "\tsmiles.  SONNY enters as noiselessly as possible, still\n",
    "\tadjusting his clothes.\n",
    "\n",
    "\t\t\t\tDON CORLEONE\n",
    "\t\tAll right, Hollywood...Now tell me\n",
    "\t\tabout this Hollywood Pezzonovanta\n",
    "\t\twho won't let you work.\n",
    "\n",
    "\t\t\t\tJOHNNY\n",
    "\t\tHe owns the studio.  Just a month\n",
    "\t\tago he bought the movie rights to\n",
    "\t\tthis book, a best seller.  And the\n",
    "\t\tmain character is a guy just like\n",
    "\t\tme.  I wouldn't even have to act,\n",
    "\t\tjust be myself.\n",
    "\n",
    "\tThe DON is silent, stern.\n",
    "\n",
    "\t\t\t\tDON CORLEONE\n",
    "\t\tYou take care of your family?\n",
    "\n",
    "\t\t\t\tJOHNNY\n",
    "\t\tSure.\n",
    "\n",
    "\tHe glances at SONNY, who makes himself as inconspicuous as\n",
    "\the can.\n",
    "\n",
    "\t\t\t\tDON CORLEONE\n",
    "\t\tYou look terrible.  I want you to\n",
    "\t\teat well, to rest.  And spend time\n",
    "\t\twith your family.  And then, at the\n",
    "\t\tend of the month, this big shot\n",
    "\t\twill give you the part you want.\n",
    "\n",
    "\t\t\t\tJOHNNY\n",
    "\t\tIt's too late.  All the contracts\n",
    "\t\thave been signed, they're almost\n",
    "\t\tready to shoot.\n",
    "\n",
    "\t\t\t\tDON CORLEONE\n",
    "\t\tI'll make him an offer he can't\n",
    "\t\trefuse.\n",
    "\n",
    "\tHe takes JOHNNY to the door, pinching his cheek hard enough\n",
    "\tto hurt.\n",
    "\n",
    "\t\t\t\tDON CORLEONE\n",
    "\t\tNow go back to the party and leave\n",
    "\t\tit to me.\n",
    "\n",
    "\tHe closes the door, smiling to himself.  Turns to HAGEN.\n",
    "\n",
    "\t\t\t\tDON CORLEONE\n",
    "\t\tWhen does my daughter leave with\n",
    "\t\ther bridegroom?\n",
    "\n",
    "\t\t\t\tHAGEN\n",
    "\t\tThey'll cut the cake in a few\n",
    "\t\tminutes...leave right after that.\n",
    "\t\tYour new son-in-law, do we give him\n",
    "\t\tsomething important?\n",
    "\n",
    "\t\t\t\tDON CORLEONE\n",
    "\t\tNo, give him a living.  But never\n",
    "\t\tlet him know the family's business.\n",
    "\t\tWhat else, Tom?\n",
    "\n",
    "\t\t\t\tHAGEN\n",
    "\t\tI've called the hospital; they've\n",
    "\t\tnotified Consiglere Genco's family\n",
    "\t\tto come and wait.  He won't last\n",
    "\t\tout the night.\n",
    "\n",
    "\tThis saddens the DON.  He sighs.\n",
    "\n",
    "\t\t\t\tDON CORLEONE\n",
    "\t\tGenco will wait for me.  Santino,\n",
    "\t\ttell your brothers they will come\n",
    "\t\twith me to the hospital to see\n",
    "\t\tGenco.  Tell Fredo to drive the big\n",
    "\t\tcar, and ask Johnny to come with us.\n",
    "\n",
    "\t\t\t\tSONNY\n",
    "\t\tAnd Michael?\n",
    "\n",
    "\t\t\t\tDON CORLEONE\n",
    "\t\tAll my sons.\n",
    "\t\t\t  (to HAGEN)\n",
    "\t\tTom, I want you to go to California\n",
    "\t\ttonight.  Make the arrangements.\n",
    "\t\tBut don't leave until I come back\n",
    "\t\tfrom the hospital and speak to you.\n",
    "\t\tUnderstood?\n",
    "\n",
    "\t\t\t\tHAGEN\n",
    "\t\tUnderstood.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORIENTATIONS = [\n",
    "#     \"Front View\", \"Profile View\", \"Back View\", \"From Behind\", \"From Above\",\n",
    "#     \"From Below\", \"Three-Quarters View\", \"Long Shot\", \"Three-Quarters Rear View\"\n",
    "# ]\n",
    "\n",
    "# CAMERA_SHOTS = [\n",
    "#     \"Aerial View\", \"Birdâ€™s-Eye View\", \"Close-Up\", \"Cowboy Shot\", \"Dolly Zoom\",\n",
    "#     \"Dutch Angle\", \"Establishing Shot\", \"Extreme Close-Up\", \"Extreme Long Shot\",\n",
    "#     \"Full Shot\", \"Long Shot\", \"Medium Close-Up\", \"Medium Long Shot\", \"Medium Shot\",\n",
    "#     \"Over-the-Shoulder Shot\", \"Point-of-View Shot\", \"Two-Shot\", \"Fisheye Shot\",\n",
    "#     \"Worm's Eye\", \"Low-Angle Shot\", \"Macro Shot\", \"Tilt-Shift Shot\", \"Telephoto Shot\"\n",
    "# ]\n",
    "\n",
    "# class Character(BaseModel):\n",
    "#     name: str\n",
    "    \n",
    "# class Scene(BaseModel):\n",
    "#     scene_number: int\n",
    "#     shot_type: str\n",
    "#     orientation: str\n",
    "#     characters: List[Character]\n",
    "#     environment: str\n",
    "#     description: str\n",
    "    \n",
    "# class SceneList(BaseModel):\n",
    "#     scenes: List[Scene]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _build_character_description(characters_dict: Dict[str, str]):\n",
    "#     \"\"\"\n",
    "#     Generates text description of character from the dictionary\n",
    "#     \"\"\"\n",
    "#     features = [\n",
    "#         characters_dict.get(\"ethnicity\", \"\"),\n",
    "#         characters_dict.get(\"age\", \"\"),\n",
    "#         characters_dict.get(\"gender\", \"\"),\n",
    "#         characters_dict.get(\"hair\", \"\"),\n",
    "#         characters_dict.get(\"facial_hair\", \"\"),\n",
    "#         characters_dict.get(\"body_type\", \"\"),\n",
    "#         f\"wearing {characters_dict.get('clothing', '')}\",\n",
    "#         f\"with {characters_dict.get('accessories', '')}\" if characters_dict.get(\"accessories\") else \"\"\n",
    "#     ]\n",
    "    \n",
    "#     return \", \".join(filter(None, features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def input_to_json(script: str, characters: Dict[str, Dict], temperature: int = 0.7):\n",
    "#     \"\"\"\n",
    "#     Converts a script and character descriptions into a JSON format for storyboard generation.\n",
    "#     \"\"\"\n",
    "#     character_descriptions = {name: _build_character_description(desc) for name, desc in characters.items()}\n",
    "    \n",
    "#     script_section = f\"Here is the film script: \\n{script}\"\n",
    "#     characters_section = f\"The characters in the script have the following descriptions: \\n{json.dumps(character_descriptions, indent=2)}\"\n",
    "    \n",
    "#     instructions = \"\"\"\n",
    "# ### Storyboard Generation Instructions\n",
    "# 1. **Number of Scenes**: Divide the entire script into a reasonable number of scenes (typically between 4 to 7 scenes), not too many or too few.\n",
    "# 2. **Single Distinct Moment**: Each scene captures a single moment.\n",
    "# 3. **Camera Angles & Orientation**: Choose from these shot types: {', '.join(self.CAMERA_SHOTS)}.  \n",
    "# Choose from these orientations: {', '.join(self.ORIENTATIONS)}.\n",
    "# 4. **Location & Time**: Clearly derive environment from the script (e.g. INT DAY, DON'S OFFICE, etc.). Describe it in its details (size, lighhting, mood, organization of the objects, etc.). Notice that if it's the same across the different scenes, it must be written in the same way\n",
    "# 5. **Characters**:\n",
    "# - List only characters relevant to the single moment in each scene.\n",
    "# - Each character must have the name and a short description (consistent from provided descriptions).\n",
    "# 6. Clearly describe the scene including actions, character positions (foreground, background, left, right), emotions, and expressions.\n",
    "# 7. **Scene Format**: Return JSON with a key 'scenes' as an array of structured objects:\n",
    "# - \"scene_number\": integer\n",
    "# - \"shot_type\": camera shot type (from provided list) \n",
    "# - \"orientation\": orientation (from provided list)\n",
    "# - \"characters\": list of objects with:\n",
    "#         - \"name\": character's name, not as they appear on the script but as they were given to you in the description.\n",
    "# - \"environment\": short description of the location\n",
    "# - \"description\": short, vivid description focusing on actions, expressions, emotions of each single character. Also their relative position is clearly described. The description must be succint, without extra articles or words, it should be visual and useful for an image generation prompt. Ensure it makes sense with the shot type (e.g., if it's medium shot, don't say that the face is covering the full image, otherwise it should be a close up). Don't write the words they say, since they occupy tokens, unless it's a fundamental part of the script. Avoid useless adjetives or adverbs, be concise and clear.\n",
    "\n",
    "# Follow the above instructions very carefully. Notice that the scenes have no knowledge of each other's contents. So in case something is necessary, describe it again. \n",
    "# \"\"\"\n",
    "\n",
    "#     example_input = \"\"\"\n",
    "# ### Example\n",
    "# Input: \n",
    "# - Script is \n",
    "# INT DAY: DON'S OFFICE (SUMMER 1945)\n",
    "\n",
    "#         DON CORLEONE\n",
    "# ACT LIKE A MAN!  By Christ in\n",
    "# Heaven, is it possible you turned\n",
    "# out no better than a Hollywood\n",
    "# finocchio.\n",
    "\n",
    "# Both HAGEN and JOHNNY cannot refrain from laughing.  The DON\n",
    "# smiles.  SONNY enters as noiselessly as possible, still\n",
    "# adjusting his clothes.\n",
    "\n",
    "#         DON CORLEONE\n",
    "# All right, Hollywood...Now tell me\n",
    "# about this Hollywood Pezzonovanta\n",
    "# who won't let you work.\n",
    "\n",
    "#         JOHNNY\n",
    "# He owns the studio.  Just a month\n",
    "# ago he bought the movie rights to\n",
    "# this book, a best seller.  And the\n",
    "# main character is a guy just like\n",
    "# me.  I wouldn't even have to act,\n",
    "# just be myself.\n",
    "\n",
    "# The DON is silent, stern.\n",
    "\n",
    "#         DON CORLEONE\n",
    "# You take care of your family?\n",
    "\n",
    "#         JOHNNY\n",
    "# Sure.\n",
    "\n",
    "# He glances at SONNY, who makes himself as inconspicuous as\n",
    "# he can.\n",
    "\n",
    "#         DON CORLEONE\n",
    "# You look terrible.  I want you to\n",
    "# eat well, to rest.  And spend time\n",
    "# with your family.  And then, at the\n",
    "# end of the month, this big shot\n",
    "# will give you the part you want.\n",
    "\n",
    "#         JOHNNY\n",
    "# It's too late.  All the contracts\n",
    "# have been signed, they're almost\n",
    "# ready to shoot.\n",
    "\n",
    "#         DON CORLEONE\n",
    "# I'll make him an offer he can't\n",
    "# refuse.\n",
    "\n",
    "# He takes JOHNNY to the door, pinching his cheek hard enough\n",
    "# to hurt.\n",
    "\n",
    "#         DON CORLEONE\n",
    "# Now go back to the party and leave\n",
    "# it to me.\n",
    "\n",
    "# He closes the door, smiling to himself.  Turns to HAGEN.\n",
    "\n",
    "#         DON CORLEONE\n",
    "# When does my daughter leave with\n",
    "# her bridegroom?\n",
    "\n",
    "#         HAGEN\n",
    "# They'll cut the cake in a few\n",
    "# minutes...leave right after that.\n",
    "# Your new son-in-law, do we give him\n",
    "# something important?\n",
    "\n",
    "#         DON CORLEONE\n",
    "# No, give him a living.  But never\n",
    "# let him know the family's business.\n",
    "# What else, Tom?\n",
    "\n",
    "#         HAGEN\n",
    "# I've called the hospital; they've\n",
    "# notified Consigliere Genco's family\n",
    "# to come and wait.  He won't last\n",
    "# out the night.\n",
    "\n",
    "# This saddens the DON.  He sighs.\n",
    "\n",
    "#         DON CORLEONE\n",
    "# Genco will wait for me.  Santino,\n",
    "# tell your brothers they will come\n",
    "# with me to the hospital to see\n",
    "# Genco.  Tell Fredo to drive the big\n",
    "# car, and ask Johnny to come with us.\n",
    "\n",
    "#         SONNY\n",
    "# And Michael?\n",
    "\n",
    "#         DON CORLEONE\n",
    "# All my sons.\n",
    "#         (to HAGEN)\n",
    "# Tom, I want you to go to California\n",
    "# tonight.  Make the arrangements.\n",
    "# But don't leave until I come back\n",
    "# from the hospital and speak to you.\n",
    "# Understood?\n",
    "\n",
    "#         HAGEN\n",
    "# Understood.\n",
    "\n",
    "# - Characters description from the dictionary gives\n",
    "#         - Don Vito Corleone: 'Italian-American, early 60s, male, slicked-back gray-black hair, stocky, slightly hunched posture, wearing dark three-piece suit, with gold ring on right hand, pocket watch'\n",
    "#         - Johnny Fontane: 'late 30s, male, short, slicked-back black hair, clean shaven, slim and fit, wearing dark, stylish suit with an open collar, with gold ring, cigarette'\n",
    "#         - Tom Hagen: 'German-Irish, early 40s, male, short, neatly combed brown hair, clean-shaven, medium build, upright posture, wearing gray suit, dark tie'\n",
    "#         - Sonny: 'Italian-American, early 30s, male, curly, dark brown hair, clean-shaven, athletic build, wearing formal suit, slightly disheveled'\n",
    "# \"\"\"\n",
    "\n",
    "#     example_output = \"\"\"\n",
    "# Example Output:\n",
    "# {\n",
    "#    \"scenes\": [\n",
    "#    {\n",
    "#    \"scene_number\": 1,\n",
    "#    \"shot_type\": \"Medium Shot\",\n",
    "#    \"orientation\": \"Front View\",\n",
    "#    \"characters\": [\n",
    "#            {\n",
    "#            \"name\": \"Don Vito Corleone\"\n",
    "#            },\n",
    "#            {\n",
    "#            \"name\": \"Johnny Fontane\"\n",
    "#            },\n",
    "#            {\n",
    "#            \"name\": \"Tom Hagen\"\n",
    "#            }\n",
    "#    ],\n",
    "#    \"environment\": \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
    "#    \"description\": \"Don Corleone stands imposingly behind desk, face stern with righteous anger, pointing finger at Johnny. Johnny appears embarrassed, head slightly bowed. Hagen stands to the right, barely containing laughter. Tension and amusement mix in intimate office atmosphere.\"\n",
    "#    },\n",
    "#    {\n",
    "#    \"scene_number\": 2,\n",
    "#    \"shot_type\": \"Two-Shot\",\n",
    "#    \"orientation\": \"Profile View\",\n",
    "#    \"characters\": [\n",
    "#            {\n",
    "#            \"name\": \"Don Vito Corleone\"\n",
    "#            },\n",
    "#            {\n",
    "#            \"name\": \"Johnny Fontane\"\n",
    "#            },\n",
    "#            {\n",
    "#            \"name\": \"Tom Hagen\"\n",
    "#            },\n",
    "#            {\n",
    "#            \"name\": \"Sonny\"\n",
    "#            }\n",
    "#    ],\n",
    "#    \"environment\": \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
    "#    \"description\": \"Sonny quietly enters room from right, adjusting disheveled clothes. Don leans forward at desk, expression softening to business-like focus. Johnny stands center, straightening posture. Hagen observes from left corner. Atmosphere shifts from personal rebuke to business discussion.\"\n",
    "#    },\n",
    "#    {\n",
    "#    \"scene_number\": 3,\n",
    "#    \"shot_type\": \"Close-Up\",\n",
    "#    \"orientation\": \"Front View\",\n",
    "#    \"characters\": [\n",
    "#            {\n",
    "#            \"name\": \"Don Vito Corleone\"\n",
    "#            }\n",
    "#    ],\n",
    "#    \"environment\": \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
    "#    \"description\": \"Don Corleone's face fills frame, stern and contemplative. Eyes narrowed, jaw set firmly. Saying 'I'll make him an offer he can't refuse' with quiet, confident menace. Power and authority emanate from his expression.\"\n",
    "#    },\n",
    "#    {\n",
    "#    \"scene_number\": 4,\n",
    "#    \"shot_type\": \"Medium Close-Up\",\n",
    "#    \"orientation\": \"Three-Quarters View\",\n",
    "#    \"characters\": [\n",
    "#            {\n",
    "#            \"name\": \"Don Vito Corleone\"\n",
    "#            },\n",
    "#            {\n",
    "#            \"name\": \"Johnny Fontane\"\n",
    "#            }\n",
    "#    ],\n",
    "#    \"environment\": \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
    "#    \"description\": \"Don Corleone escorts Johnny to door, pinching his cheek firmly. Don's expression shows affection mixed with dominance. Johnny winces slightly at pain while showing relief and gratitude. Door frame visible on right edge of shot.\"\n",
    "#    },\n",
    "#    {\n",
    "#    \"scene_number\": 5,\n",
    "#    \"shot_type\": \"Medium Shot\",\n",
    "#    \"orientation\": \"Front View\",\n",
    "#    \"characters\": [\n",
    "#            {\n",
    "#            \"name\": \"Don Vito Corleone\"\n",
    "#            },\n",
    "#            {\n",
    "#            \"name\": \"Tom Hagen\"\n",
    "#            }\n",
    "#    ],\n",
    "#    \"environment\": \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
    "#    \"description\": \"Don Corleone turns from closed door, small smile fading to serious business expression. Hagen stands attentively near desk, notepad ready. Don moves toward chair, shoulders slightly hunched, gold ring catching light as he gestures.\"\n",
    "#    },\n",
    "#    {\n",
    "#    \"scene_number\": 6,\n",
    "#    \"shot_type\": \"Over-the-Shoulder Shot\",\n",
    "#    \"orientation\": \"Profile View\",\n",
    "#    \"characters\": [\n",
    "#            {\n",
    "#            \"name\": \"Don Vito Corleone\"\n",
    "#            },\n",
    "#            {\n",
    "#            \"name\": \"Tom Hagen\"\n",
    "#            },\n",
    "#            {\n",
    "#            \"name\": \"Sonny\"\n",
    "#            }\n",
    "#    ],\n",
    "#    \"environment\": \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
    "#    \"description\": \"Camera over Don's shoulder, facing Hagen and Sonny. Don's gray-black hair and dark suit visible in foreground. Hagen's face shows respectful attention. Sonny stands beside him, now composed. Don's voice carries weight as he issues final instructions about hospital visit.\"\n",
    "#    }\n",
    "#    ]\n",
    "# }\n",
    "# \"\"\"\n",
    "\n",
    "#     # Combine all content together without nesting f-strings\n",
    "#     user_content = f\"{script_section}\\n\\n{characters_section}\\n\\n{instructions}\\n{example_input}\\n{example_output}\"\n",
    "    \n",
    "#     messages = [\n",
    "#         {\"role\": \"system\", \"content\": (\n",
    "#             \"You are an AI specialized in creating structured storyboard scenes from a film script \"\n",
    "#             \"for image generation (e.g., stable diffusion). Each scene must capture a single distinct moment, \"\n",
    "#             \"should list relevant characters with consistent appearances, specify the environment, camera shot, \"\n",
    "#             \"and orientation, and provide direct clues for a diffusion model to generate images.\"\n",
    "#             )},\n",
    "#         {\"role\": \"user\", \"content\": user_content}\n",
    "#     ]\n",
    "    \n",
    "#     response = together.chat.completions.create(\n",
    "#         model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "#         messages=messages,\n",
    "#         max_tokens=10000,\n",
    "#         temperature=temperature,\n",
    "#         response_format={\"type\": \"json_object\", \"schema\": SceneList.model_json_schema()}\n",
    "#     )\n",
    "\n",
    "#     try:\n",
    "#         output_json = response.choices[0].message.content\n",
    "#         return json.loads(output_json)[\"scenes\"]\n",
    "#     except (json.JSONDecodeError, KeyError) as e:\n",
    "#         print(\"Error parsing JSON output:\", e)\n",
    "#         return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"scene_number\": 1,\n",
      "        \"shot_type\": \"Medium Shot\",\n",
      "        \"orientation\": \"Front View\",\n",
      "        \"characters\": [\n",
      "            {\n",
      "                \"name\": \"Don Vito Corleone\"\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"Johnny Fontane\"\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"Tom Hagen\"\n",
      "            }\n",
      "        ],\n",
      "        \"environment\": \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
      "        \"description\": \"Don Corleone stands imposingly behind desk, face stern with righteous anger, pointing finger at Johnny. Johnny appears embarrassed, head slightly bowed. Hagen stands to the right, barely containing laughter. Tension and amusement mix in intimate office atmosphere.\"\n",
      "    },\n",
      "    {\n",
      "        \"scene_number\": 2,\n",
      "        \"shot_type\": \"Two-Shot\",\n",
      "        \"orientation\": \"Profile View\",\n",
      "        \"characters\": [\n",
      "            {\n",
      "                \"name\": \"Don Vito Corleone\"\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"Johnny Fontane\"\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"Tom Hagen\"\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"Sonny\"\n",
      "            }\n",
      "        ],\n",
      "        \"environment\": \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
      "        \"description\": \"Sonny quietly enters room from right, adjusting disheveled clothes. Don leans forward at desk, expression softening to business-like focus. Johnny stands center, straightening posture. Hagen observes from left corner. Atmosphere shifts from personal rebuke to business discussion.\"\n",
      "    },\n",
      "    {\n",
      "        \"scene_number\": 3,\n",
      "        \"shot_type\": \"Close-Up\",\n",
      "        \"orientation\": \"Front View\",\n",
      "        \"characters\": [\n",
      "            {\n",
      "                \"name\": \"Don Vito Corleone\"\n",
      "            }\n",
      "        ],\n",
      "        \"environment\": \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
      "        \"description\": \"Don Corleone's face fills frame, stern and contemplative. Eyes narrowed, jaw set firmly. Saying 'I'll make him an offer he can't refuse' with quiet, confident menace. Power and authority emanate from his expression.\"\n",
      "    },\n",
      "    {\n",
      "        \"scene_number\": 4,\n",
      "        \"shot_type\": \"Medium Close-Up\",\n",
      "        \"orientation\": \"Three-Quarters View\",\n",
      "        \"characters\": [\n",
      "            {\n",
      "                \"name\": \"Don Vito Corleone\"\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"Johnny Fontane\"\n",
      "            }\n",
      "        ],\n",
      "        \"environment\": \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
      "        \"description\": \"Don Corleone escorts Johnny to door, pinching his cheek firmly. Don's expression shows affection mixed with dominance. Johnny winces slightly at pain while showing relief and gratitude. Door frame visible on right edge of shot.\"\n",
      "    },\n",
      "    {\n",
      "        \"scene_number\": 5,\n",
      "        \"shot_type\": \"Medium Shot\",\n",
      "        \"orientation\": \"Front View\",\n",
      "        \"characters\": [\n",
      "            {\n",
      "                \"name\": \"Don Vito Corleone\"\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"Tom Hagen\"\n",
      "            }\n",
      "        ],\n",
      "        \"environment\": \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
      "        \"description\": \"Don Corleone turns from closed door, small smile fading to serious business expression. Hagen stands attentively near desk, notepad ready. Don moves toward chair, shoulders slightly hunched, gold ring catching light as he gestures.\"\n",
      "    },\n",
      "    {\n",
      "        \"scene_number\": 6,\n",
      "        \"shot_type\": \"Over-the-Shoulder Shot\",\n",
      "        \"orientation\": \"Profile View\",\n",
      "        \"characters\": [\n",
      "            {\n",
      "                \"name\": \"Don Vito Corleone\"\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"Tom Hagen\"\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"Sonny\"\n",
      "            }\n",
      "        ],\n",
      "        \"environment\": \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
      "        \"description\": \"Camera over Don's shoulder, facing Hagen and Sonny. Don's gray-black hair and dark suit visible in foreground. Hagen's face shows respectful attention. Sonny stands beside him, now composed. Don's voice carries weight as he issues final instructions about hospital visit.\"\n",
      "    },\n",
      "    {\n",
      "        \"scene_number\": 7,\n",
      "        \"shot_type\": \"Medium Shot\",\n",
      "        \"orientation\": \"Front View\",\n",
      "        \"characters\": [\n",
      "            {\n",
      "                \"name\": \"Don Vito Corleone\"\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"Tom Hagen\"\n",
      "            }\n",
      "        ],\n",
      "        \"environment\": \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
      "        \"description\": \"Don Corleone turns to Hagen, expression serious. Hagen stands attentively near desk, notepad ready. Don gestures with hands, highlighting points. Weight of family business hangs in the air.\"\n",
      "    },\n",
      "    {\n",
      "        \"scene_number\": 8,\n",
      "        \"shot_type\": \"Over-the-Shoulder Shot\",\n",
      "        \"orientation\": \"Profile View\",\n",
      "        \"characters\": [\n",
      "            {\n",
      "                \"name\": \"Don Vito Corleone\"\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"Hagen\"\n",
      "            }\n",
      "        ],\n",
      "        \"environment\": \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
      "        \"description\": \"Camera over Don's shoulder, facing Hagen. Don's gray-black hair and dark suit visible in foreground. Hagen's face shows respect and understanding. Don's final instructions hang in the air, awaiting Hagen's response.\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# scenes = input_to_json(script, characters_dict)\n",
    "# print(json.dumps(scenes, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scenes_to_formatted_prompts(scenes, characters_dict, style=\"storyboard\", prompt_weights=[2, 1.0, 1.2, 1.5, 0.9]):\n",
    "#     \"\"\"\n",
    "#     Converts a list of scenes into structured diffusion model prompts with weights in one pass,\n",
    "#     with a fallback for character names not matching exactly.\n",
    "    \n",
    "#     Parameters:\n",
    "#     - scenes (list): List of scene dictionaries.\n",
    "#     - characters_dict (dict): Dictionary with character details.\n",
    "#     - style (str): Artistic style string (default \"storyboard\").\n",
    "#     - prompt_weights (list): Weights for \"style\", \"environment\", \"shot\", \"description\", and characters.\n",
    "    \n",
    "#     Returns:\n",
    "#     - List of tuples (subprompt_texts, subprompt_weights) for each scene.\n",
    "#     \"\"\"\n",
    "#     # Define weight mapping for non-character keys\n",
    "#     weight_map = {\n",
    "#         \"style\": prompt_weights[0],\n",
    "#         \"environment\": prompt_weights[1],\n",
    "#         \"shot\": prompt_weights[2],\n",
    "#         \"description\": prompt_weights[3]\n",
    "#     }\n",
    "#     character_weight = prompt_weights[4]\n",
    "\n",
    "#     # Determine style string based on input style\n",
    "#     if style == \"storyboard\":\n",
    "#         style_value = \"rough b&w pencil sketch, simple sketch lines, minimal shading, rough hatching, draft-style, J.C. Leyendecker style\"\n",
    "#     else:\n",
    "#         style_value = style\n",
    "\n",
    "#     formatted_results = []\n",
    "\n",
    "#     for scene in scenes:\n",
    "#         subprompts = {}\n",
    "\n",
    "#         # Add each character's prompt with fallback handling\n",
    "#         for i, char in enumerate(scene[\"characters\"]):\n",
    "#             char_name = char[\"name\"]\n",
    "#             char_info = characters_dict.get(char_name)\n",
    "            \n",
    "#             # If not found, try to find a key that contains the given name as a substring\n",
    "#             if not char_info:\n",
    "#                 matching_keys = [key for key in characters_dict if char_name in key]\n",
    "#                 if matching_keys:\n",
    "#                     char_info = characters_dict[matching_keys[0]]\n",
    "#                 else:\n",
    "#                     # Provide a default description if still not found\n",
    "#                     char_info = {\"age\": \"unknown\", \"gender\": \"unknown\", \"hair\": \"unknown\",\n",
    "#                                  \"clothing\": \"unknown\", \"body_type\": \"unknown\"}\n",
    "            \n",
    "#             char_desc = _build_character_description(char_info)\n",
    "#             subprompts[f\"character{i+1}\"] = f\"{char_name}: {char_desc}\"\n",
    "        \n",
    "#         # Add other scene details\n",
    "#         subprompts[\"style\"] = style_value\n",
    "#         subprompts[\"environment\"] = scene[\"environment\"]\n",
    "#         subprompts[\"shot\"] = f\"{scene['shot_type']}, {scene['orientation']}\"\n",
    "#         subprompts[\"description\"] = scene[\"description\"]\n",
    "\n",
    "#         # Prepare lists for texts and weights\n",
    "#         subprompt_texts = []\n",
    "#         subprompt_weights = []\n",
    "#         for key, text in subprompts.items():\n",
    "#             subprompt_texts.append(text)\n",
    "#             if key.startswith(\"character\"):\n",
    "#                 subprompt_weights.append(character_weight)\n",
    "#             else:\n",
    "#                 subprompt_weights.append(weight_map.get(key, 1.0))\n",
    "        \n",
    "#         formatted_results.append((subprompt_texts, subprompt_weights))\n",
    "    \n",
    "#     return formatted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatted_prompts = scenes_to_formatted_prompts(scenes, characters_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['Don Vito Corleone: Italian-American, early 60s, male, slicked-back gray-black hair, stocky, slightly hunched posture, wearing dark three-piece suit, with gold ring on right hand, pocket watch',\n",
       "   'Johnny Fontane: late 30s, male, short, slicked-back black hair, clean shaven, slim and fit, wearing dark, stylish suit with an open collar, with gold ring, cigarette',\n",
       "   'Tom Hagen: German-Irish, early 40s, male, short, neatly combed brown hair, clean-shaven, medium build, upright posture, wearing gray suit, dark tie',\n",
       "   'rough b&w pencil sketch, simple sketch lines, minimal shading, rough hatching, draft-style, J.C. Leyendecker style',\n",
       "   \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
       "   'Medium Shot, Front View',\n",
       "   'Don Corleone stands imposingly behind desk, face stern with righteous anger, pointing finger at Johnny. Johnny appears embarrassed, head slightly bowed. Hagen stands to the right, barely containing laughter. Tension and amusement mix in intimate office atmosphere.'],\n",
       "  [0.9, 0.9, 0.9, 2, 1.0, 1.2, 1.5]),\n",
       " (['Don Vito Corleone: Italian-American, early 60s, male, slicked-back gray-black hair, stocky, slightly hunched posture, wearing dark three-piece suit, with gold ring on right hand, pocket watch',\n",
       "   'Johnny Fontane: late 30s, male, short, slicked-back black hair, clean shaven, slim and fit, wearing dark, stylish suit with an open collar, with gold ring, cigarette',\n",
       "   'Tom Hagen: German-Irish, early 40s, male, short, neatly combed brown hair, clean-shaven, medium build, upright posture, wearing gray suit, dark tie',\n",
       "   'Sonny: Italian-American, early 30s, male, curly, dark brown hair, clean-shaven, athletic build, wearing formal suit, slightly disheveled',\n",
       "   'rough b&w pencil sketch, simple sketch lines, minimal shading, rough hatching, draft-style, J.C. Leyendecker style',\n",
       "   \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
       "   'Two-Shot, Profile View',\n",
       "   'Sonny quietly enters room from right, adjusting disheveled clothes. Don leans forward at desk, expression softening to business-like focus. Johnny stands center, straightening posture. Hagen observes from left corner. Atmosphere shifts from personal rebuke to business discussion.'],\n",
       "  [0.9, 0.9, 0.9, 0.9, 2, 1.0, 1.2, 1.5]),\n",
       " (['Don Vito Corleone: Italian-American, early 60s, male, slicked-back gray-black hair, stocky, slightly hunched posture, wearing dark three-piece suit, with gold ring on right hand, pocket watch',\n",
       "   'rough b&w pencil sketch, simple sketch lines, minimal shading, rough hatching, draft-style, J.C. Leyendecker style',\n",
       "   \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
       "   'Close-Up, Front View',\n",
       "   \"Don Corleone's face fills frame, stern and contemplative. Eyes narrowed, jaw set firmly. Saying 'I'll make him an offer he can't refuse' with quiet, confident menace. Power and authority emanate from his expression.\"],\n",
       "  [0.9, 2, 1.0, 1.2, 1.5]),\n",
       " (['Don Vito Corleone: Italian-American, early 60s, male, slicked-back gray-black hair, stocky, slightly hunched posture, wearing dark three-piece suit, with gold ring on right hand, pocket watch',\n",
       "   'Johnny Fontane: late 30s, male, short, slicked-back black hair, clean shaven, slim and fit, wearing dark, stylish suit with an open collar, with gold ring, cigarette',\n",
       "   'rough b&w pencil sketch, simple sketch lines, minimal shading, rough hatching, draft-style, J.C. Leyendecker style',\n",
       "   \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
       "   'Medium Close-Up, Three-Quarters View',\n",
       "   \"Don Corleone escorts Johnny to door, pinching his cheek firmly. Don's expression shows affection mixed with dominance. Johnny winces slightly at pain while showing relief and gratitude. Door frame visible on right edge of shot.\"],\n",
       "  [0.9, 0.9, 2, 1.0, 1.2, 1.5]),\n",
       " (['Don Vito Corleone: Italian-American, early 60s, male, slicked-back gray-black hair, stocky, slightly hunched posture, wearing dark three-piece suit, with gold ring on right hand, pocket watch',\n",
       "   'Tom Hagen: German-Irish, early 40s, male, short, neatly combed brown hair, clean-shaven, medium build, upright posture, wearing gray suit, dark tie',\n",
       "   'rough b&w pencil sketch, simple sketch lines, minimal shading, rough hatching, draft-style, J.C. Leyendecker style',\n",
       "   \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
       "   'Medium Shot, Front View',\n",
       "   'Don Corleone turns from closed door, small smile fading to serious business expression. Hagen stands attentively near desk, notepad ready. Don moves toward chair, shoulders slightly hunched, gold ring catching light as he gestures.'],\n",
       "  [0.9, 0.9, 2, 1.0, 1.2, 1.5]),\n",
       " (['Don Vito Corleone: Italian-American, early 60s, male, slicked-back gray-black hair, stocky, slightly hunched posture, wearing dark three-piece suit, with gold ring on right hand, pocket watch',\n",
       "   'Tom Hagen: German-Irish, early 40s, male, short, neatly combed brown hair, clean-shaven, medium build, upright posture, wearing gray suit, dark tie',\n",
       "   'Sonny: Italian-American, early 30s, male, curly, dark brown hair, clean-shaven, athletic build, wearing formal suit, slightly disheveled',\n",
       "   'rough b&w pencil sketch, simple sketch lines, minimal shading, rough hatching, draft-style, J.C. Leyendecker style',\n",
       "   \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
       "   'Over-the-Shoulder Shot, Profile View',\n",
       "   \"Camera over Don's shoulder, facing Hagen and Sonny. Don's gray-black hair and dark suit visible in foreground. Hagen's face shows respectful attention. Sonny stands beside him, now composed. Don's voice carries weight as he issues final instructions about hospital visit.\"],\n",
       "  [0.9, 0.9, 0.9, 2, 1.0, 1.2, 1.5]),\n",
       " (['Don Vito Corleone: Italian-American, early 60s, male, slicked-back gray-black hair, stocky, slightly hunched posture, wearing dark three-piece suit, with gold ring on right hand, pocket watch',\n",
       "   'Tom Hagen: German-Irish, early 40s, male, short, neatly combed brown hair, clean-shaven, medium build, upright posture, wearing gray suit, dark tie',\n",
       "   'rough b&w pencil sketch, simple sketch lines, minimal shading, rough hatching, draft-style, J.C. Leyendecker style',\n",
       "   \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
       "   'Medium Shot, Front View',\n",
       "   'Don Corleone turns to Hagen, expression serious. Hagen stands attentively near desk, notepad ready. Don gestures with hands, highlighting points. Weight of family business hangs in the air.'],\n",
       "  [0.9, 0.9, 2, 1.0, 1.2, 1.5]),\n",
       " (['Don Vito Corleone: Italian-American, early 60s, male, slicked-back gray-black hair, stocky, slightly hunched posture, wearing dark three-piece suit, with gold ring on right hand, pocket watch',\n",
       "   'Hagen: German-Irish, early 40s, male, short, neatly combed brown hair, clean-shaven, medium build, upright posture, wearing gray suit, dark tie',\n",
       "   'rough b&w pencil sketch, simple sketch lines, minimal shading, rough hatching, draft-style, J.C. Leyendecker style',\n",
       "   \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
       "   'Over-the-Shoulder Shot, Profile View',\n",
       "   \"Camera over Don's shoulder, facing Hagen. Don's gray-black hair and dark suit visible in foreground. Hagen's face shows respect and understanding. Don's final instructions hang in the air, awaiting Hagen's response.\"],\n",
       "  [0.9, 0.9, 2, 1.0, 1.2, 1.5])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# formatted_prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unique Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78d23f2ab8014692adee2740939ff367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# from diffusers import StableDiffusionPipeline, UniPCMultistepScheduler\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16)\n",
    "# pipe = pipe.to(device)\n",
    "# pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "# pipe.enable_model_cpu_offload()\n",
    "# pipe.enable_attention_slicing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_unique_prompts(formatted_prompts, style_override=\"rough b&w simple pencil sketch, J.C. Leyendecker style,\"):\n",
    "#     \"\"\"\n",
    "#     Given formatted prompts (a list of tuples where each tuple is \n",
    "#     (subprompt_texts, subprompt_weights)), build a unique prompt for each scene\n",
    "#     by concatenating the style, shot prompt, and description.\n",
    "    \n",
    "#     Returns:\n",
    "#         List[str]: Unique prompt strings for each scene.\n",
    "#     \"\"\"\n",
    "#     unique_prompts = []\n",
    "#     for subprompt_texts, _ in formatted_prompts:\n",
    "#         shot_prompt = subprompt_texts[-2]  \n",
    "#         description = subprompt_texts[-1]\n",
    "#         unique_prompt = f\"{style_override} {shot_prompt}: {description}\"\n",
    "#         unique_prompts.append(unique_prompt)\n",
    "#     return unique_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_and_save_images_unique_prompts(formatted_prompts, pipe, save_dir, device,\n",
    "#                                             negative_prompt=\"low quality, photorealistic, 3d render, overly detailed, digital art, painting, vibrant colors, fine art, NSFW\",\n",
    "#                                             num_inference_steps=50):\n",
    "#     \"\"\"\n",
    "#     Generate images using unique prompts built from formatted_prompts.\n",
    "#     \"\"\"\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "#     unique_prompts = build_unique_prompts(formatted_prompts)\n",
    "#     generated_images = []\n",
    "    \n",
    "#     for i, unique_prompt in enumerate(unique_prompts):\n",
    "#         with torch.no_grad():\n",
    "#             output = pipe(prompt=unique_prompt,\n",
    "#                           negative_prompt=negative_prompt,\n",
    "#                           num_inference_steps=num_inference_steps)\n",
    "#         generated_image = output.images[0]\n",
    "#         generated_images.append(generated_image)\n",
    "#         image_path = os.path.join(save_dir, f\"image_{i+1}.png\")\n",
    "#         generated_image.save(image_path)\n",
    "#         print(f\"Image {i+1} saved to {image_path}\")\n",
    "    \n",
    "#     return generated_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "419a3d8a7faa47948462f05b1b3d80e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (79 > 77). Running this sequence through the model will result in indexing errors\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['discussion .']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 1 saved to stories/unique_prompts\\image_1.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f8404d30e7f4cd4820779c69f9962e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 2 saved to stories/unique_prompts\\image_2.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f3aa772e1ef47dea217f707a8f5eece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 3 saved to stories/unique_prompts\\image_3.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e307ba204375485c9d4f6ee53b5ea913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 4 saved to stories/unique_prompts\\image_4.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9007f94074264972b633861e90b9bd18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['instructions about hospital visit .']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 5 saved to stories/unique_prompts\\image_5.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b76b05aad704d1bb637f9a7a7c1c1aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 6 saved to stories/unique_prompts\\image_6.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15e3745cd66945d5ae119513f8759fd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 7 saved to stories/unique_prompts\\image_7.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "359b655723ba4fa8adfb8831a0605df3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 8 saved to stories/unique_prompts\\image_8.png\n"
     ]
    }
   ],
   "source": [
    "# save_directory = \"stories/unique_prompts\"\n",
    "# generated_images = generate_and_save_images_unique_prompts(formatted_prompts, pipe, save_directory, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subprompt Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# from diffusers import StableDiffusionPipeline, UniPCMultistepScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ac4219275584a84bdc5bda1fd01dfa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16)\n",
    "# pipe = pipe.to(device)\n",
    "# pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "# pipe.enable_model_cpu_offload()\n",
    "# pipe.enable_attention_slicing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def weighted_sum_prompt_embeddings(pipe, subprompt_texts, subprompt_weights, device, num_images_per_prompt=1):\n",
    "#     \"\"\"\n",
    "#     Computes a weighted sum of text embeddings for a list of subprompts.\n",
    "    \n",
    "#     Args:\n",
    "#         pipe: The Stable Diffusion pipeline instance.\n",
    "#         subprompt_texts (List[str]): List of subprompt strings.\n",
    "#         subprompt_weights (List[float]): Corresponding weights for each subprompt.\n",
    "#         device (str): The device to run on (\"cuda\" or \"cpu\").\n",
    "#         num_images_per_prompt (int): Number of images to generate per prompt.\n",
    "    \n",
    "#     Returns:\n",
    "#         torch.Tensor: Combined prompt embeddings of shape (batch_size * num_images_per_prompt, seq_len, embed_dim)\n",
    "#     \"\"\"\n",
    "#     encoded_prompts = []\n",
    "#     for text in subprompt_texts:\n",
    "#         # Tokenize the subprompt text\n",
    "#         text_inputs = pipe.tokenizer(\n",
    "#             text,\n",
    "#             padding=\"max_length\",\n",
    "#             max_length=pipe.tokenizer.model_max_length,\n",
    "#             truncation=True,\n",
    "#             return_tensors=\"pt\",\n",
    "#         )\n",
    "#         input_ids = text_inputs.input_ids.to(device)\n",
    "#         attention_mask = text_inputs.attention_mask.to(device) if \"attention_mask\" in text_inputs else None\n",
    "\n",
    "#         # Encode the subprompt into text embeddings\n",
    "#         text_embeds = pipe.text_encoder(input_ids, attention_mask=attention_mask)[0]\n",
    "#         encoded_prompts.append(text_embeds)\n",
    "    \n",
    "#     # Compute the weighted sum of the embeddings\n",
    "#     weighted_embedding = sum(weight * embeds for weight, embeds in zip(subprompt_weights, encoded_prompts))\n",
    "#     weight_total = sum(subprompt_weights)\n",
    "#     combined_embedding = weighted_embedding / weight_total  # Normalize if desired\n",
    "\n",
    "#     # Duplicate embeddings for each image per prompt if necessary\n",
    "#     batch_size, seq_len, embed_dim = combined_embedding.shape\n",
    "#     combined_embedding = combined_embedding.repeat(1, num_images_per_prompt, 1)\n",
    "#     combined_embedding = combined_embedding.view(batch_size * num_images_per_prompt, seq_len, embed_dim)\n",
    "    \n",
    "#     return combined_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_and_save_images_prompt_weights(scenes, characters_dict, pipe, save_dir, device,\n",
    "#                              negative_prompt=\"low quality, photorealistic, 3d render, overly detailed, digital art, painting, vibrant colors, fine art, NSFW\",\n",
    "#                              num_inference_steps=50):\n",
    "#     \"\"\"\n",
    "#     Generate images for each scene using prompt embeddings from the provided pipeline\n",
    "#     and save each image to the specified directory with a unique filename.\n",
    "\n",
    "#     Args:\n",
    "#         scenes (list): List of scene objects.\n",
    "#         characters_dict (dict): Dictionary of character descriptions.\n",
    "#         pipe: The Stable Diffusion pipeline instance.\n",
    "#         save_dir (str): The directory where images will be saved.\n",
    "#         device (str): The device to use (\"cuda\" or \"cpu\").\n",
    "#         negative_prompt (str, optional): Negative prompt to steer generation.\n",
    "#         num_inference_steps (int, optional): Number of inference steps for image generation.\n",
    "\n",
    "#     Returns:\n",
    "#         list: List of generated PIL.Image objects.\n",
    "#     \"\"\"\n",
    "#     print(\"Generating images...\")\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "#     generated_images = []\n",
    "#     for i, scene in enumerate(scenes):\n",
    "#         # scene_prompts = _scene_to_prompts(scene, characters_dict)\n",
    "#         # subprompt_texts, subprompt_weights = format_subprompts_for_diffusion(scene_prompts)\n",
    "#         subprompt_texts, subprompt_weights = scenes_to_formatted_prompts([scene], characters_dict)[0]\n",
    "#         combined_embeddings = weighted_sum_prompt_embeddings(pipe, subprompt_texts, subprompt_weights, device)\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             output = pipe(prompt_embeds=combined_embeddings,\n",
    "#                           negative_prompt=negative_prompt,\n",
    "#                           num_inference_steps=num_inference_steps)\n",
    "#         generated_image = output.images[0]\n",
    "#         generated_images.append(generated_image)\n",
    "#         image_path = os.path.join(save_dir, f\"image_{i+1}.png\")\n",
    "#         generated_image.save(image_path)\n",
    "#         print(f\"Image {i+1} saved to {image_path}\")\n",
    "        \n",
    "#     return generated_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceb5c6bc52e4439499b67c447114e051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 1 saved to stories/prompt_weight\\image_1.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1099750b5fa41229c70dded67b40a09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 2 saved to stories/prompt_weight\\image_2.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e9b65ee3944d768c425adccd9f0d3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 3 saved to stories/prompt_weight\\image_3.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec89ef76e669474dbd17485aade860fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 4 saved to stories/prompt_weight\\image_4.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a96d572bcd00470c97937863dd064163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 5 saved to stories/prompt_weight\\image_5.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b863ef9109c54eee9d3f46f4af98900c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 6 saved to stories/prompt_weight\\image_6.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd8d8c9c39b94358a960055a97b946ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 7 saved to stories/prompt_weight\\image_7.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8414fd987a1a4b18b1e5a0f73390d467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 8 saved to stories/prompt_weight\\image_8.png\n"
     ]
    }
   ],
   "source": [
    "# save_directory = \"stories/prompt_weight\"\n",
    "# generated_images = generate_and_save_images_prompt_weights(scenes, characters_dict, pipe, save_directory, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified Classifier-Free Guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# from diffusers import StableDiffusionPipeline, UniPCMultistepScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def encode_subprompt(pipe: StableDiffusionPipeline, text: str, device: str = \"cuda\"):\n",
    "#     \"\"\"\n",
    "#     Tokenize and encode a single subprompt into a [batch_size=1, seq_len, hidden_dim] embedding.\n",
    "#     \"\"\"\n",
    "#     text_inputs = pipe.tokenizer(\n",
    "#         text,\n",
    "#         padding=\"max_length\",\n",
    "#         max_length=pipe.tokenizer.model_max_length,\n",
    "#         truncation=True,\n",
    "#         return_tensors=\"pt\",\n",
    "#     )\n",
    "#     text_embeds = pipe.text_encoder(\n",
    "#         text_inputs.input_ids.to(device),\n",
    "#         attention_mask=text_inputs.attention_mask.to(device)\n",
    "#     )[0]\n",
    "#     return text_embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Unconditional Pass + Multiple Conditional Passes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\hat{\\epsilon}_{\\text{cond\\_combined}}=\\frac{1}{\\sum_{i=1}^nw_i}\\sum_{i=1}^nw_i\\hat{\\epsilon}_{\\text{cond}_i}$\n",
    "where we have one pass per subprompt to get $\\hat{\\epsilon}_{\\text{cond}_i}$ and $n$ is the number of subprompts.\n",
    "Then the classifier free guidance with scale $g$ is $$\\hat{\\epsilon}=\\hat{\\epsilon}_{\\text{uncond}}+g(\\hat{\\epsilon}_{\\text{cond\\_combined}}-\\hat{\\epsilon}_{\\text{uncond}})$$\n",
    "where we have one unconditional pass at each step to get $\\hat{\\epsilon}_{\\text{uncond}}$\n",
    "\n",
    "- Total UNet calls per step: $1+n$\n",
    "- Each subprompt has a relative weight but they all share the same baseline unconditional pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiPromptPipelineApproach1(StableDiffusionPipeline):\n",
    "#     \"\"\"\n",
    "#     Multi-Prompt CFG with a SINGLE unconditional pass:\n",
    "#       - At each diffusion step:\n",
    "#         1. uncond_out = UNet(latent, uncond_embeds)\n",
    "#         2. cond_out_i = UNet(latent, cond_embeds_i) for each subprompt i\n",
    "#         3. cond_combined = weighted average of all cond_out_i\n",
    "#         4. final_out = uncond_out + guidance_scale*(cond_combined - uncond_out)\n",
    "#     \"\"\"\n",
    "\n",
    "#     @torch.no_grad()\n",
    "#     def __call__(\n",
    "#         self,\n",
    "#         subprompt_embeds: list[torch.Tensor],\n",
    "#         subprompt_weights: list[float],\n",
    "#         uncond_embeds: torch.Tensor,\n",
    "#         height: int = 512,\n",
    "#         width: int = 512,\n",
    "#         guidance_scale: float = 7.5,\n",
    "#         num_inference_steps: int = 50,\n",
    "#         generator: torch.Generator = None,\n",
    "#         latents: torch.Tensor = None,\n",
    "#         output_type: str = \"pil\",\n",
    "#         return_dict: bool = True,\n",
    "#         **kwargs\n",
    "#     ):\n",
    "#         device = self._execution_device\n",
    "#         batch_size = uncond_embeds.shape[0]\n",
    "#         num_subprompts = len(subprompt_embeds)\n",
    "\n",
    "#         if num_subprompts != len(subprompt_weights):\n",
    "#             raise ValueError(\"subprompt_embeds and subprompt_weights must have the same length.\")\n",
    "\n",
    "#         # 1. Validate or fallback to default height/width\n",
    "#         if not height or not width:\n",
    "#             height, width = self._default_height_width()\n",
    "\n",
    "#         # 2. Set timesteps on the scheduler\n",
    "#         self.scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "#         timesteps = self.scheduler.timesteps\n",
    "\n",
    "#         # 3. Prepare latents\n",
    "#         if latents is None:\n",
    "#             shape = (batch_size, self.unet.config.in_channels, height // 8, width // 8)\n",
    "#             latents = torch.randn(shape, generator=generator, device=device, dtype=uncond_embeds.dtype)\n",
    "#             latents = latents * self.scheduler.init_noise_sigma\n",
    "#         else:\n",
    "#             latents = latents.to(device)\n",
    "\n",
    "#         # 4. Diffusion loop\n",
    "#         for i, t in enumerate(timesteps):\n",
    "#             latent_model_input = self.scheduler.scale_model_input(latents, t)\n",
    "\n",
    "#             # (A) Unconditional pass\n",
    "#             uncond_out = self.unet(latent_model_input, t, encoder_hidden_states=uncond_embeds, **kwargs).sample\n",
    "\n",
    "#             # (B) Conditional passes (one per subprompt)\n",
    "#             cond_outs = []\n",
    "#             for cond_embed in subprompt_embeds:\n",
    "#                 out = self.unet(latent_model_input, t, encoder_hidden_states=cond_embed, **kwargs).sample\n",
    "#                 cond_outs.append(out)\n",
    "\n",
    "#             # (C) Weighted average of conditional outputs\n",
    "#             total_w = sum(subprompt_weights)\n",
    "#             cond_combined = sum(w * o for w, o in zip(subprompt_weights, cond_outs)) / total_w\n",
    "\n",
    "#             # (D) Classifier-Free Guidance\n",
    "#             guided_out = uncond_out + guidance_scale * (cond_combined - uncond_out)\n",
    "\n",
    "#             # (E) Step\n",
    "#             latents = self.scheduler.step(guided_out, t, latents, **kwargs).prev_sample\n",
    "\n",
    "#         # 5. Decode latents\n",
    "#         if output_type == \"latent\":\n",
    "#             if return_dict:\n",
    "#                 from diffusers.pipelines.stable_diffusion.pipeline_output import StableDiffusionPipelineOutput\n",
    "#                 return StableDiffusionPipelineOutput(images=latents, nsfw_content_detected=None)\n",
    "#             return latents\n",
    "\n",
    "#         image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=False)[0]\n",
    "#         image = self.image_processor.postprocess(image, output_type=output_type)\n",
    "\n",
    "#         if return_dict:\n",
    "#             from diffusers.pipelines.stable_diffusion.pipeline_output import StableDiffusionPipelineOutput\n",
    "#             return StableDiffusionPipelineOutput(images=image, nsfw_content_detected=None)\n",
    "#         return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Approach 1 pipeline for scenes...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94324b6c33d24f90bee893a9041b9980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # --- Load the Multi-Prompt Approach 1 pipeline ---\n",
    "# print(\"Loading Approach 1 pipeline for scenes...\")\n",
    "# pipe1 = MultiPromptPipelineApproach1.from_pretrained(\n",
    "#     \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16\n",
    "# ).to(\"cuda\")\n",
    "# pipe1.scheduler = UniPCMultistepScheduler.from_config(pipe1.scheduler.config)\n",
    "# pipe1.enable_model_cpu_offload()\n",
    "# pipe1.enable_attention_slicing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Approach 1 pipeline...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19c0d53a5241437bbecff9dd0c52cdd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "####### EXAMPLE\n",
    "# # Subprompts\n",
    "# subprompts = [\n",
    "#     \"ancient forest, misty atmosphere\",\n",
    "#     \"mysterious ruins in the distance\"\n",
    "# ]\n",
    "# # Encode each subprompt\n",
    "# subprompt_embeds_1 = [encode_subprompt(pipe1, sp) for sp in subprompts]\n",
    "\n",
    "# # Encode unconditional\n",
    "# uncond_embeds_1 = encode_subprompt(pipe1, \"\")  # blank or negative prompt\n",
    "\n",
    "# # Generate with approach 1\n",
    "\n",
    "# # Weights for each subprompt\n",
    "# weights_1 = [2.0, 5.0]\n",
    "# print(\"Generating image with Approach 1 (single unconditional pass)...\")\n",
    "# output1 = pipe1(\n",
    "#     subprompt_embeds=subprompt_embeds_1,\n",
    "#     subprompt_weights=weights_1,\n",
    "#     uncond_embeds=uncond_embeds_1,\n",
    "#     guidance_scale=7.5,\n",
    "#     num_inference_steps=25\n",
    "# )\n",
    "# output1.images[0].save(\"approach1_result.png\")\n",
    "# print(\"Saved approach1_result.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Generate images for each scene ---\n",
    "# def generate_and_save_images_multi_prompt(scenes, characters_dict, pipe, save_dir, device,\n",
    "#                                           num_inference_steps=50, guidance_scale=7.5):\n",
    "#     \"\"\"\n",
    "#     Generate images for each scene using the Multi-Prompt pipeline and save each image to the specified directory.\n",
    "    \n",
    "#     Args:\n",
    "#         scenes (list): List of scene objects (each scene is a dict).\n",
    "#         characters_dict (dict): Dictionary of character descriptions.\n",
    "#         pipe: The MultiPromptPipelineApproach1 pipeline instance.\n",
    "#         save_dir (str): Directory where images will be saved.\n",
    "#         device (str): Device to use (e.g., \"cuda\" or \"cpu\").\n",
    "#         num_inference_steps (int, optional): Number of diffusion steps.\n",
    "#         guidance_scale (float, optional): Guidance scale for classifier-free guidance.\n",
    "        \n",
    "#     Returns:\n",
    "#         list: List of generated PIL.Image objects.\n",
    "#     \"\"\"\n",
    "#     import os\n",
    "#     import torch\n",
    "\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "#     generated_images = []\n",
    "#     uncond_embeds = encode_subprompt(pipe,\n",
    "#                                      \"low quality, photorealistic, 3d render, overly detailed, digital art, painting, vibrant colors, fine art, NSFW\",\n",
    "#                                      device=device)\n",
    "\n",
    "#     # Iterate over scenes\n",
    "#     for i, scene in enumerate(scenes):\n",
    "#         # Convert the scene to subprompts and their corresponding weights\n",
    "#         subprompt_texts, subprompt_weights = scenes_to_formatted_prompts([scene], characters_dict)[0]\n",
    "\n",
    "#         # Encode each subprompt into an embedding\n",
    "#         subprompt_embeds = [encode_subprompt(pipe, sp, device=device) for sp in subprompt_texts]\n",
    "#         # Encode unconditional (negative/blank) prompt for the baseline\n",
    "#         # uncond_embeds = encode_subprompt(pipe, \"\", device=device)\n",
    "\n",
    "#         print(f\"Generating image for scene {i+1}...\")\n",
    "#         with torch.no_grad():\n",
    "#             output = pipe(\n",
    "#                 subprompt_embeds=subprompt_embeds,\n",
    "#                 subprompt_weights=subprompt_weights,\n",
    "#                 uncond_embeds=uncond_embeds,\n",
    "#                 guidance_scale=guidance_scale,\n",
    "#                 num_inference_steps=num_inference_steps\n",
    "#             )\n",
    "#         generated_image = output.images[0]\n",
    "#         generated_images.append(generated_image)\n",
    "#         image_path = os.path.join(save_dir, f\"scene_{i+1}.png\")\n",
    "#         generated_image.save(image_path)\n",
    "#         print(f\"Image {i+1} saved to {image_path}\")\n",
    "\n",
    "#     return generated_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating image for scene 1...\n",
      "Image 1 saved to stories/multi_prompt_approach1\\scene_1.png\n",
      "Generating image for scene 2...\n",
      "Image 2 saved to stories/multi_prompt_approach1\\scene_2.png\n",
      "Generating image for scene 3...\n",
      "Image 3 saved to stories/multi_prompt_approach1\\scene_3.png\n",
      "Generating image for scene 4...\n",
      "Image 4 saved to stories/multi_prompt_approach1\\scene_4.png\n",
      "Generating image for scene 5...\n",
      "Image 5 saved to stories/multi_prompt_approach1\\scene_5.png\n",
      "Generating image for scene 6...\n",
      "Image 6 saved to stories/multi_prompt_approach1\\scene_6.png\n",
      "Generating image for scene 7...\n",
      "Image 7 saved to stories/multi_prompt_approach1\\scene_7.png\n",
      "Generating image for scene 8...\n",
      "Image 8 saved to stories/multi_prompt_approach1\\scene_8.png\n"
     ]
    }
   ],
   "source": [
    "# save_directory = \"stories/multi_prompt_approach1\"\n",
    "# generated_images = generate_and_save_images_multi_prompt(scenes, characters_dict, pipe1, save_directory, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Unconditional Passes (One per Subprompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have $$\\hat{\\epsilon}=\\hat{\\epsilon}_{\\text{uncond}}+g\\sum_{i=1}^nw_i(\\hat{\\epsilon}_{\\text{cond}_i}-\\hat{\\epsilon}_{\\text{uncond}_i})$$\n",
    "- Total UNet calls per step: $1+2n$ (One global unconditional + two passes for each subprompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiPromptPipelineApproach2(StableDiffusionPipeline):\n",
    "#     \"\"\"\n",
    "#     Multi-Prompt CFG with MULTIPLE unconditional passes:\n",
    "#       - 1 global unconditional pass per step: e_uncond\n",
    "#       - For each subprompt i:\n",
    "#           e_uncond_i (subprompt-specific unconditional)\n",
    "#           e_cond_i    (subprompt conditional)\n",
    "#       - Combine: e = e_uncond + g * sum_i[ w_i * ( e_cond_i - e_uncond_i ) ]\n",
    "#     \"\"\"\n",
    "\n",
    "#     @torch.no_grad()\n",
    "#     def __call__(\n",
    "#         self,\n",
    "#         global_uncond_embeds: torch.Tensor,\n",
    "#         subprompt_pairs: list[tuple[torch.Tensor, torch.Tensor]],\n",
    "#         subprompt_weights: list[float],\n",
    "#         guidance_scale: float = 7.5,\n",
    "#         height: int = 512,\n",
    "#         width: int = 512,\n",
    "#         num_inference_steps: int = 50,\n",
    "#         generator: torch.Generator = None,\n",
    "#         latents: torch.Tensor = None,\n",
    "#         output_type: str = \"pil\",\n",
    "#         return_dict: bool = True,\n",
    "#         **kwargs\n",
    "#     ):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             global_uncond_embeds (Tensor): [batch, seq_len, hidden_dim] for the entire prompt's unconditional pass.\n",
    "#             subprompt_pairs (list of (uncond_i, cond_i)):\n",
    "#                 Each element is a tuple: (uncond_embeds_i, cond_embeds_i).\n",
    "#             subprompt_weights (list[float]): Weights w_i for each subprompt i.\n",
    "#         \"\"\"\n",
    "#         device = self._execution_device\n",
    "#         batch_size = global_uncond_embeds.shape[0]\n",
    "#         num_subprompts = len(subprompt_pairs)\n",
    "\n",
    "#         if num_subprompts != len(subprompt_weights):\n",
    "#             raise ValueError(\"subprompt_pairs and subprompt_weights must have the same length.\")\n",
    "\n",
    "#         # 1. Validate or fallback to default\n",
    "#         if not height or not width:\n",
    "#             height, width = self._default_height_width()\n",
    "\n",
    "#         # 2. Scheduler timesteps\n",
    "#         self.scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "#         timesteps = self.scheduler.timesteps\n",
    "\n",
    "#         # 3. Prepare latents\n",
    "#         if latents is None:\n",
    "#             shape = (batch_size, self.unet.config.in_channels, height // 8, width // 8)\n",
    "#             latents = torch.randn(shape, generator=generator, device=device, dtype=global_uncond_embeds.dtype)\n",
    "#             latents = latents * self.scheduler.init_noise_sigma\n",
    "#         else:\n",
    "#             latents = latents.to(device)\n",
    "\n",
    "#         # 4. Diffusion loop\n",
    "#         for i, t in enumerate(timesteps):\n",
    "#             latent_model_input = self.scheduler.scale_model_input(latents, t)\n",
    "\n",
    "#             # (A) Single global unconditional pass\n",
    "#             e_uncond_global = self.unet(\n",
    "#                 latent_model_input, t, encoder_hidden_states=global_uncond_embeds, **kwargs\n",
    "#             ).sample\n",
    "\n",
    "#             # (B) For each subprompt: unconditional + conditional\n",
    "#             sub_deltas = []\n",
    "#             for (uncond_i, cond_i), w in zip(subprompt_pairs, subprompt_weights):\n",
    "#                 e_uncond_i = self.unet(latent_model_input, t, encoder_hidden_states=uncond_i, **kwargs).sample\n",
    "#                 e_cond_i = self.unet(latent_model_input, t, encoder_hidden_states=cond_i, **kwargs).sample\n",
    "\n",
    "#                 # Delta for subprompt i\n",
    "#                 delta_i = w * (e_cond_i - e_uncond_i)\n",
    "#                 sub_deltas.append(delta_i)\n",
    "\n",
    "#             # (C) Combine sub-deltas\n",
    "#             sum_deltas = sum(sub_deltas)  # sum_i w_i ( e_cond_i - e_uncond_i )\n",
    "\n",
    "#             # (D) Final output\n",
    "#             guided_out = e_uncond_global + guidance_scale * sum_deltas\n",
    "\n",
    "#             # (E) Scheduler step\n",
    "#             latents = self.scheduler.step(guided_out, t, latents, **kwargs).prev_sample\n",
    "\n",
    "#         # 5. Decode\n",
    "#         if output_type == \"latent\":\n",
    "#             if return_dict:\n",
    "#                 from diffusers.pipelines.stable_diffusion.pipeline_output import StableDiffusionPipelineOutput\n",
    "#                 return StableDiffusionPipelineOutput(images=latents, nsfw_content_detected=None)\n",
    "#             return latents\n",
    "\n",
    "#         image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=False)[0]\n",
    "#         image = self.image_processor.postprocess(image, output_type=output_type)\n",
    "\n",
    "#         if return_dict:\n",
    "#             from diffusers.pipelines.stable_diffusion.pipeline_output import StableDiffusionPipelineOutput\n",
    "#             return StableDiffusionPipelineOutput(images=image, nsfw_content_detected=None)\n",
    "#         return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Approach 2 pipeline...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a716b70c12248b1a0fa29af3be7c159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(\"Loading Approach 2 pipeline...\")\n",
    "# pipe2 = MultiPromptPipelineApproach2.from_pretrained(\n",
    "#     \"runwayml/stable-diffusion-v1-5\",\n",
    "#     torch_dtype=torch.float16\n",
    "# ).to(\"cuda\")\n",
    "# pipe2.scheduler = UniPCMultistepScheduler.from_config(pipe2.scheduler.config)\n",
    "# pipe2.enable_model_cpu_offload()\n",
    "# pipe2.enable_attention_slicing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### EXAMPLE\n",
    "# # Suppose we want environment and style separately\n",
    "# global_uncond = encode_subprompt(pipe2, \"\")  # global unconditional\n",
    "# env_uncond = encode_subprompt(pipe2, \"\")     # unconditional for environment\n",
    "# env_cond   = encode_subprompt(pipe2, \"ancient forest, misty atmosphere\")\n",
    "# style_uncond = encode_subprompt(pipe2, \"\")   # unconditional for style\n",
    "# style_cond   = encode_subprompt(pipe2, \"cinematic style, high contrast\")\n",
    "\n",
    "# # subprompt_pairs = [ (uncond_env, cond_env), (uncond_style, cond_style) ]\n",
    "# subprompt_pairs_2 = [\n",
    "#     (env_uncond, env_cond),\n",
    "#     (style_uncond, style_cond)\n",
    "# ]\n",
    "\n",
    "# weights_2 = [1.5, 1.8]\n",
    "# print(\"Generating image with Approach 2 (multiple unconditional passes)...\")\n",
    "# output2 = pipe2(\n",
    "#     global_uncond_embeds=global_uncond,\n",
    "#     subprompt_pairs=subprompt_pairs_2,\n",
    "#     subprompt_weights=weights_2,\n",
    "#     guidance_scale=7.5,\n",
    "#     num_inference_steps=25\n",
    "# )\n",
    "# output2.images[0].save(\"approach2_result.png\")\n",
    "# print(\"Saved approach2_result.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_and_save_images_multi_prompt2(scenes, characters_dict, pipe, save_dir, device,\n",
    "#                                              num_inference_steps=50, guidance_scale=7.5):\n",
    "#     \"\"\"\n",
    "#     Generate images for each scene using Multi-Prompt Approach 2 (multiple unconditional passes)\n",
    "#     and save each image to the specified directory.\n",
    "    \n",
    "#     Args:\n",
    "#         scenes (list): List of scene objects (each scene is a dict).\n",
    "#         characters_dict (dict): Dictionary of character descriptions.\n",
    "#         pipe: The MultiPromptPipelineApproach2 pipeline instance.\n",
    "#         save_dir (str): Directory where images will be saved.\n",
    "#         device (str): Device to use (e.g., \"cuda\" or \"cpu\").\n",
    "#         num_inference_steps (int, optional): Number of diffusion steps.\n",
    "#         guidance_scale (float, optional): Guidance scale for classifier-free guidance.\n",
    "        \n",
    "#     Returns:\n",
    "#         list: List of generated PIL.Image objects.\n",
    "#     \"\"\"\n",
    "#     import os\n",
    "#     import torch\n",
    "\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "#     generated_images = []\n",
    "\n",
    "#     for i, scene in enumerate(scenes):\n",
    "#         # Get subprompt texts and corresponding weights for the scene.\n",
    "#         subprompt_texts, subprompt_weights = scenes_to_formatted_prompts([scene], characters_dict)[0]\n",
    "\n",
    "#         # Encode the global unconditional prompt once.\n",
    "#         global_uncond_embeds = encode_subprompt(pipe, \"\", device=device)\n",
    "\n",
    "#         # For each subprompt, encode a pair: (unconditional, conditional)\n",
    "#         subprompt_pairs = []\n",
    "#         for sp in subprompt_texts:\n",
    "#             uncond_i = encode_subprompt(pipe, \"\", device=device)\n",
    "#             cond_i = encode_subprompt(pipe, sp, device=device)\n",
    "#             subprompt_pairs.append((uncond_i, cond_i))\n",
    "\n",
    "#         print(f\"Generating image for scene {i+1} using Approach 2...\")\n",
    "#         with torch.no_grad():\n",
    "#             output = pipe(\n",
    "#                 global_uncond_embeds=global_uncond_embeds,\n",
    "#                 subprompt_pairs=subprompt_pairs,\n",
    "#                 subprompt_weights=subprompt_weights,\n",
    "#                 guidance_scale=guidance_scale,\n",
    "#                 num_inference_steps=num_inference_steps\n",
    "#             )\n",
    "#         generated_image = output.images[0]\n",
    "#         generated_images.append(generated_image)\n",
    "#         image_path = os.path.join(save_dir, f\"scene_{i+1}_approach2.png\")\n",
    "#         generated_image.save(image_path)\n",
    "#         print(f\"Image {i+1} saved to {image_path}\")\n",
    "\n",
    "#     return generated_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating image for scene 1 using Approach 2...\n",
      "Image 1 saved to stories/multi_prompt_approach2\\scene_1_approach2.png\n",
      "Generating image for scene 2 using Approach 2...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[0;32m      2\u001b[0m save_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstories/multi_prompt_approach2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m generated_images \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_and_save_images_multi_prompt2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscenes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcharacters_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipe2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[30], line 41\u001b[0m, in \u001b[0;36mgenerate_and_save_images_multi_prompt2\u001b[1;34m(scenes, characters_dict, pipe, save_dir, device, num_inference_steps, guidance_scale)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating image for scene \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m using Approach 2...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 41\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mglobal_uncond_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_uncond_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubprompt_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprompt_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubprompt_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprompt_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguidance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_inference_steps\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m generated_image \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mimages[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     49\u001b[0m generated_images\u001b[38;5;241m.\u001b[39mappend(generated_image)\n",
      "File \u001b[1;32mc:\\Users\\sandr\\anaconda3\\envs\\mldl-ecole\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[27], line 70\u001b[0m, in \u001b[0;36mMultiPromptPipelineApproach2.__call__\u001b[1;34m(self, global_uncond_embeds, subprompt_pairs, subprompt_weights, guidance_scale, height, width, num_inference_steps, generator, latents, output_type, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (uncond_i, cond_i), w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(subprompt_pairs, subprompt_weights):\n\u001b[0;32m     69\u001b[0m     e_uncond_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munet(latent_model_input, t, encoder_hidden_states\u001b[38;5;241m=\u001b[39muncond_i, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39msample\n\u001b[1;32m---> 70\u001b[0m     e_cond_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_model_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcond_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;66;03m# Delta for subprompt i\u001b[39;00m\n\u001b[0;32m     73\u001b[0m     delta_i \u001b[38;5;241m=\u001b[39m w \u001b[38;5;241m*\u001b[39m (e_cond_i \u001b[38;5;241m-\u001b[39m e_uncond_i)\n",
      "File \u001b[1;32mc:\\Users\\sandr\\anaconda3\\envs\\mldl-ecole\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sandr\\anaconda3\\envs\\mldl-ecole\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\sandr\\anaconda3\\envs\\mldl-ecole\\Lib\\site-packages\\accelerate\\hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_forward\u001b[39m(module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 165\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hf_hook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mno_grad:\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[1;32mc:\\Users\\sandr\\anaconda3\\envs\\mldl-ecole\\Lib\\site-packages\\accelerate\\hooks.py:707\u001b[0m, in \u001b[0;36mCpuOffload.pre_forward\u001b[1;34m(self, module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev_module_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev_module_hook\u001b[38;5;241m.\u001b[39moffload()\n\u001b[1;32m--> 707\u001b[0m     \u001b[43mclear_device_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    708\u001b[0m module\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device)\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m send_to_device(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device), send_to_device(kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device)\n",
      "File \u001b[1;32mc:\\Users\\sandr\\anaconda3\\envs\\mldl-ecole\\Lib\\site-packages\\accelerate\\utils\\memory.py:60\u001b[0m, in \u001b[0;36mclear_device_cache\u001b[1;34m(garbage_collection)\u001b[0m\n\u001b[0;32m     58\u001b[0m     torch\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_cuda_available():\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sandr\\anaconda3\\envs\\mldl-ecole\\Lib\\site-packages\\torch\\cuda\\memory.py:192\u001b[0m, in \u001b[0;36mempty_cache\u001b[1;34m()\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Release all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m`nvidia-smi`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[1;32m--> 192\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # Example usage:\n",
    "# save_directory = \"stories/multi_prompt_approach2\"\n",
    "# generated_images = generate_and_save_images_multi_prompt2(scenes, characters_dict, pipe2, save_directory, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopped this because it's extremely slow (20 min for one image) and it's not good either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from together import Together\n",
    "# from pydantic import BaseModel  # (assumes you have a SceneList schema)\n",
    "from src.prompt_scheme import SceneList\n",
    "from diffusers import StableDiffusionPipeline, UniPCMultistepScheduler\n",
    "from src.models import MultiPromptPipelineApproach1, MultiPromptPipelineApproach2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoryboardGenerator:\n",
    "    # Class-level lists for available camera shots and orientations\n",
    "    ORIENTATIONS = [\n",
    "        \"Front View\", \"Profile View\", \"Back View\", \"From Behind\", \"From Above\",\n",
    "        \"From Below\", \"Three-Quarters View\", \"Long Shot\", \"Three-Quarters Rear View\"\n",
    "    ]\n",
    "    CAMERA_SHOTS = [\n",
    "        \"Aerial View\", \"Birdâ€™s-Eye View\", \"Close-Up\", \"Cowboy Shot\", \"Dolly Zoom\",\n",
    "        \"Dutch Angle\", \"Establishing Shot\", \"Extreme Close-Up\", \"Extreme Long Shot\",\n",
    "        \"Full Shot\", \"Long Shot\", \"Medium Close-Up\", \"Medium Long Shot\", \"Medium Shot\",\n",
    "        \"Over-the-Shoulder Shot\", \"Point-of-View Shot\", \"Two-Shot\", \"Fisheye Shot\",\n",
    "        \"Worm's Eye\", \"Low-Angle Shot\", \"Macro Shot\", \"Tilt-Shift Shot\", \"Telephoto Shot\"\n",
    "    ]\n",
    "    \n",
    "    def __init__(self, script: str, characters: dict,\n",
    "                 generation_type: str = \"unique\",\n",
    "                 prompt_weights: list = [2, 1.0, 1.2, 1.5, 0.9],\n",
    "                 style: str = \"storyboard\",\n",
    "                 temperature: float = 0.7,\n",
    "                 device: str = \"cpu\"):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "          - script: The film script text.\n",
    "          - characters: A dictionary with character names as keys and a dict of attributes as values.\n",
    "          - generation_type: One of \"unique\", \"prompt_weights\", \"cf1\", or \"cf2\".\n",
    "          - prompt_weights: List of weights [style, environment, shot, description, character].\n",
    "          - style: A string to specify the artistic style (if \"storyboard\", a default is used).\n",
    "          - temperature: Temperature parameter for the together API.\n",
    "          - pipe_unique: (Optional) A StableDiffusionPipeline for generation with unique and prompt weights methods.\n",
    "          - pipe_cf1: (Optional) A pipeline instance for MultiPrompt Approach 1.\n",
    "          - pipe_cf2: (Optional) A pipeline instance for MultiPrompt Approach 2.\n",
    "        \"\"\"\n",
    "        \n",
    "        load_dotenv()  # load .env (for TOGETHER_API_KEY, etc.)\n",
    "        self.script = script\n",
    "        self.characters = characters\n",
    "        self.generation_type = generation_type\n",
    "        self.prompt_weights = prompt_weights\n",
    "        self.style = style\n",
    "        self.temperature = temperature\n",
    "        self.device = device\n",
    "        \n",
    "        # Initialize the Together API client (used in scene generation)\n",
    "        self.together = Together()\n",
    "        \n",
    "        # initialize the pipelines if not provided\n",
    "        if generation_type == \"unique\" or generation_type == \"prompt_weights\":\n",
    "            self.pipe = StableDiffusionPipeline.from_pretrained(\n",
    "                \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16\n",
    "            )\n",
    "            self.pipe = self.pipe.to(self.device)\n",
    "            self.pipe.scheduler = UniPCMultistepScheduler.from_config(self.pipe.scheduler.config)\n",
    "            self.pipe.enable_model_cpu_offload()\n",
    "            self.pipe.enable_attention_slicing()\n",
    "        elif generation_type == \"cf1\":\n",
    "            self.pipe = MultiPromptPipelineApproach1.from_pretrained(\n",
    "                \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16\n",
    "            )\n",
    "            self.pipe = self.pipe.to(self.device)\n",
    "            self.pipe.scheduler = UniPCMultistepScheduler.from_config(self.pipe.scheduler.config)\n",
    "            self.pipe.enable_model_cpu_offload()\n",
    "            self.pipe.enable_attention_slicing()\n",
    "        elif generation_type == \"cf2\":\n",
    "            self.pipe = MultiPromptPipelineApproach2.from_pretrained(\n",
    "                \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16\n",
    "            )\n",
    "            self.pipe = self.pipe.to(self.device)\n",
    "            self.pipe.scheduler = UniPCMultistepScheduler.from_config(self.pipe.scheduler.config)\n",
    "            self.pipe.enable_model_cpu_offload()\n",
    "            self.pipe.enable_attention_slicing()\n",
    "\n",
    "        # These will be filled later\n",
    "        self.scenes = None\n",
    "        self.formatted_prompts = None\n",
    "        \n",
    "    @staticmethod\n",
    "    def _build_character_description(characters_dict: dict) -> str:\n",
    "        \"\"\"\n",
    "        Given a character attribute dict, return a concise description.\n",
    "        \"\"\"\n",
    "        features = [\n",
    "            characters_dict.get(\"ethnicity\", \"\"),\n",
    "            characters_dict.get(\"age\", \"\"),\n",
    "            characters_dict.get(\"gender\", \"\"),\n",
    "            characters_dict.get(\"hair\", \"\"),\n",
    "            characters_dict.get(\"facial_hair\", \"\"),\n",
    "            characters_dict.get(\"body_type\", \"\"),\n",
    "            f\"wearing {characters_dict.get('clothing', '')}\",\n",
    "            f\"with {characters_dict.get('accessories', '')}\" if characters_dict.get(\"accessories\") else \"\"\n",
    "        ]\n",
    "        return \", \".join(filter(None, features))\n",
    "    \n",
    "    def generate_scenes(self):\n",
    "        \"\"\"\n",
    "        Use the Together API (with a specialized model) to generate JSON-formatted storyboard scenes.\n",
    "        The returned JSON should match a schema (e.g. SceneList) that you have defined.\n",
    "        \"\"\"\n",
    "        # Build character descriptions\n",
    "        character_descriptions = {\n",
    "            name: self._build_character_description(desc)\n",
    "            for name, desc in self.characters.items()\n",
    "        }\n",
    "        script_section = f\"Here is the film script: \\n{self.script}\"\n",
    "        characters_section = f\"The characters in the script have the following descriptions: \\n{json.dumps(character_descriptions, indent=2)}\"\n",
    "        \n",
    "        # Instructions with embedded camera shot and orientation lists\n",
    "        instructions = f\"\"\"\n",
    "### Storyboard Generation Instructions\n",
    "1. **Number of Scenes**: Divide the entire script into a reasonable number of scenes (typically between 4 to 7 scenes), not too many or too few.\n",
    "2. **Single Distinct Moment**: Each scene captures a single moment.\n",
    "3. **Camera Angles & Orientation**: Choose from these shot types: {', '.join(self.CAMERA_SHOTS)}.  \n",
    "Choose from these orientations: {', '.join(self.ORIENTATIONS)}.\n",
    "4. **Location & Time**: Clearly derive environment from the script (e.g. INT DAY, DON'S OFFICE, etc.). Describe it in its details (size, lighhting, mood, organization of the objects, etc.). Notice that if it's the same across the different scenes, it must be written in the same way\n",
    "5. **Characters**:\n",
    "- List only characters relevant to the single moment in each scene.\n",
    "- Each character must have the name and a short description (consistent from provided descriptions).\n",
    "6. Clearly describe the scene including actions, character positions (foreground, background, left, right), emotions, and expressions.\n",
    "7. **Scene Format**: Return JSON with a key 'scenes' as an array of structured objects:\n",
    "- \"scene_number\": integer\n",
    "- \"shot_type\": camera shot type (from provided list) \n",
    "- \"orientation\": orientation (from provided list)\n",
    "- \"characters\": list of objects with:\n",
    "        - \"name\": character's name, not as they appear on the script but as they were given to you in the description.\n",
    "- \"environment\": short description of the location\n",
    "- \"description\": short, vivid description focusing on actions, expressions, emotions of each single character. Also their relative position is clearly described. The description must be succint, without extra articles or words, it should be visual and useful for an image generation prompt. Ensure it makes sense with the shot type (e.g., if it's medium shot, don't say that the face is covering the full image, otherwise it should be a close up). Don't write the words they say, since they occupy tokens, unless it's a fundamental part of the script. Avoid useless adjetives or adverbs, be concise and clear.\n",
    "\n",
    "Follow the above instructions very carefully. Notice that the scenes have no knowledge of each other's contents. So in case something is necessary, describe it again. \n",
    "\"\"\"\n",
    "\n",
    "        example_input = \"\"\"\n",
    "### Example\n",
    "Input: \n",
    "- Script is \n",
    "INT DAY: DON'S OFFICE (SUMMER 1945)\n",
    "\n",
    "        DON CORLEONE\n",
    "ACT LIKE A MAN!  By Christ in\n",
    "Heaven, is it possible you turned\n",
    "out no better than a Hollywood\n",
    "finocchio.\n",
    "\n",
    "Both HAGEN and JOHNNY cannot refrain from laughing.  The DON\n",
    "smiles.  SONNY enters as noiselessly as possible, still\n",
    "adjusting his clothes.\n",
    "\n",
    "        DON CORLEONE\n",
    "All right, Hollywood...Now tell me\n",
    "about this Hollywood Pezzonovanta\n",
    "who won't let you work.\n",
    "\n",
    "        JOHNNY\n",
    "He owns the studio.  Just a month\n",
    "ago he bought the movie rights to\n",
    "this book, a best seller.  And the\n",
    "main character is a guy just like\n",
    "me.  I wouldn't even have to act,\n",
    "just be myself.\n",
    "\n",
    "The DON is silent, stern.\n",
    "\n",
    "        DON CORLEONE\n",
    "You take care of your family?\n",
    "\n",
    "        JOHNNY\n",
    "Sure.\n",
    "\n",
    "He glances at SONNY, who makes himself as inconspicuous as\n",
    "he can.\n",
    "\n",
    "        DON CORLEONE\n",
    "You look terrible.  I want you to\n",
    "eat well, to rest.  And spend time\n",
    "with your family.  And then, at the\n",
    "end of the month, this big shot\n",
    "will give you the part you want.\n",
    "\n",
    "        JOHNNY\n",
    "It's too late.  All the contracts\n",
    "have been signed, they're almost\n",
    "ready to shoot.\n",
    "\n",
    "        DON CORLEONE\n",
    "I'll make him an offer he can't\n",
    "refuse.\n",
    "\n",
    "He takes JOHNNY to the door, pinching his cheek hard enough\n",
    "to hurt.\n",
    "\n",
    "        DON CORLEONE\n",
    "Now go back to the party and leave\n",
    "it to me.\n",
    "\n",
    "He closes the door, smiling to himself.  Turns to HAGEN.\n",
    "\n",
    "        DON CORLEONE\n",
    "When does my daughter leave with\n",
    "her bridegroom?\n",
    "\n",
    "        HAGEN\n",
    "They'll cut the cake in a few\n",
    "minutes...leave right after that.\n",
    "Your new son-in-law, do we give him\n",
    "something important?\n",
    "\n",
    "        DON CORLEONE\n",
    "No, give him a living.  But never\n",
    "let him know the family's business.\n",
    "What else, Tom?\n",
    "\n",
    "        HAGEN\n",
    "I've called the hospital; they've\n",
    "notified Consigliere Genco's family\n",
    "to come and wait.  He won't last\n",
    "out the night.\n",
    "\n",
    "This saddens the DON.  He sighs.\n",
    "\n",
    "        DON CORLEONE\n",
    "Genco will wait for me.  Santino,\n",
    "tell your brothers they will come\n",
    "with me to the hospital to see\n",
    "Genco.  Tell Fredo to drive the big\n",
    "car, and ask Johnny to come with us.\n",
    "\n",
    "        SONNY\n",
    "And Michael?\n",
    "\n",
    "        DON CORLEONE\n",
    "All my sons.\n",
    "        (to HAGEN)\n",
    "Tom, I want you to go to California\n",
    "tonight.  Make the arrangements.\n",
    "But don't leave until I come back\n",
    "from the hospital and speak to you.\n",
    "Understood?\n",
    "\n",
    "        HAGEN\n",
    "Understood.\n",
    "\n",
    "- Characters description from the dictionary gives\n",
    "        - Don Vito Corleone: 'Italian-American, early 60s, male, slicked-back gray-black hair, stocky, slightly hunched posture, wearing dark three-piece suit, with gold ring on right hand, pocket watch'\n",
    "        - Johnny Fontane: 'late 30s, male, short, slicked-back black hair, clean shaven, slim and fit, wearing dark, stylish suit with an open collar, with gold ring, cigarette'\n",
    "        - Tom Hagen: 'German-Irish, early 40s, male, short, neatly combed brown hair, clean-shaven, medium build, upright posture, wearing gray suit, dark tie'\n",
    "        - Sonny: 'Italian-American, early 30s, male, curly, dark brown hair, clean-shaven, athletic build, wearing formal suit, slightly disheveled'\n",
    "\"\"\"\n",
    "\n",
    "        example_output = \"\"\"\n",
    "Example Output:\n",
    "{\n",
    "   \"scenes\": [\n",
    "   {\n",
    "   \"scene_number\": 1,\n",
    "   \"shot_type\": \"Medium Shot\",\n",
    "   \"orientation\": \"Front View\",\n",
    "   \"characters\": [\n",
    "           {\n",
    "           \"name\": \"Don Vito Corleone\"\n",
    "           },\n",
    "           {\n",
    "           \"name\": \"Johnny Fontane\"\n",
    "           },\n",
    "           {\n",
    "           \"name\": \"Tom Hagen\"\n",
    "           }\n",
    "   ],\n",
    "   \"environment\": \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
    "   \"description\": \"Don Corleone stands imposingly behind desk, face stern with righteous anger, pointing finger at Johnny. Johnny appears embarrassed, head slightly bowed. Hagen stands to the right, barely containing laughter. Tension and amusement mix in intimate office atmosphere.\"\n",
    "   },\n",
    "   {\n",
    "   \"scene_number\": 2,\n",
    "   \"shot_type\": \"Two-Shot\",\n",
    "   \"orientation\": \"Profile View\",\n",
    "   \"characters\": [\n",
    "           {\n",
    "           \"name\": \"Don Vito Corleone\"\n",
    "           },\n",
    "           {\n",
    "           \"name\": \"Johnny Fontane\"\n",
    "           },\n",
    "           {\n",
    "           \"name\": \"Tom Hagen\"\n",
    "           },\n",
    "           {\n",
    "           \"name\": \"Sonny\"\n",
    "           }\n",
    "   ],\n",
    "   \"environment\": \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
    "   \"description\": \"Sonny quietly enters room from right, adjusting disheveled clothes. Don leans forward at desk, expression softening to business-like focus. Johnny stands center, straightening posture. Hagen observes from left corner. Atmosphere shifts from personal rebuke to business discussion.\"\n",
    "   },\n",
    "   {\n",
    "   \"scene_number\": 3,\n",
    "   \"shot_type\": \"Close-Up\",\n",
    "   \"orientation\": \"Front View\",\n",
    "   \"characters\": [\n",
    "           {\n",
    "           \"name\": \"Don Vito Corleone\"\n",
    "           }\n",
    "   ],\n",
    "   \"environment\": \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
    "   \"description\": \"Don Corleone's face fills frame, stern and contemplative. Eyes narrowed, jaw set firmly. Saying 'I'll make him an offer he can't refuse' with quiet, confident menace. Power and authority emanate from his expression.\"\n",
    "   },\n",
    "   {\n",
    "   \"scene_number\": 4,\n",
    "   \"shot_type\": \"Medium Close-Up\",\n",
    "   \"orientation\": \"Three-Quarters View\",\n",
    "   \"characters\": [\n",
    "           {\n",
    "           \"name\": \"Don Vito Corleone\"\n",
    "           },\n",
    "           {\n",
    "           \"name\": \"Johnny Fontane\"\n",
    "           }\n",
    "   ],\n",
    "   \"environment\": \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
    "   \"description\": \"Don Corleone escorts Johnny to door, pinching his cheek firmly. Don's expression shows affection mixed with dominance. Johnny winces slightly at pain while showing relief and gratitude. Door frame visible on right edge of shot.\"\n",
    "   },\n",
    "   {\n",
    "   \"scene_number\": 5,\n",
    "   \"shot_type\": \"Medium Shot\",\n",
    "   \"orientation\": \"Front View\",\n",
    "   \"characters\": [\n",
    "           {\n",
    "           \"name\": \"Don Vito Corleone\"\n",
    "           },\n",
    "           {\n",
    "           \"name\": \"Tom Hagen\"\n",
    "           }\n",
    "   ],\n",
    "   \"environment\": \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
    "   \"description\": \"Don Corleone turns from closed door, small smile fading to serious business expression. Hagen stands attentively near desk, notepad ready. Don moves toward chair, shoulders slightly hunched, gold ring catching light as he gestures.\"\n",
    "   },\n",
    "   {\n",
    "   \"scene_number\": 6,\n",
    "   \"shot_type\": \"Over-the-Shoulder Shot\",\n",
    "   \"orientation\": \"Profile View\",\n",
    "   \"characters\": [\n",
    "           {\n",
    "           \"name\": \"Don Vito Corleone\"\n",
    "           },\n",
    "           {\n",
    "           \"name\": \"Tom Hagen\"\n",
    "           },\n",
    "           {\n",
    "           \"name\": \"Sonny\"\n",
    "           }\n",
    "   ],\n",
    "   \"environment\": \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
    "   \"description\": \"Camera over Don's shoulder, facing Hagen and Sonny. Don's gray-black hair and dark suit visible in foreground. Hagen's face shows respectful attention. Sonny stands beside him, now composed. Don's voice carries weight as he issues final instructions about hospital visit.\"\n",
    "   }\n",
    "   ]\n",
    "}\n",
    "\"\"\"\n",
    "        # (Optionally, you could also include an example input and output here.)\n",
    "        user_content = f\"{script_section}\\n\\n{characters_section}\\n\\n{instructions}\\n{example_input}\\n{example_output}\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": (\n",
    "                \"You are an AI specialized in creating structured storyboard scenes from a film script \"\n",
    "                \"for image generation (e.g., stable diffusion). Each scene must capture a single distinct moment, \"\n",
    "                \"should list relevant characters with consistent appearances, specify the environment, camera shot, \"\n",
    "                \"and orientation, and provide direct clues for a diffusion model to generate images.\"\n",
    "            )},\n",
    "            {\"role\": \"user\", \"content\": user_content}\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            response = self.together.chat.completions.create(\n",
    "                model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "                messages=messages,\n",
    "                max_tokens=10000,\n",
    "                temperature=self.temperature,\n",
    "                response_format={\"type\": \"json_object\", \"schema\": SceneList.model_json_schema()}\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\"API request failed:\", e)\n",
    "            raise RuntimeError(\"API request failed.\") from e\n",
    "        \n",
    "        try:\n",
    "            output_json = response.choices[0].message.content\n",
    "            self.scenes = json.loads(output_json)[\"scenes\"]\n",
    "        except (json.JSONDecodeError, KeyError) as e:\n",
    "            print(\"Error parsing JSON output:\", e)\n",
    "            self.scenes = []\n",
    "        return self.scenes\n",
    "    \n",
    "    def scenes_to_formatted_prompts(self):\n",
    "        \"\"\"\n",
    "        Convert the generated scenes into formatted prompts.\n",
    "        Returns a list of tuples (subprompt_texts, subprompt_weights) for each scene.\n",
    "        \"\"\"\n",
    "        formatted_results = []\n",
    "        weight_map = {\n",
    "            \"style\": self.prompt_weights[0],\n",
    "            \"environment\": self.prompt_weights[1],\n",
    "            \"shot\": self.prompt_weights[2],\n",
    "            \"description\": self.prompt_weights[3]\n",
    "        }\n",
    "        character_weight = self.prompt_weights[4]\n",
    "        # Determine style string based on input style\n",
    "        if self.style == \"storyboard\":\n",
    "            style_value = \"rough b&w pencil sketch, simple sketch lines, minimal shading, rough hatching, draft-style, J.C. Leyendecker style\"\n",
    "        else:\n",
    "            style_value = self.style\n",
    "\n",
    "        for scene in self.scenes:\n",
    "            subprompts = {}\n",
    "            for i, char in enumerate(scene[\"characters\"]):\n",
    "                char_name = char[\"name\"]\n",
    "                char_info = self.characters.get(char_name)\n",
    "                if not char_info:\n",
    "                    # Fallback: try a substring match or a default description\n",
    "                    matching_keys = [key for key in self.characters if char_name in key]\n",
    "                    if matching_keys:\n",
    "                        char_info = self.characters[matching_keys[0]]\n",
    "                    else:\n",
    "                        char_info = {\"age\": \"unknown\", \"gender\": \"unknown\", \"hair\": \"unknown\",\n",
    "                                     \"clothing\": \"unknown\", \"body_type\": \"unknown\"}\n",
    "                char_desc = self._build_character_description(char_info)\n",
    "                subprompts[f\"character{i+1}\"] = f\"{char_name}: {char_desc}\"\n",
    "            subprompts[\"style\"] = style_value\n",
    "            subprompts[\"environment\"] = scene[\"environment\"]\n",
    "            subprompts[\"shot\"] = f\"{scene['shot_type']}, {scene['orientation']}\"\n",
    "            subprompts[\"description\"] = scene[\"description\"]\n",
    "\n",
    "            subprompt_texts = []\n",
    "            subprompt_weights = []\n",
    "            for key, text in subprompts.items():\n",
    "                subprompt_texts.append(text)\n",
    "                if key.startswith(\"character\"):\n",
    "                    subprompt_weights.append(character_weight)\n",
    "                else:\n",
    "                    subprompt_weights.append(weight_map.get(key, 1.0))\n",
    "            formatted_results.append((subprompt_texts, subprompt_weights))\n",
    "        self.formatted_prompts = formatted_results\n",
    "        return self.formatted_prompts\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_unique_prompts(formatted_prompts, style_override=\"rough b&w simple pencil sketch, J.C. Leyendecker style,\"):\n",
    "        \"\"\"\n",
    "        Given formatted prompts, build unique prompt strings for each scene.\n",
    "        \"\"\"\n",
    "        unique_prompts = []\n",
    "        for subprompt_texts, _ in formatted_prompts:\n",
    "            shot_prompt = subprompt_texts[-2]  # second to last is the shot prompt\n",
    "            description = subprompt_texts[-1]\n",
    "            unique_prompt = f\"{style_override} {shot_prompt}: {description}\"\n",
    "            unique_prompts.append(unique_prompt)\n",
    "        return unique_prompts\n",
    "    \n",
    "    @staticmethod\n",
    "    def weighted_sum_prompt_embeddings(pipe, subprompt_texts, subprompt_weights, device, num_images_per_prompt=1):\n",
    "        \"\"\"\n",
    "        Computes a weighted sum of text embeddings for a list of subprompts.\n",
    "        \"\"\"\n",
    "        encoded_prompts = []\n",
    "        for text in subprompt_texts:\n",
    "            text_inputs = pipe.tokenizer(\n",
    "                text,\n",
    "                padding=\"max_length\",\n",
    "                max_length=pipe.tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            input_ids = text_inputs.input_ids.to(device)\n",
    "            attention_mask = text_inputs.attention_mask.to(device) if \"attention_mask\" in text_inputs else None\n",
    "            text_embeds = pipe.text_encoder(input_ids, attention_mask=attention_mask)[0]\n",
    "            encoded_prompts.append(text_embeds)\n",
    "        weighted_embedding = sum(weight * embeds for weight, embeds in zip(subprompt_weights, encoded_prompts))\n",
    "        weight_total = sum(subprompt_weights)\n",
    "        combined_embedding = weighted_embedding / weight_total\n",
    "        batch_size, seq_len, embed_dim = combined_embedding.shape\n",
    "        combined_embedding = combined_embedding.repeat(1, num_images_per_prompt, 1)\n",
    "        combined_embedding = combined_embedding.view(batch_size * num_images_per_prompt, seq_len, embed_dim)\n",
    "        return combined_embedding\n",
    "    \n",
    "    @staticmethod\n",
    "    def encode_subprompt(pipe, text, device=\"cuda\"):\n",
    "        \"\"\"\n",
    "        Tokenize and encode a single subprompt into text embeddings.\n",
    "        \"\"\"\n",
    "        text_inputs = pipe.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            max_length=pipe.tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        text_embeds = pipe.text_encoder(\n",
    "            text_inputs.input_ids.to(device),\n",
    "            attention_mask=text_inputs.attention_mask.to(device)\n",
    "        )[0]\n",
    "        return text_embeds\n",
    "    \n",
    "    def generate_and_save_images_unique(self, save_dir, num_inference_steps=50,\n",
    "                                          negative_prompt=\"low quality, photorealistic, 3d render, overly detailed, digital art, painting, vibrant colors, fine art, NSFW\"):\n",
    "        \"\"\"\n",
    "        Generate images using unique prompts (concatenated style, shot, and description).\n",
    "        \"\"\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        # Ensure formatted prompts exist\n",
    "        if self.formatted_prompts is None:\n",
    "            self.scenes_to_formatted_prompts()\n",
    "        unique_prompts = self.build_unique_prompts(self.formatted_prompts)\n",
    "        generated_images = []\n",
    "        for i, unique_prompt in enumerate(unique_prompts):\n",
    "            with torch.no_grad():\n",
    "                output = self.pipe(\n",
    "                    prompt=unique_prompt,\n",
    "                    negative_prompt=negative_prompt,\n",
    "                    num_inference_steps=num_inference_steps\n",
    "                )\n",
    "            generated_image = output.images[0]\n",
    "            generated_images.append(generated_image)\n",
    "            image_path = os.path.join(save_dir, f\"image_{i+1}.png\")\n",
    "            generated_image.save(image_path)\n",
    "            print(f\"Image {i+1} saved to {image_path}\")\n",
    "        return generated_images\n",
    "    \n",
    "    def generate_and_save_images_prompt_weights(self, save_dir, num_inference_steps=50,\n",
    "                                                  negative_prompt=\"low quality, photorealistic, 3d render, overly detailed, digital art, painting, vibrant colors, fine art, NSFW\"):\n",
    "        \"\"\"\n",
    "        Generate images using a weighted sum of prompt embeddings.\n",
    "        \"\"\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        generated_images = []\n",
    "        # Update formatted prompts\n",
    "        formatted = self.scenes_to_formatted_prompts()\n",
    "        for i, (subprompt_texts, subprompt_weights) in enumerate(formatted):\n",
    "            combined_embeddings = self.weighted_sum_prompt_embeddings(\n",
    "                self.pipe, subprompt_texts, subprompt_weights, self.device\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                output = self.pipe(\n",
    "                    prompt_embeds=combined_embeddings,\n",
    "                    negative_prompt=negative_prompt,\n",
    "                    num_inference_steps=num_inference_steps\n",
    "                )\n",
    "            generated_image = output.images[0]\n",
    "            generated_images.append(generated_image)\n",
    "            image_path = os.path.join(save_dir, f\"image_{i+1}.png\")\n",
    "            generated_image.save(image_path)\n",
    "            print(f\"Image {i+1} saved to {image_path}\")\n",
    "        return generated_images\n",
    "    \n",
    "    def generate_and_save_images_cf1(self, save_dir, num_inference_steps=50, guidance_scale=7.5):\n",
    "        \"\"\"\n",
    "        Generate images using Multi-Prompt Approach 1 (single unconditional pass).\n",
    "        \"\"\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        generated_images = []\n",
    "        uncond_text = \"low quality, photorealistic, 3d render, overly detailed, digital art, painting, vibrant colors, fine art, NSFW\"\n",
    "        uncond_embeds = self.encode_subprompt(self.pipe, uncond_text, device=self.device)\n",
    "        formatted = self.scenes_to_formatted_prompts()\n",
    "        for i, (subprompt_texts, subprompt_weights) in enumerate(formatted):\n",
    "            subprompt_embeds = [\n",
    "                self.encode_subprompt(self.pipe, sp, device=self.device)\n",
    "                for sp in subprompt_texts\n",
    "            ]\n",
    "            print(f\"Generating image for scene {i+1} using cf1 approach...\")\n",
    "            with torch.no_grad():\n",
    "                output = self.pipe(\n",
    "                    subprompt_embeds=subprompt_embeds,\n",
    "                    subprompt_weights=subprompt_weights,\n",
    "                    uncond_embeds=uncond_embeds,\n",
    "                    guidance_scale=guidance_scale,\n",
    "                    num_inference_steps=num_inference_steps\n",
    "                )\n",
    "            generated_image = output.images[0]\n",
    "            generated_images.append(generated_image)\n",
    "            image_path = os.path.join(save_dir, f\"scene_{i+1}.png\")\n",
    "            generated_image.save(image_path)\n",
    "            print(f\"Image {i+1} saved to {image_path}\")\n",
    "        return generated_images\n",
    "    \n",
    "    def generate_and_save_images_cf2(self, save_dir, num_inference_steps=50, guidance_scale=7.5):\n",
    "        \"\"\"\n",
    "        Generate images using Multi-Prompt Approach 2 (multiple unconditional passes).\n",
    "        \"\"\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        generated_images = []\n",
    "        formatted = self.scenes_to_formatted_prompts()\n",
    "        for i, (subprompt_texts, subprompt_weights) in enumerate(formatted):\n",
    "            global_uncond_embeds = self.encode_subprompt(self.pipe, \"\", device=self.device)\n",
    "            subprompt_pairs = []\n",
    "            for sp in subprompt_texts:\n",
    "                uncond_i = self.encode_subprompt(self.pipe, \"\", device=self.device)\n",
    "                cond_i = self.encode_subprompt(self.pipe, sp, device=self.device)\n",
    "                subprompt_pairs.append((uncond_i, cond_i))\n",
    "            print(f\"Generating image for scene {i+1} using cf2 approach...\")\n",
    "            with torch.no_grad():\n",
    "                output = self.pipe(\n",
    "                    global_uncond_embeds=global_uncond_embeds,\n",
    "                    subprompt_pairs=subprompt_pairs,\n",
    "                    subprompt_weights=subprompt_weights,\n",
    "                    guidance_scale=guidance_scale,\n",
    "                    num_inference_steps=num_inference_steps\n",
    "                )\n",
    "            generated_image = output.images[0]\n",
    "            generated_images.append(generated_image)\n",
    "            image_path = os.path.join(save_dir, f\"scene_{i+1}_approach2.png\")\n",
    "            generated_image.save(image_path)\n",
    "            print(f\"Image {i+1} saved to {image_path}\")\n",
    "        return generated_images\n",
    "    \n",
    "    def generate_images(self, save_dir, num_inference_steps=50, guidance_scale=7.5,\n",
    "                        negative_prompt=\"low quality, photorealistic, 3d render, overly detailed, digital art, painting, vibrant colors, fine art, NSFW\"):\n",
    "        \"\"\"\n",
    "        Main method to generate images. It selects the generation method based on self.generation_type.\n",
    "        \"\"\"\n",
    "        if self.generation_type == \"unique\":\n",
    "            # Ensure prompts are formatted\n",
    "            if self.formatted_prompts is None:\n",
    "                self.scenes_to_formatted_prompts()\n",
    "            return self.generate_and_save_images_unique(save_dir, num_inference_steps, negative_prompt)\n",
    "        elif self.generation_type == \"prompt_weights\":\n",
    "            if self.formatted_prompts is None:\n",
    "                self.scenes_to_formatted_prompts()\n",
    "            return self.generate_and_save_images_prompt_weights(save_dir, num_inference_steps, negative_prompt)\n",
    "        elif self.generation_type == \"cf1\":\n",
    "            if self.formatted_prompts is None:\n",
    "                self.scenes_to_formatted_prompts()\n",
    "            return self.generate_and_save_images_cf1(save_dir, num_inference_steps, guidance_scale)\n",
    "        elif self.generation_type == \"cf2\":\n",
    "            if self.formatted_prompts is None:\n",
    "                self.scenes_to_formatted_prompts()\n",
    "            return self.generate_and_save_images_cf2(save_dir, num_inference_steps, guidance_scale)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid generation type. Choose one of: unique, prompt_weights, cf1, cf2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6b8f9443ad44b57af963371deafc8f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = StoryboardGenerator(script, characters_dict, generation_type=\"unique\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes = generator.generate_scenes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'scene_number': 1,\n",
       "  'shot_type': 'Medium Shot',\n",
       "  'orientation': 'Front View',\n",
       "  'characters': [{'name': 'Don Vito Corleone'},\n",
       "   {'name': 'Johnny Fontane'},\n",
       "   {'name': 'Tom Hagen'}],\n",
       "  'environment': \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
       "  'description': 'Don Corleone stands imposingly behind desk, face stern with righteous anger, pointing finger at Johnny. Johnny appears embarrassed, head slightly bowed. Hagen stands to the right, barely containing laughter. Tension and amusement mix in intimate office atmosphere.'},\n",
       " {'scene_number': 2,\n",
       "  'shot_type': 'Two-Shot',\n",
       "  'orientation': 'Profile View',\n",
       "  'characters': [{'name': 'Don Vito Corleone'},\n",
       "   {'name': 'Johnny Fontane'},\n",
       "   {'name': 'Tom Hagen'},\n",
       "   {'name': 'Sonny'}],\n",
       "  'environment': \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
       "  'description': 'Sonny quietly enters room from right, adjusting disheveled clothes. Don leans forward at desk, expression softening to business-like focus. Johnny stands center, straightening posture. Hagen observes from left corner. Atmosphere shifts from personal rebuke to business discussion.'},\n",
       " {'scene_number': 3,\n",
       "  'shot_type': 'Close-Up',\n",
       "  'orientation': 'Front View',\n",
       "  'characters': [{'name': 'Don Vito Corleone'}],\n",
       "  'environment': \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
       "  'description': \"Don Corleone's face fills frame, stern and contemplative. Eyes narrowed, jaw set firmly. Saying 'I'll make him an offer he can't refuse' with quiet, confident menace. Power and authority emanate from his expression.\"},\n",
       " {'scene_number': 4,\n",
       "  'shot_type': 'Medium Close-Up',\n",
       "  'orientation': 'Three-Quarters View',\n",
       "  'characters': [{'name': 'Don Vito Corleone'}, {'name': 'Johnny Fontane'}],\n",
       "  'environment': \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
       "  'description': \"Don Corleone escorts Johnny to door, pinching his cheek firmly. Don's expression shows affection mixed with dominance. Johnny winces slightly at pain while showing relief and gratitude. Door frame visible on right edge of shot.\"},\n",
       " {'scene_number': 5,\n",
       "  'shot_type': 'Medium Shot',\n",
       "  'orientation': 'Front View',\n",
       "  'characters': [{'name': 'Don Vito Corleone'}, {'name': 'Tom Hagen'}],\n",
       "  'environment': \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
       "  'description': 'Don Corleone turns from closed door, small smile fading to serious business expression. Hagen stands attentively near desk, notepad ready. Don moves toward chair, shoulders slightly hunched, gold ring catching light as he gestures.'},\n",
       " {'scene_number': 6,\n",
       "  'shot_type': 'Over-the-Shoulder Shot',\n",
       "  'orientation': 'Profile View',\n",
       "  'characters': [{'name': 'Don Vito Corleone'},\n",
       "   {'name': 'Tom Hagen'},\n",
       "   {'name': 'Sonny'}],\n",
       "  'environment': \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
       "  'description': \"Camera over Don's shoulder, facing Hagen and Sonny. Don's gray-black hair and dark suit visible in foreground. Hagen's face shows respectful attention. Sonny stands beside him, now composed. Don's voice carries weight as he issues final instructions about hospital visit.\"},\n",
       " {'scene_number': 7,\n",
       "  'shot_type': 'Extreme Close-Up',\n",
       "  'orientation': 'Front View',\n",
       "  'characters': [{'name': 'Don Vito Corleone'}],\n",
       "  'environment': \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
       "  'description': \"Close-up of Don's face, eyes saddened by news of Genco's impending death. Jaw clenched, nostrils flared. Heavy with sorrow and concern for his loyal Consigliere.\"},\n",
       " {'scene_number': 8,\n",
       "  'shot_type': 'Medium Long Shot',\n",
       "  'orientation': 'Front View',\n",
       "  'characters': [{'name': 'Don Vito Corleone'},\n",
       "   {'name': 'Tom Hagen'},\n",
       "   {'name': 'Sonny'}],\n",
       "  'environment': \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
       "  'description': \"Don stands, serious and resolute, pointing at Hagen. Sonny stands beside him, watching intently. Hagen listens, notepad forgotten. Don's voice carries weight as he assigns tasks and duties. Authority and leadership radiate from him.\"},\n",
       " {'scene_number': 9,\n",
       "  'shot_type': 'Medium Close-Up',\n",
       "  'orientation': 'Three-Quarters View',\n",
       "  'characters': [{'name': 'Don Vito Corleone'}],\n",
       "  'environment': \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
       "  'description': \"Don's face fills frame, eyes locked on Hagen. Voice low and firm, words dripping with authority. 'Tom, I want you to go to California tonight. Make the arrangements. But don't leave until I come back from the hospital and speak to you.'\"}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_prompts = generator.scenes_to_formatted_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['Don Vito Corleone: Italian-American, early 60s, male, slicked-back gray-black hair, stocky, slightly hunched posture, wearing dark three-piece suit, with gold ring on right hand, pocket watch',\n",
       "   'Johnny Fontane: late 30s, male, short, slicked-back black hair, clean shaven, slim and fit, wearing dark, stylish suit with an open collar, with gold ring, cigarette',\n",
       "   'Tom Hagen: German-Irish, early 40s, male, short, neatly combed brown hair, clean-shaven, medium build, upright posture, wearing gray suit, dark tie',\n",
       "   'rough b&w pencil sketch, simple sketch lines, minimal shading, rough hatching, draft-style, J.C. Leyendecker style',\n",
       "   \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
       "   'Medium Shot, Front View',\n",
       "   'Don Corleone stands imposingly behind desk, face stern with righteous anger, pointing finger at Johnny. Johnny appears embarrassed, head slightly bowed. Hagen stands to the right, barely containing laughter. Tension and amusement mix in intimate office atmosphere.'],\n",
       "  [0.9, 0.9, 0.9, 2, 1.0, 1.2, 1.5]),\n",
       " (['Don Vito Corleone: Italian-American, early 60s, male, slicked-back gray-black hair, stocky, slightly hunched posture, wearing dark three-piece suit, with gold ring on right hand, pocket watch',\n",
       "   'Johnny Fontane: late 30s, male, short, slicked-back black hair, clean shaven, slim and fit, wearing dark, stylish suit with an open collar, with gold ring, cigarette',\n",
       "   'Tom Hagen: German-Irish, early 40s, male, short, neatly combed brown hair, clean-shaven, medium build, upright posture, wearing gray suit, dark tie',\n",
       "   'Sonny: Italian-American, early 30s, male, curly, dark brown hair, clean-shaven, athletic build, wearing formal suit, slightly disheveled',\n",
       "   'rough b&w pencil sketch, simple sketch lines, minimal shading, rough hatching, draft-style, J.C. Leyendecker style',\n",
       "   \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
       "   'Two-Shot, Profile View',\n",
       "   'Sonny quietly enters room from right, adjusting disheveled clothes. Don leans forward at desk, expression softening to business-like focus. Johnny stands center, straightening posture. Hagen observes from left corner. Atmosphere shifts from personal rebuke to business discussion.'],\n",
       "  [0.9, 0.9, 0.9, 0.9, 2, 1.0, 1.2, 1.5]),\n",
       " (['Don Vito Corleone: Italian-American, early 60s, male, slicked-back gray-black hair, stocky, slightly hunched posture, wearing dark three-piece suit, with gold ring on right hand, pocket watch',\n",
       "   'rough b&w pencil sketch, simple sketch lines, minimal shading, rough hatching, draft-style, J.C. Leyendecker style',\n",
       "   \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
       "   'Close-Up, Front View',\n",
       "   \"Don Corleone's face fills frame, stern and contemplative. Eyes narrowed, jaw set firmly. Saying 'I'll make him an offer he can't refuse' with quiet, confident menace. Power and authority emanate from his expression.\"],\n",
       "  [0.9, 2, 1.0, 1.2, 1.5]),\n",
       " (['Don Vito Corleone: Italian-American, early 60s, male, slicked-back gray-black hair, stocky, slightly hunched posture, wearing dark three-piece suit, with gold ring on right hand, pocket watch',\n",
       "   'Johnny Fontane: late 30s, male, short, slicked-back black hair, clean shaven, slim and fit, wearing dark, stylish suit with an open collar, with gold ring, cigarette',\n",
       "   'rough b&w pencil sketch, simple sketch lines, minimal shading, rough hatching, draft-style, J.C. Leyendecker style',\n",
       "   \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
       "   'Medium Close-Up, Three-Quarters View',\n",
       "   \"Don Corleone escorts Johnny to door, pinching his cheek firmly. Don's expression shows affection mixed with dominance. Johnny winces slightly at pain while showing relief and gratitude. Door frame visible on right edge of shot.\"],\n",
       "  [0.9, 0.9, 2, 1.0, 1.2, 1.5]),\n",
       " (['Don Vito Corleone: Italian-American, early 60s, male, slicked-back gray-black hair, stocky, slightly hunched posture, wearing dark three-piece suit, with gold ring on right hand, pocket watch',\n",
       "   'Tom Hagen: German-Irish, early 40s, male, short, neatly combed brown hair, clean-shaven, medium build, upright posture, wearing gray suit, dark tie',\n",
       "   'rough b&w pencil sketch, simple sketch lines, minimal shading, rough hatching, draft-style, J.C. Leyendecker style',\n",
       "   \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
       "   'Medium Shot, Front View',\n",
       "   'Don Corleone turns from closed door, small smile fading to serious business expression. Hagen stands attentively near desk, notepad ready. Don moves toward chair, shoulders slightly hunched, gold ring catching light as he gestures.'],\n",
       "  [0.9, 0.9, 2, 1.0, 1.2, 1.5]),\n",
       " (['Don Vito Corleone: Italian-American, early 60s, male, slicked-back gray-black hair, stocky, slightly hunched posture, wearing dark three-piece suit, with gold ring on right hand, pocket watch',\n",
       "   'Tom Hagen: German-Irish, early 40s, male, short, neatly combed brown hair, clean-shaven, medium build, upright posture, wearing gray suit, dark tie',\n",
       "   'Sonny: Italian-American, early 30s, male, curly, dark brown hair, clean-shaven, athletic build, wearing formal suit, slightly disheveled',\n",
       "   'rough b&w pencil sketch, simple sketch lines, minimal shading, rough hatching, draft-style, J.C. Leyendecker style',\n",
       "   \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
       "   'Over-the-Shoulder Shot, Profile View',\n",
       "   \"Camera over Don's shoulder, facing Hagen and Sonny. Don's gray-black hair and dark suit visible in foreground. Hagen's face shows respectful attention. Sonny stands beside him, now composed. Don's voice carries weight as he issues final instructions about hospital visit.\"],\n",
       "  [0.9, 0.9, 0.9, 2, 1.0, 1.2, 1.5]),\n",
       " (['Don Vito Corleone: Italian-American, early 60s, male, slicked-back gray-black hair, stocky, slightly hunched posture, wearing dark three-piece suit, with gold ring on right hand, pocket watch',\n",
       "   'rough b&w pencil sketch, simple sketch lines, minimal shading, rough hatching, draft-style, J.C. Leyendecker style',\n",
       "   \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
       "   'Extreme Close-Up, Front View',\n",
       "   \"Close-up of Don's face, eyes saddened by news of Genco's impending death. Jaw clenched, nostrils flared. Heavy with sorrow and concern for his loyal Consigliere.\"],\n",
       "  [0.9, 2, 1.0, 1.2, 1.5]),\n",
       " (['Don Vito Corleone: Italian-American, early 60s, male, slicked-back gray-black hair, stocky, slightly hunched posture, wearing dark three-piece suit, with gold ring on right hand, pocket watch',\n",
       "   'Tom Hagen: German-Irish, early 40s, male, short, neatly combed brown hair, clean-shaven, medium build, upright posture, wearing gray suit, dark tie',\n",
       "   'Sonny: Italian-American, early 30s, male, curly, dark brown hair, clean-shaven, athletic build, wearing formal suit, slightly disheveled',\n",
       "   'rough b&w pencil sketch, simple sketch lines, minimal shading, rough hatching, draft-style, J.C. Leyendecker style',\n",
       "   \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
       "   'Medium Long Shot, Front View',\n",
       "   \"Don stands, serious and resolute, pointing at Hagen. Sonny stands beside him, watching intently. Hagen listens, notepad forgotten. Don's voice carries weight as he assigns tasks and duties. Authority and leadership radiate from him.\"],\n",
       "  [0.9, 0.9, 0.9, 2, 1.0, 1.2, 1.5]),\n",
       " (['Don Vito Corleone: Italian-American, early 60s, male, slicked-back gray-black hair, stocky, slightly hunched posture, wearing dark three-piece suit, with gold ring on right hand, pocket watch',\n",
       "   'rough b&w pencil sketch, simple sketch lines, minimal shading, rough hatching, draft-style, J.C. Leyendecker style',\n",
       "   \"Don's office, daytime, summer 1945. Elegant wood-paneled room with large desk, leather chairs, warm lighting filtering through venetian blinds.\",\n",
       "   'Medium Close-Up, Three-Quarters View',\n",
       "   \"Don's face fills frame, eyes locked on Hagen. Voice low and firm, words dripping with authority. 'Tom, I want you to go to California tonight. Make the arrangements. But don't leave until I come back from the hospital and speak to you.'\"],\n",
       "  [0.9, 2, 1.0, 1.2, 1.5])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e50ad6127331412fb7b40a5e2a7ca46b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (79 > 77). Running this sequence through the model will result in indexing errors\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['discussion .']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 1 saved to stories/my_storyboard\\image_1.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d2e8bef358d4c5c8ab655479b891e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 2 saved to stories/my_storyboard\\image_2.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e41564b6a1e94d86b64b90d91441f619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 3 saved to stories/my_storyboard\\image_3.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fae09d44d544439955720052d06602c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m save_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstories/my_storyboard\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 607\u001b[0m, in \u001b[0;36mStoryboardGenerator.generate_images\u001b[1;34m(self, save_dir, num_inference_steps, guidance_scale, negative_prompt)\u001b[0m\n\u001b[0;32m    605\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformatted_prompts \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    606\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscenes_to_formatted_prompts()\n\u001b[1;32m--> 607\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_and_save_images_unique\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    609\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformatted_prompts \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[9], line 500\u001b[0m, in \u001b[0;36mStoryboardGenerator.generate_and_save_images_unique\u001b[1;34m(self, save_dir, num_inference_steps, negative_prompt)\u001b[0m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, unique_prompt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(unique_prompts):\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 500\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munique_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnegative_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_inference_steps\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m     generated_image \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mimages[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    506\u001b[0m     generated_images\u001b[38;5;241m.\u001b[39mappend(generated_image)\n",
      "File \u001b[1;32mc:\\Users\\sandr\\anaconda3\\envs\\mldl-ecole\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sandr\\anaconda3\\envs\\mldl-ecole\\Lib\\site-packages\\diffusers\\pipelines\\stable_diffusion\\pipeline_stable_diffusion.py:1055\u001b[0m, in \u001b[0;36mStableDiffusionPipeline.__call__\u001b[1;34m(self, prompt, height, width, num_inference_steps, timesteps, sigmas, guidance_scale, negative_prompt, num_images_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, ip_adapter_image, ip_adapter_image_embeds, output_type, return_dict, cross_attention_kwargs, guidance_rescale, clip_skip, callback_on_step_end, callback_on_step_end_tensor_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1052\u001b[0m     noise_pred \u001b[38;5;241m=\u001b[39m rescale_noise_cfg(noise_pred, noise_pred_text, guidance_rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mguidance_rescale)\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;66;03m# compute the previous noisy sample x_t -> x_t-1\u001b[39;00m\n\u001b[1;32m-> 1055\u001b[0m latents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoise_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_step_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callback_on_step_end \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1058\u001b[0m     callback_kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\sandr\\anaconda3\\envs\\mldl-ecole\\Lib\\site-packages\\diffusers\\schedulers\\scheduling_unipc_multistep.py:985\u001b[0m, in \u001b[0;36mUniPCMultistepScheduler.step\u001b[1;34m(self, model_output, timestep, sample, return_dict)\u001b[0m\n\u001b[0;32m    983\u001b[0m model_output_convert \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_model_output(model_output, sample\u001b[38;5;241m=\u001b[39msample)\n\u001b[0;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_corrector:\n\u001b[1;32m--> 985\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultistep_uni_c_bh_update\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    986\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthis_model_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_output_convert\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    987\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlast_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_sample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    988\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthis_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    989\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthis_order\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    990\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39msolver_order \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    993\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_outputs[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_outputs[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\sandr\\anaconda3\\envs\\mldl-ecole\\Lib\\site-packages\\diffusers\\schedulers\\scheduling_unipc_multistep.py:854\u001b[0m, in \u001b[0;36mUniPCMultistepScheduler.multistep_uni_c_bh_update\u001b[1;34m(self, this_model_output, last_sample, this_sample, order, *args, **kwargs)\u001b[0m\n\u001b[0;32m    851\u001b[0m     D1s\u001b[38;5;241m.\u001b[39mappend((mi \u001b[38;5;241m-\u001b[39m m0) \u001b[38;5;241m/\u001b[39m rk)\n\u001b[0;32m    853\u001b[0m rks\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m--> 854\u001b[0m rks \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    856\u001b[0m R \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    857\u001b[0m b \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "save_directory = \"stories/my_storyboard\"\n",
    "images = generator.generate_images(save_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
