{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.movies import get_movie_script\n",
    "from src.storyboard_generator import StoryboardGenerator\n",
    "import torch \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "script, characters = get_movie_script(\"The godfather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Don Vito Corleone': {'age': 'early 60s',\n",
       "  'gender': 'male',\n",
       "  'hair': 'slicked-back gray-black hair',\n",
       "  'clothing': 'dark three-piece suit',\n",
       "  'body_type': 'stocky, slightly hunched posture',\n",
       "  'accessories': 'gold ring on right hand, pocket watch',\n",
       "  'ethnicity': 'Italian-American'},\n",
       " 'Tom Hagen': {'age': 'early 40s',\n",
       "  'gender': 'male',\n",
       "  'hair': 'short, neatly combed brown hair',\n",
       "  'facial_hair': 'clean-shaven',\n",
       "  'clothing': 'gray suit, dark tie',\n",
       "  'body_type': 'medium build, upright posture',\n",
       "  'ethnicity': 'German-Irish'},\n",
       " 'Johnny Fontane': {'age': 'late 30s',\n",
       "  'gender': 'male',\n",
       "  'hair': 'short, slicked-back black hair',\n",
       "  'facial_hair': 'clean shaven',\n",
       "  'clothing': 'dark, stylish suit with an open collar',\n",
       "  'body_type': 'slim and fit',\n",
       "  'accessories': 'gold ring, cigarette'},\n",
       " 'Sonny': {'age': 'early 30s',\n",
       "  'gender': 'male',\n",
       "  'hair': 'curly, dark brown hair',\n",
       "  'facial_hair': 'clean-shaven',\n",
       "  'clothing': 'formal suit, slightly disheveled',\n",
       "  'body_type': 'athletic build',\n",
       "  'ethnicity': 'Italian-American'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = StoryboardGenerator(script, characters_dict, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.generate_and_save(save_dir=\"unique\", generation_type=\"unique\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.generate_and_save(save_dir=\"prompt_weights\", generation_type=\"prompt_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.generate_and_save(save_dir=\"modified-cfg\", generation_type=\"modified-cfg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Comments on Modified Classifier-Free Guidance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Unconditional Pass + Multiple Conditional Passes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\hat{\\epsilon}_{\\text{cond\\_combined}}=\\frac{1}{\\sum_{i=1}^nw_i}\\sum_{i=1}^nw_i\\hat{\\epsilon}_{\\text{cond}_i}$\n",
    "where we have one pass per subprompt to get $\\hat{\\epsilon}_{\\text{cond}_i}$ and $n$ is the number of subprompts.\n",
    "Then the classifier free guidance with scale $g$ is $$\\hat{\\epsilon}=\\hat{\\epsilon}_{\\text{uncond}}+g(\\hat{\\epsilon}_{\\text{cond\\_combined}}-\\hat{\\epsilon}_{\\text{uncond}})$$\n",
    "where we have one unconditional pass at each step to get $\\hat{\\epsilon}_{\\text{uncond}}$\n",
    "\n",
    "- Total UNet calls per step: $1+n$\n",
    "- Each subprompt has a relative weight but they all share the same baseline unconditional pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Unconditional Passes (One per Subprompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have $$\\hat{\\epsilon}=\\hat{\\epsilon}_{\\text{uncond}}+g\\sum_{i=1}^nw_i(\\hat{\\epsilon}_{\\text{cond}_i}-\\hat{\\epsilon}_{\\text{uncond}_i})$$\n",
    "- Total UNet calls per step: $1+2n$ (One global unconditional + two passes for each subprompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiPromptPipelineApproach2(StableDiffusionPipeline):\n",
    "#     \"\"\"\n",
    "#     Multi-Prompt CFG with MULTIPLE unconditional passes:\n",
    "#       - 1 global unconditional pass per step: e_uncond\n",
    "#       - For each subprompt i:\n",
    "#           e_uncond_i (subprompt-specific unconditional)\n",
    "#           e_cond_i    (subprompt conditional)\n",
    "#       - Combine: e = e_uncond + g * sum_i[ w_i * ( e_cond_i - e_uncond_i ) ]\n",
    "#     \"\"\"\n",
    "\n",
    "#     @torch.no_grad()\n",
    "#     def __call__(\n",
    "#         self,\n",
    "#         global_uncond_embeds: torch.Tensor,\n",
    "#         subprompt_pairs: list[tuple[torch.Tensor, torch.Tensor]],\n",
    "#         subprompt_weights: list[float],\n",
    "#         guidance_scale: float = 7.5,\n",
    "#         height: int = 512,\n",
    "#         width: int = 512,\n",
    "#         num_inference_steps: int = 50,\n",
    "#         generator: torch.Generator = None,\n",
    "#         latents: torch.Tensor = None,\n",
    "#         output_type: str = \"pil\",\n",
    "#         return_dict: bool = True,\n",
    "#         **kwargs\n",
    "#     ):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             global_uncond_embeds (Tensor): [batch, seq_len, hidden_dim] for the entire prompt's unconditional pass.\n",
    "#             subprompt_pairs (list of (uncond_i, cond_i)):\n",
    "#                 Each element is a tuple: (uncond_embeds_i, cond_embeds_i).\n",
    "#             subprompt_weights (list[float]): Weights w_i for each subprompt i.\n",
    "#         \"\"\"\n",
    "#         device = self._execution_device\n",
    "#         batch_size = global_uncond_embeds.shape[0]\n",
    "#         num_subprompts = len(subprompt_pairs)\n",
    "\n",
    "#         if num_subprompts != len(subprompt_weights):\n",
    "#             raise ValueError(\"subprompt_pairs and subprompt_weights must have the same length.\")\n",
    "\n",
    "#         # 1. Validate or fallback to default\n",
    "#         if not height or not width:\n",
    "#             height, width = self._default_height_width()\n",
    "\n",
    "#         # 2. Scheduler timesteps\n",
    "#         self.scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "#         timesteps = self.scheduler.timesteps\n",
    "\n",
    "#         # 3. Prepare latents\n",
    "#         if latents is None:\n",
    "#             shape = (batch_size, self.unet.config.in_channels, height // 8, width // 8)\n",
    "#             latents = torch.randn(shape, generator=generator, device=device, dtype=global_uncond_embeds.dtype)\n",
    "#             latents = latents * self.scheduler.init_noise_sigma\n",
    "#         else:\n",
    "#             latents = latents.to(device)\n",
    "\n",
    "#         # 4. Diffusion loop\n",
    "#         for i, t in enumerate(timesteps):\n",
    "#             latent_model_input = self.scheduler.scale_model_input(latents, t)\n",
    "\n",
    "#             # (A) Single global unconditional pass\n",
    "#             e_uncond_global = self.unet(\n",
    "#                 latent_model_input, t, encoder_hidden_states=global_uncond_embeds, **kwargs\n",
    "#             ).sample\n",
    "\n",
    "#             # (B) For each subprompt: unconditional + conditional\n",
    "#             sub_deltas = []\n",
    "#             for (uncond_i, cond_i), w in zip(subprompt_pairs, subprompt_weights):\n",
    "#                 e_uncond_i = self.unet(latent_model_input, t, encoder_hidden_states=uncond_i, **kwargs).sample\n",
    "#                 e_cond_i = self.unet(latent_model_input, t, encoder_hidden_states=cond_i, **kwargs).sample\n",
    "\n",
    "#                 # Delta for subprompt i\n",
    "#                 delta_i = w * (e_cond_i - e_uncond_i)\n",
    "#                 sub_deltas.append(delta_i)\n",
    "\n",
    "#             # (C) Combine sub-deltas\n",
    "#             sum_deltas = sum(sub_deltas)  # sum_i w_i ( e_cond_i - e_uncond_i )\n",
    "\n",
    "#             # (D) Final output\n",
    "#             guided_out = e_uncond_global + guidance_scale * sum_deltas\n",
    "\n",
    "#             # (E) Scheduler step\n",
    "#             latents = self.scheduler.step(guided_out, t, latents, **kwargs).prev_sample\n",
    "\n",
    "#         # 5. Decode\n",
    "#         if output_type == \"latent\":\n",
    "#             if return_dict:\n",
    "#                 from diffusers.pipelines.stable_diffusion.pipeline_output import StableDiffusionPipelineOutput\n",
    "#                 return StableDiffusionPipelineOutput(images=latents, nsfw_content_detected=None)\n",
    "#             return latents\n",
    "\n",
    "#         image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=False)[0]\n",
    "#         image = self.image_processor.postprocess(image, output_type=output_type)\n",
    "\n",
    "#         if return_dict:\n",
    "#             from diffusers.pipelines.stable_diffusion.pipeline_output import StableDiffusionPipelineOutput\n",
    "#             return StableDiffusionPipelineOutput(images=image, nsfw_content_detected=None)\n",
    "#         return image\n",
    "\n",
    "# print(\"Loading Approach 2 pipeline...\")\n",
    "# pipe2 = MultiPromptPipelineApproach2.from_pretrained(\n",
    "#     \"runwayml/stable-diffusion-v1-5\",\n",
    "#     torch_dtype=torch.float16\n",
    "# ).to(\"cuda\")\n",
    "# pipe2.scheduler = UniPCMultistepScheduler.from_config(pipe2.scheduler.config)\n",
    "# pipe2.enable_model_cpu_offload()\n",
    "# pipe2.enable_attention_slicing()\n",
    "\n",
    "#### EXAMPLE\n",
    "# # Suppose we want environment and style separately\n",
    "# global_uncond = encode_subprompt(pipe2, \"\")  # global unconditional\n",
    "# env_uncond = encode_subprompt(pipe2, \"\")     # unconditional for environment\n",
    "# env_cond   = encode_subprompt(pipe2, \"ancient forest, misty atmosphere\")\n",
    "# style_uncond = encode_subprompt(pipe2, \"\")   # unconditional for style\n",
    "# style_cond   = encode_subprompt(pipe2, \"cinematic style, high contrast\")\n",
    "\n",
    "# # subprompt_pairs = [ (uncond_env, cond_env), (uncond_style, cond_style) ]\n",
    "# subprompt_pairs_2 = [\n",
    "#     (env_uncond, env_cond),\n",
    "#     (style_uncond, style_cond)\n",
    "# ]\n",
    "\n",
    "# weights_2 = [1.5, 1.8]\n",
    "# print(\"Generating image with Approach 2 (multiple unconditional passes)...\")\n",
    "# output2 = pipe2(\n",
    "#     global_uncond_embeds=global_uncond,\n",
    "#     subprompt_pairs=subprompt_pairs_2,\n",
    "#     subprompt_weights=weights_2,\n",
    "#     guidance_scale=7.5,\n",
    "#     num_inference_steps=25\n",
    "# )\n",
    "# output2.images[0].save(\"approach2_result.png\")\n",
    "# print(\"Saved approach2_result.png\")\n",
    "\n",
    "# def generate_and_save_images_multi_prompt2(scenes, characters_dict, pipe, save_dir, device,\n",
    "#                                              num_inference_steps=50, guidance_scale=7.5):\n",
    "#     \"\"\"\n",
    "#     Generate images for each scene using Multi-Prompt Approach 2 (multiple unconditional passes)\n",
    "#     and save each image to the specified directory.\n",
    "    \n",
    "#     Args:\n",
    "#         scenes (list): List of scene objects (each scene is a dict).\n",
    "#         characters_dict (dict): Dictionary of character descriptions.\n",
    "#         pipe: The MultiPromptPipelineApproach2 pipeline instance.\n",
    "#         save_dir (str): Directory where images will be saved.\n",
    "#         device (str): Device to use (e.g., \"cuda\" or \"cpu\").\n",
    "#         num_inference_steps (int, optional): Number of diffusion steps.\n",
    "#         guidance_scale (float, optional): Guidance scale for classifier-free guidance.\n",
    "        \n",
    "#     Returns:\n",
    "#         list: List of generated PIL.Image objects.\n",
    "#     \"\"\"\n",
    "#     import os\n",
    "#     import torch\n",
    "\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "#     generated_images = []\n",
    "\n",
    "#     for i, scene in enumerate(scenes):\n",
    "#         # Get subprompt texts and corresponding weights for the scene.\n",
    "#         subprompt_texts, subprompt_weights = scenes_to_formatted_prompts([scene], characters_dict)[0]\n",
    "\n",
    "#         # Encode the global unconditional prompt once.\n",
    "#         global_uncond_embeds = encode_subprompt(pipe, \"\", device=device)\n",
    "\n",
    "#         # For each subprompt, encode a pair: (unconditional, conditional)\n",
    "#         subprompt_pairs = []\n",
    "#         for sp in subprompt_texts:\n",
    "#             uncond_i = encode_subprompt(pipe, \"\", device=device)\n",
    "#             cond_i = encode_subprompt(pipe, sp, device=device)\n",
    "#             subprompt_pairs.append((uncond_i, cond_i))\n",
    "\n",
    "#         print(f\"Generating image for scene {i+1} using Approach 2...\")\n",
    "#         with torch.no_grad():\n",
    "#             output = pipe(\n",
    "#                 global_uncond_embeds=global_uncond_embeds,\n",
    "#                 subprompt_pairs=subprompt_pairs,\n",
    "#                 subprompt_weights=subprompt_weights,\n",
    "#                 guidance_scale=guidance_scale,\n",
    "#                 num_inference_steps=num_inference_steps\n",
    "#             )\n",
    "#         generated_image = output.images[0]\n",
    "#         generated_images.append(generated_image)\n",
    "#         image_path = os.path.join(save_dir, f\"scene_{i+1}_approach2.png\")\n",
    "#         generated_image.save(image_path)\n",
    "#         print(f\"Image {i+1} saved to {image_path}\")\n",
    "\n",
    "#     return generated_images\n",
    "\n",
    "# # Example usage:\n",
    "# save_directory = \"stories/multi_prompt_approach2\"\n",
    "# generated_images = generate_and_save_images_multi_prompt2(scenes, characters_dict, pipe2, save_directory, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopped this because it's extremely slow (20 min for one image) and it's not good either."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldl-ecole",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
